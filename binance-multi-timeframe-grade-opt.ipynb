{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVndzwfXSHzWJUXl5yAcSO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ifeLight/ml-bot/blob/main/binance-multi-timeframe-grade-opt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas-ta\n",
        "!pip install backtrader[plotting]\n",
        "!pip install plotly\n",
        "!pip install --upgrade firebase-admin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZkp-JVxXjc6",
        "outputId": "58af5451-c5a4-4db1-dd5e-8f7ed3e76726"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas-ta\n",
            "  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from pandas-ta) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas-ta) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas-ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas-ta) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas-ta) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->pandas-ta) (1.17.0)\n",
            "Building wheels for collected packages: pandas-ta\n",
            "  Building wheel for pandas-ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas-ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218909 sha256=109622ec3041de1164d2c15d0f3367997724361fb2c079e37ee4dadd97d61163\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/33/8b/50b245c5c65433cd8f5cb24ac15d97e5a3db2d41a8b6ae957d\n",
            "Successfully built pandas-ta\n",
            "Installing collected packages: pandas-ta\n",
            "Successfully installed pandas-ta-0.3.14b0\n",
            "Collecting backtrader[plotting]\n",
            "  Downloading backtrader-1.9.78.123-py2.py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from backtrader[plotting]) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->backtrader[plotting]) (1.17.0)\n",
            "Downloading backtrader-1.9.78.123-py2.py3-none-any.whl (419 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.5/419.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: backtrader\n",
            "Successfully installed backtrader-1.9.78.123\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Requirement already satisfied: firebase-admin in /usr/local/lib/python3.11/dist-packages (6.6.0)\n",
            "Requirement already satisfied: cachecontrol>=0.12.14 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (0.14.2)\n",
            "Requirement already satisfied: google-api-python-client>=1.7.8 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (2.160.0)\n",
            "Requirement already satisfied: google-cloud-storage>=1.37.1 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (2.19.0)\n",
            "Requirement already satisfied: pyjwt>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.5.0->firebase-admin) (2.10.1)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=1.22.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (2.24.1)\n",
            "Requirement already satisfied: google-cloud-firestore>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (2.20.1)\n",
            "Requirement already satisfied: requests>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from cachecontrol>=0.12.14->firebase-admin) (2.32.3)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from cachecontrol>=0.12.14->firebase-admin) (1.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.69.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (4.25.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.26.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (2.38.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.62.3)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (4.1.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-firestore>=2.19.0->firebase-admin) (2.4.2)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage>=1.37.1->firebase-admin) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage>=1.37.1->firebase-admin) (1.6.0)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.5.0->firebase-admin) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.5.0->firebase-admin) (1.17.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client>=1.7.8->firebase-admin) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16.0->cachecontrol>=0.12.14->firebase-admin) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16.0->cachecontrol>=0.12.14->firebase-admin) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16.0->cachecontrol>=0.12.14->firebase-admin) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16.0->cachecontrol>=0.12.14->firebase-admin) (2025.1.31)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.5.0->firebase-admin) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import backtrader as bt\n",
        "import pandas_ta as ta\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import subprocess\n",
        "import firebase_admin\n",
        "from firebase_admin import firestore\n",
        "from requests import Request, Session\n",
        "from requests.exceptions import ConnectionError, Timeout, TooManyRedirects\n",
        "from google.colab import auth\n",
        "import google.auth"
      ],
      "metadata": {
        "id": "dtUTSBApSZe4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "#Configure Google cloud project\n",
        "project_id = 'ifelight'\n",
        "!gcloud config set project {project_id}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELE8dAggI0FQ",
        "outputId": "8bb54e40-ea92-4000-c44c-435955f3d1e8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Working GCP Bucket name\n",
        "bucket_name = 'ife-storage'\n",
        "# Working on Firestore name\n",
        "firestore_collection_name = 'trade-models'"
      ],
      "metadata": {
        "id": "Fs_TurcmdJql"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binance_base_url = 'https://52on3577u3.execute-api.eu-central-1.amazonaws.com'\n",
        "\n",
        "def get_binance_candles(symbol: str, interval='1h', limit: int = 50, **kwargs):\n",
        "    url = f'{binance_base_url}/api/v3/uiKlines?symbol={symbol}&interval={interval}&limit={limit}'\n",
        "    for key, value in kwargs.items():\n",
        "        url += f'&{key}={value}'\n",
        "    response = requests.get(url)\n",
        "    result = json.loads(response.text)\n",
        "    # print(result)\n",
        "    def map_result(x):\n",
        "        return {\n",
        "            'Date': x[0],\n",
        "            'Open': x[1],\n",
        "            'High': x[2],\n",
        "            'Low': x[3],\n",
        "            'Close': x[4],\n",
        "            'Volume': x[5],\n",
        "        }\n",
        "    mappeded_result = []\n",
        "    for x in result:\n",
        "        mappeded_result.append(map_result(x))\n",
        "    return mappeded_result\n",
        "\n",
        "\n",
        "def candles_to_df(data):\n",
        "    df =  pd.DataFrame(data)\n",
        "    df['Date'] = pd.to_datetime(df['Date'], unit='ms')\n",
        "    df['Open'] = df['Open'].astype(float)\n",
        "    df['High'] = df['High'].astype(float)\n",
        "    df['Low'] = df['Low'].astype(float)\n",
        "    df['Close'] = df['Close'].astype(float)\n",
        "    df['Volume'] = df['Volume'].astype(float)\n",
        "    df.set_index('Date', inplace=True)\n",
        "    return df\n",
        "\n",
        "def get_all_binance_candles(symbol: str, interval='1h', start_date=None, end_date=None, limit=1000):\n",
        "  try:\n",
        "    return load_candles_from_cloud_storage(symbol, interval, start_date, end_date)\n",
        "  except FileNotFoundError:\n",
        "    pass\n",
        "  result = []\n",
        "  raw_start_date = start_date\n",
        "  raw_end_date = end_date\n",
        "  start_date = pd.to_datetime(start_date) if start_date else pd.to_datetime('2015-01-01')\n",
        "  end_date = pd.to_datetime(end_date) if end_date else pd.to_datetime('today')\n",
        "  while True:\n",
        "    candles = get_binance_candles(symbol, interval, limit, startTime=int(start_date.timestamp() * 1000), endTime=int(end_date.timestamp() * 1000))\n",
        "    if len(candles) <= 1:\n",
        "      break;\n",
        "    result += candles\n",
        "    start_date = pd.to_datetime(datetime.datetime.fromtimestamp(candles[-1]['Date'] / 1000))\n",
        "  candles_df = candles_to_df(result)\n",
        "  save_candles_to_cloud_storage(candles_df, symbol, interval, raw_start_date, raw_end_date)\n",
        "  return candles_df\n",
        "\n",
        "def candles_storage_file_name(symbol: str, interval='1h', start_date=None, end_date=None):\n",
        "  file_name = f'binance_{symbol}_{interval}_{start_date}_{end_date}.csv'\n",
        "  return file_name\n",
        "\n",
        "def load_candles_from_cloud_storage(symbol: str, interval: str, start_date=None, end_date=None):\n",
        "  file_name = candles_storage_file_name(symbol, interval, start_date, end_date)\n",
        "  try:\n",
        "    # Download the file from cloud storage.\n",
        "    subprocess.run(['gsutil', 'cp', f'gs://{bucket_name}/trade/candles/{file_name}', f'/tmp/{file_name}'], check=True)\n",
        "\n",
        "    # Load the data into a Pandas DataFrame.\n",
        "    with open(f'/tmp/{file_name}', 'r') as f:\n",
        "      return pd.read_csv(f, index_col=0, parse_dates=True)\n",
        "  except subprocess.CalledProcessError:\n",
        "    # Raise a FileNotFoundError if the file is not found in cloud storage.\n",
        "    raise FileNotFoundError(f\"File not found: gs://{bucket_name}/trade/candles/{file_name}\")\n",
        "\n",
        "def save_candles_to_cloud_storage(df: pd.DataFrame, symbol: str, interval: str, start_date, end_date):\n",
        "  file_name = candles_storage_file_name(symbol, interval, start_date, end_date)\n",
        "  df.to_csv(f'/tmp/{file_name}')\n",
        "  !gsutil cp /tmp/{file_name} gs://{bucket_name}/trade/candles/{file_name}\n"
      ],
      "metadata": {
        "id": "r0DdxBTXLcao"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_pivots(df, window=5):\n",
        "    \"\"\"\n",
        "    Calculate the probability of price going up and down based on higher and lower pivots.\n",
        "    \"\"\"\n",
        "    df['Higher_Pivot'] = df['High'].rolling(window=2 * window + 1, center=True).apply(\n",
        "        lambda x: 1 if x.iloc[window] == x.max() else 0, raw=False\n",
        "    )\n",
        "    df['Lower_Pivot'] = df['Low'].rolling(window=2 * window + 1, center=True).apply(\n",
        "        lambda x: 1 if x.iloc[window] == x.min() else 0, raw=False\n",
        "    )\n",
        "\n",
        "    # Step 2: Ensure no two successive pivots of the same type\n",
        "    pivot_type = None  # Tracks the type of the last pivot\n",
        "    last_pivot_index = None  # Tracks the index of the last pivot\n",
        "\n",
        "    # Remove duplicated index\n",
        "    df = df[~df.index.duplicated(keep='first')]\n",
        "\n",
        "    for i in range(len(df.index)):\n",
        "      if df.loc[df.index[i], 'Higher_Pivot'].item() == 1:\n",
        "          if pivot_type == 'higher':\n",
        "              # Compare current higher pivot with the last higher pivot\n",
        "              if df.loc[df.index[i], 'High'].item() > df.loc[df.index[last_pivot_index], 'High'].item():\n",
        "                  # Remove the last higher pivot\n",
        "                  df.loc[df.index[last_pivot_index], 'Higher_Pivot'] = 0\n",
        "                  # Update the last pivot\n",
        "                  last_pivot_index = i\n",
        "              else:\n",
        "                  # Remove the current higher pivot\n",
        "                  df.loc[df.index[i], 'Higher_Pivot'] = 0\n",
        "          else:\n",
        "              # Update pivot type and index\n",
        "              pivot_type = 'higher'\n",
        "              last_pivot_index = i\n",
        "\n",
        "      elif df.loc[df.index[i], 'Lower_Pivot'].item() == 1:\n",
        "          if pivot_type == 'lower':\n",
        "              # Compare current lower pivot with the last lower pivot\n",
        "              if df.loc[df.index[i], 'Low'].item() < df.loc[df.index[last_pivot_index], 'Low'].item():\n",
        "                  # Remove the last lower pivot\n",
        "                  df.loc[df.index[last_pivot_index], 'Lower_Pivot'] = 0\n",
        "                  # Update the last pivot\n",
        "                  last_pivot_index = i\n",
        "              else:\n",
        "                  # Remove the current lower pivot\n",
        "                  df.loc[df.index[i], 'Lower_Pivot'] = 0\n",
        "          else:\n",
        "              # Update pivot type and index\n",
        "              pivot_type = 'lower'\n",
        "              last_pivot_index = i\n",
        "    return df"
      ],
      "metadata": {
        "id": "551o_MKlZhr2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_pivot_proximity(df):\n",
        "    \"\"\"\n",
        "    Calculates the pivot proximity using a loop-based approach, finding the closest\n",
        "    previous and next pivots without generating intermediate lists of all pivots.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with 'Pivot' and 'Close' columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with 'Pivot_Proximity' column added.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    pivot_values = df['Pivot'].values\n",
        "    close_values = df['Close'].values\n",
        "    proximity_values = [0.0] * len(df)\n",
        "\n",
        "    for i in range(len(df)):\n",
        "      pivot = pivot_values[i]\n",
        "      if pivot == -1:\n",
        "        proximity_values[i] = -1.0\n",
        "      elif pivot == 1:\n",
        "        proximity_values[i] = 1.0\n",
        "      else:\n",
        "        closest_previous_pivot_index = None\n",
        "        for j in range(i - 1, -1, -1):\n",
        "          if pivot_values[j] != 0:\n",
        "            closest_previous_pivot_index = j\n",
        "            break\n",
        "\n",
        "        if closest_previous_pivot_index is not None:\n",
        "          closest_previous_pivot_value = pivot_values[closest_previous_pivot_index]\n",
        "          closest_previous_close = close_values[closest_previous_pivot_index]\n",
        "\n",
        "          closest_next_pivot_index = None\n",
        "          if closest_previous_pivot_value == -1:\n",
        "            for j in range(i + 1, len(df)):\n",
        "              if pivot_values[j] == 1:\n",
        "                closest_next_pivot_index = j\n",
        "                break\n",
        "          else:\n",
        "            for j in range(i + 1, len(df)):\n",
        "              if pivot_values[j] == -1:\n",
        "                closest_next_pivot_index = j\n",
        "                break\n",
        "\n",
        "          if closest_next_pivot_index is not None:\n",
        "            closest_next_close = close_values[closest_next_pivot_index]\n",
        "\n",
        "            distance_to_previous = abs(close_values[i] - closest_previous_close)\n",
        "            distance_to_next = abs(close_values[i] - closest_next_close)\n",
        "\n",
        "            if distance_to_previous + distance_to_next != 0:\n",
        "              if closest_previous_pivot_value == -1:\n",
        "                proximity_values[i] = (distance_to_previous - distance_to_next) / (distance_to_previous + distance_to_next)\n",
        "              else:\n",
        "                proximity_values[i] = (distance_to_next - distance_to_previous) / (distance_to_previous + distance_to_next)\n",
        "\n",
        "    df['Pivot_Proximity'] = proximity_values\n",
        "    return df"
      ],
      "metadata": {
        "id": "Ozb2WlKwCCOB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_scaled_rsi(df, window=14, prefix = '', features_columns=[]):\n",
        "  series = ta.rsi(df['Close'], length=window)\n",
        "  column_name = f\"{prefix}RSI_{window}\"\n",
        "  df[column_name] = series / 100\n",
        "  if(column_name not in features_columns):\n",
        "    features_columns.append(column_name)\n",
        "  return df\n",
        "\n",
        "def add_scaled_ema(df, window=50, prefix = '', features_columns=[]):\n",
        "  series = ta.ema(df['Close'], length=window)\n",
        "  column_name = f\"{prefix}EMA_{window}\"\n",
        "  df[column_name] = series / df['Close']\n",
        "  if column_name not in features_columns:\n",
        "    features_columns.append(column_name)\n",
        "  return df\n",
        "\n",
        "def add_scaled_sma(df, window=50, prefix = '', features_columns=[]):\n",
        "  series = ta.sma(df['Close'], length=window)\n",
        "  column_name = f\"{prefix}SMA_{window}\"\n",
        "  df[column_name] = series / df['Close']\n",
        "  if column_name not in features_columns:\n",
        "    features_columns.append(column_name)\n",
        "  return df\n",
        "\n",
        "def add_scaled_macd(df, prefix= '', features_columns=[], fast = 12, slow = 26, signal=9):\n",
        "  macd_df = ta.macd(df['Close'], fast=fast, slow=slow, signal=signal)\n",
        "  suffix = f\"{fast}_{slow}_{signal}\"\n",
        "  macd_column_name = f\"{prefix}MACD_{suffix}\"\n",
        "  macds_column_name = f\"{prefix}MACDs_{suffix}\"\n",
        "  macdh_column_name = f\"{prefix}MACDh_{suffix}\"\n",
        "  df[macd_column_name] = macd_df[macd_df.columns[0]] / df['Close']\n",
        "  df[macds_column_name] = macd_df[macd_df.columns[2]] / df['Close']\n",
        "  df[macdh_column_name] = macd_df[macd_df.columns[1]] / df['Close']\n",
        "  if macd_column_name not in features_columns:\n",
        "    features_columns.append(macd_column_name)\n",
        "  if macds_column_name not in features_columns:\n",
        "    features_columns.append(macds_column_name)\n",
        "  if macdh_column_name not in features_columns:\n",
        "    features_columns.append(macdh_column_name)\n",
        "  return df\n",
        "\n",
        "def add_scaled_bbands(df, window=20, std=2.0, prefix= '', features_columns=[]):\n",
        "  bbands_df = ta.bbands(df['Close'], length=window, std=std)\n",
        "  suffix = f\"{window}_{std}\"\n",
        "  bbl_column_name = f\"{prefix}BBL_{suffix}\"\n",
        "  bbm_column_name = f\"{prefix}BBM_{suffix}\"\n",
        "  bbu_column_name = f\"{prefix}BBU_{suffix}\"\n",
        "  bbb_column_name = f\"{prefix}BBB_{suffix}\"\n",
        "  bbp_column_name = f\"{prefix}BBP_{suffix}\"\n",
        "  df[bbl_column_name] = bbands_df[bbands_df.columns[0]] / df['Close']\n",
        "  df[bbm_column_name] = bbands_df[bbands_df.columns[1]] / df['Close']\n",
        "  df[bbu_column_name] = bbands_df[bbands_df.columns[2]] / df['Close']\n",
        "  df[bbb_column_name] = bbands_df[bbands_df.columns[3]]\n",
        "  df[bbp_column_name] = bbands_df[bbands_df.columns[4]]\n",
        "  if bbl_column_name not in features_columns: features_columns.append(bbl_column_name)\n",
        "  if bbm_column_name not in features_columns: features_columns.append(bbm_column_name)\n",
        "  if bbu_column_name not in features_columns: features_columns.append(bbu_column_name)\n",
        "  if bbb_column_name not in features_columns: features_columns.append(bbb_column_name)\n",
        "  if bbp_column_name not in features_columns: features_columns.append(bbp_column_name)\n",
        "  return df"
      ],
      "metadata": {
        "id": "Rp7SlrZXaupS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_candlesticks_data(df1, df2):\n",
        "  \"\"\"\n",
        "  Merges two candlestick DataFrames with forward fill, handling different timeframes,\n",
        "  and prevents duplicate OHLCV columns.\n",
        "  Ensure both DataFrames have a datetime index.\n",
        "  And ensure the second DataFrame is the larger timeframe\n",
        "\n",
        "  Args:\n",
        "      df1: First candlestick DataFrame with datetime index.\n",
        "      df2: Second candlestick DataFrame with datetime index.\n",
        "\n",
        "  Returns:\n",
        "      Merged DataFrame with forward-filled values, and no duplicate OHLCV columns.\n",
        "  \"\"\"\n",
        "  # Ensure both DataFrames have a datetime index\n",
        "  if not isinstance(df1.index, pd.DatetimeIndex) or not isinstance(df2.index, pd.DatetimeIndex):\n",
        "      raise ValueError(\"DataFrames must have a datetime index.\")\n",
        "  # Identify OHLCV columns\n",
        "  ohlcv_cols = ['open', 'high', 'low', 'close', 'volume']\n",
        "\n",
        "  # Rename columns in df2 that conflict with df1's OHLCV columns\n",
        "  for col in df2.columns:\n",
        "    if col.lower() in ohlcv_cols and col.lower() in df1.columns.str.lower():\n",
        "      del df2[col]\n",
        "\n",
        "  # Merge the DataFrames using outer join, which preserves all dates\n",
        "  merged_df = pd.merge(df1, df2, how='outer', left_index=True, right_index=True, suffixes=('_df1', '_df2'))\n",
        "\n",
        "  # Forward fill the missing values for each column\n",
        "  for col in merged_df.columns:\n",
        "    merged_df[col] = merged_df[col].ffill()\n",
        "\n",
        "  return merged_df"
      ],
      "metadata": {
        "id": "8nz7qqCsit1b"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resample_candles(df, interval='1h'):\n",
        "    return df.resample(interval).agg({'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last', 'Volume': 'sum'}).ffill()"
      ],
      "metadata": {
        "id": "t9eNtD9QgvIa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, features_columns, target_col, seq_length):\n",
        "    \"\"\"\n",
        "    Create sequences of `seq_length` time steps for LSTM input, optimized for performance.\n",
        "    \"\"\"\n",
        "    num_samples = len(data) - seq_length\n",
        "    num_features = len(features_columns)\n",
        "\n",
        "    X = np.zeros((num_samples, seq_length, num_features))\n",
        "    y = np.zeros(num_samples)\n",
        "\n",
        "    features_data = data[features_columns].values\n",
        "    target_data = data[target_col].values\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        X[i] = features_data[i:i + seq_length]\n",
        "        y[i] = target_data[i + seq_length]\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "77dtP0gVDcVF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomEarlyStopping(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, patience=2):\n",
        "        super(CustomEarlyStopping, self).__init__()\n",
        "        self.patience = patience  # Number of consecutive increases allowed\n",
        "        self.best_val_mae = float('inf')  # Track the best validation MAE\n",
        "        self.increase_count = 0  # Counter for consecutive increases\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current_val_mae = logs.get('val_mae')  # Get validation MAE for the current epoch\n",
        "\n",
        "        if current_val_mae < self.best_val_mae:\n",
        "            # If validation MAE improves, reset the counter\n",
        "            self.best_val_mae = current_val_mae\n",
        "            self.increase_count = 0\n",
        "        else:\n",
        "            # If validation MAE increases, increment the counter\n",
        "            self.increase_count += 1\n",
        "\n",
        "        # Stop training if validation MAE increases consecutively for 'patience' epochs\n",
        "        if self.increase_count >= self.patience:\n",
        "            print(f\"\\nEarly stopping: Validation MAE increased {self.patience} times in a row.\")\n",
        "            self.model.stop_training = True"
      ],
      "metadata": {
        "id": "PYV3ovEs6j_w"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_in_batches(model, test_data, features_columns, seq_length, batch_size=128):\n",
        "    \"\"\"\n",
        "    Predicts in batches to speed up inference.\n",
        "    \"\"\"\n",
        "    num_samples = len(test_data) - seq_length\n",
        "    feature_array = test_data[features_columns].values\n",
        "    predicted_values = np.full(len(test_data), np.nan)\n",
        "\n",
        "    for start_idx in range(0, num_samples, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, num_samples)\n",
        "        batch_indices = range(start_idx + seq_length, end_idx + seq_length)\n",
        "        batch_input = np.array([feature_array[i - seq_length:i] for i in batch_indices])\n",
        "\n",
        "        if len(batch_input) > 0 :\n",
        "            predictions = model.predict(batch_input, verbose=0)\n",
        "            predicted_values[batch_indices] = predictions.flatten() #Flatten to 1d array.\n",
        "\n",
        "    test_data['Predicted_Value'] = predicted_values\n",
        "    return test_data"
      ],
      "metadata": {
        "id": "UxAXoG8PE9je"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PredictedValueStrategy(bt.Strategy):\n",
        "    params = (\n",
        "        ('buy_threshold', 0.6),\n",
        "        ('sell_threshold', 0.6),\n",
        "        ('leverage', 1),  # Leverage ratio\n",
        "        ('margin', 1000),\n",
        "        ('log', True)\n",
        "    )\n",
        "\n",
        "    def __init__(self):\n",
        "        # To keep track of pending orders and buy price/commission\n",
        "        self.order = None\n",
        "        self.buyprice = None\n",
        "        self.buycomm = None\n",
        "\n",
        "        # Add the Predicted_Value as a data feed\n",
        "        self.predicted_value = self.datas[0].predicted_value\n",
        "\n",
        "    def next(self):\n",
        "        # Check if an order is pending ... if yes, we cannot send a 2nd one\n",
        "        if self.order:\n",
        "            return\n",
        "         # # Check if we are in the market\n",
        "        if not self.position:\n",
        "          cash = self.broker.getcash()\n",
        "          position_size = (self.params.margin * self.params.leverage) / self.data.close[0]\n",
        "          # Long signal\n",
        "          if self.predicted_value[0] > self.params.buy_threshold:\n",
        "            self.log('LONG POSITION CREATED, %.2f' % self.datas[0].close[0])\n",
        "            self.order = self.buy(size=position_size)\n",
        "\n",
        "          # Short signal\n",
        "          elif self.predicted_value[0] < self.params.sell_threshold:\n",
        "            self.log('SHORT POSITION CREATED, %.2f' % self.datas[0].close[0])\n",
        "            self.order = self.sell(size=position_size)\n",
        "\n",
        "        else:\n",
        "          if self.predicted_value[0] > self.params.buy_threshold and self.position.size < 0:\n",
        "            self.log('CLOSE SHORT POSITION CREATED, %.2f' % self.datas[0].close[0])\n",
        "            self.order = self.close()\n",
        "          elif self.predicted_value[0] < self.params.sell_threshold and self.position.size > 0:\n",
        "            self.log('CLOSE LONG POSITION CREATED, %.2f' % self.datas[0].close[0])\n",
        "            self.order = self.close()\n",
        "\n",
        "    def notify_order(self, order):\n",
        "        if order.status in [order.Submitted, order.Accepted]:\n",
        "            # Buy/Sell order submitted/accepted to/by broker - Nothing to do\n",
        "            return\n",
        "        # Check if an order has been completed\n",
        "        # Attention: broker could reject order if not enough cash\n",
        "        if order.status in [order.Completed]:\n",
        "            if order.isbuy():\n",
        "                self.log('BUY EXECUTED, %.2f' % order.executed.price)\n",
        "            elif order.issell():\n",
        "                self.log('SELL EXECUTED, %.2f' % order.executed.price)\n",
        "        elif order.status in [order.Canceled, order.Margin, order.Rejected]:\n",
        "            if order.status == order.Canceled:\n",
        "                self.log('Order Canceled')\n",
        "            elif order.status == order.Margin:\n",
        "                self.log(f'Order Margin Not Enough - Available cash: {self.broker.getcash()}')\n",
        "            elif order.status == order.Rejected:\n",
        "                self.log('Order Rejected')\n",
        "\n",
        "        # Write down: no pending order\n",
        "        self.order = None\n",
        "\n",
        "    def notify_trade(self, trade):\n",
        "        if not trade.isclosed:\n",
        "            return\n",
        "        self.log(f'TRADE COMPLETED, GROSS {trade.pnl:.2f}, NET {trade.pnlcomm:.2f}, Available Cash {self.broker.getcash():.2f}')\n",
        "\n",
        "    def log(self, txt, dt=None):\n",
        "        if not self.params.log:\n",
        "          return\n",
        "        dt = dt or self.datas[0].datetime.date(0)\n",
        "        time = self.datas[0].datetime.time()\n",
        "        print(f'{dt.isoformat()} {time.isoformat()}, {txt}')\n"
      ],
      "metadata": {
        "id": "SvTq9isY3ctm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extend PandasData to include the custom column\n",
        "class CustomPandasData(bt.feeds.PandasData):\n",
        "    # Add custom columns\n",
        "    lines = ('predicted_value',)  # Add the custom line\n",
        "    params = (\n",
        "        ('predicted_value', 'Predicted_Value'),  # Map the column name\n",
        "    )\n"
      ],
      "metadata": {
        "id": "gRqdeATXlKaO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dict_even(data):\n",
        "    \"\"\"\n",
        "    Recursively makes all numeric values in a dictionary even.\n",
        "\n",
        "    Args:\n",
        "        data (dict): The input dictionary (can have nested dictionaries or lists).\n",
        "\n",
        "    Returns:\n",
        "        dict: A new dictionary with all numeric values made even.\n",
        "    \"\"\"\n",
        "    if isinstance(data, dict):\n",
        "        new_dict = {}\n",
        "        for key, value in data.items():\n",
        "            new_dict[key] = make_dict_even(value)\n",
        "        return new_dict\n",
        "    elif isinstance(data, list):\n",
        "        return [make_dict_even(item) for item in data]\n",
        "    elif isinstance(data, (int, float)):\n",
        "        if isinstance(data, int):\n",
        "            return data if data % 2 == 0 else data + 1\n",
        "        else: # float. We will round to an int, and then make even.\n",
        "            int_value = round(data)\n",
        "            return int_value if int_value % 2 == 0 else int_value + 1\n",
        "\n",
        "    else:\n",
        "        return data  # Return non-numeric values as they are"
      ],
      "metadata": {
        "id": "Y1RTvfAxGxYu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_cloud_storage_path(bucket_name, local_file_name):\n",
        "    \"\"\"\n",
        "    Returns the cloud storage path for a given model name.\n",
        "\n",
        "    Parameters:\n",
        "        local_file_name (str): The name of the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The cloud storage path. (e.g., gs://<bucket_name>/trade/models/<model_name>.h5).\n",
        "    \"\"\"\n",
        "    return f'gs://{bucket_name}/trade/models/{local_file_name}'\n",
        "\n",
        "def save_model_to_cloud_storage(model: tf.keras.Model, model_name: str, bucket_name: str):\n",
        "    \"\"\"\n",
        "    Saves a TensorFlow model to Google Cloud Storage and returns the cloud storage file path.\n",
        "\n",
        "    Parameters:\n",
        "        model (tf.keras.Model): The TensorFlow model to save.\n",
        "        model_name (str): The name of the model (used to create the file name).\n",
        "        bucket_name (str): The name of the Google Cloud Storage bucket.\n",
        "\n",
        "    Returns:\n",
        "        str: The cloud storage file path (e.g., gs://<bucket_name>/trade/models/<model_name>.h5).\n",
        "    \"\"\"\n",
        "    # Define the local and cloud storage file paths\n",
        "    local_file_name = f'{model_name}.h5'\n",
        "    local_file_path = f'/tmp/{local_file_name}'\n",
        "    cloud_file_path = get_model_cloud_storage_path(bucket_name, local_file_name)\n",
        "\n",
        "    # Save the model locally\n",
        "    model.save(local_file_path)\n",
        "\n",
        "    try:\n",
        "        # Upload the model to Google Cloud Storage\n",
        "        subprocess.run(['gsutil', 'cp', local_file_path, cloud_file_path], check=True)\n",
        "        print(f\"Model saved to {cloud_file_path}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle errors during the upload process\n",
        "        raise RuntimeError(f\"Failed to upload model to Google Cloud Storage: {e}\")\n",
        "    finally:\n",
        "        # Clean up the local file\n",
        "        if os.path.exists(local_file_path):\n",
        "            os.remove(local_file_path)\n",
        "    return cloud_file_path\n",
        "\n",
        "def load_model_from_cloud_storage(model_name: str, bucket_name: str):\n",
        "    \"\"\"\n",
        "    Loads a TensorFlow model from Google Cloud Storage.\n",
        "\n",
        "    Parameters:\n",
        "        model_name (str): The name of the model (used to create the file name).\n",
        "        bucket_name (str): The name of the Google Cloud Storage bucket.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: The loaded TensorFlow model.\n",
        "    \"\"\"\n",
        "    # Define the local and cloud storage file paths\n",
        "    local_file_name = f'{model_name}.h5'\n",
        "    local_file_path = f'/tmp/{local_file_name}'\n",
        "    cloud_file_path = get_model_cloud_storage_path(bucket_name, local_file_name)\n",
        "\n",
        "    try:\n",
        "        # Download the model from Google Cloud Storage\n",
        "        subprocess.run(['gsutil', 'cp', cloud_file_path, local_file_path], check=True)\n",
        "\n",
        "        # Load the model\n",
        "        model = tf.keras.models.load_model(local_file_path)\n",
        "        print(f\"Model loaded from {cloud_file_path}\")\n",
        "        return model\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        raise FileNotFoundError(f\"Model not found in Google Cloud Storage: {e}\")\n",
        "    finally:\n",
        "        # Clean up the local file\n",
        "        if os.path.exists(local_file_path):\n",
        "            os.remove(local_file_path)\n"
      ],
      "metadata": {
        "id": "8eAPe1BN4bO-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_firestore(project_id):\n",
        "    \"\"\"\n",
        "    Initialize the Firestore client using the service account key.\n",
        "\n",
        "    Parameters:\n",
        "        project_id (str): The Google Cloud project ID.\n",
        "\n",
        "    Returns:\n",
        "        firestore.Client: Initialized Firestore client.\n",
        "    \"\"\"\n",
        "    if not firebase_admin._apps:\n",
        "        # cred = credentials.Certificate(service_account_key_path)\n",
        "        cred = firebase_admin.credentials.ApplicationDefault()\n",
        "        cred._project_id = project_id  # Add this line\n",
        "        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project_id\n",
        "        firebase_admin.initialize_app(cred, {'projectId': project_id})\n",
        "    return firestore.client()\n",
        "\n",
        "\n",
        "def save_to_firestore(project_id, collection_name, data):\n",
        "    \"\"\"\n",
        "    Save data to a Firestore collection.\n",
        "\n",
        "    Parameters:\n",
        "        project_id (str): The Google Cloud project ID.\n",
        "        service_account_key_path (str): Path to the Firebase service account key JSON file.\n",
        "        collection_name (str): Name of the Firestore collection.\n",
        "        data (dict): Data to save in the document.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Initialize Firestore client\n",
        "    db = initialize_firestore(project_id)\n",
        "\n",
        "    # Save data to Firestore\n",
        "    doc_ref = db.collection(collection_name).document()\n",
        "    doc_ref.set(data)\n",
        "    print(f\"Data saved to Firestore: Collection={collection_name}, Document ID={doc_ref.id}\")"
      ],
      "metadata": {
        "id": "ySNwgm17Ce54"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "options = {\n",
        "  'timeframe': '15m',\n",
        "  'symbol': 'BTCUSDT',\n",
        "  'exchange': 'binance',\n",
        "  'start_date': '01-01-2024',\n",
        "  'end_date': '28-02-2025' ,\n",
        "  'pivot_windows': 10\n",
        "}"
      ],
      "metadata": {
        "id": "i2eIpKkrnB80"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_model(**options):\n",
        "  # Extract options\n",
        "  timeframe = options['timeframe'] if 'timeframe' in options else '15m'\n",
        "  symbol = options['symbol'] if 'symbol' in options else 'BTCUSDT'\n",
        "  exchange = options['exchange'] if 'exchange' in options else 'binance'\n",
        "  start_date = options['start_date'] if 'start_date' in options else '01-01-2024'\n",
        "  end_date = options['end_date'] if 'end_date' in options else '28-02-2025'\n",
        "  pivot_windows = options['pivot_windows'] if 'pivot_windows' in options else 10\n",
        "\n",
        "  # Get raw candles\n",
        "  data = get_all_binance_candles(symbol, timeframe, start_date, end_date)\n",
        "  # Add pivots\n",
        "  data = add_pivots(data, int(pivot_windows))\n",
        "  data['Pivot'] = np.where(data['Higher_Pivot'] == 1, -1, np.where(data['Lower_Pivot'] == 1, 1, 0))\n",
        "  del data['Higher_Pivot']\n",
        "  del data['Lower_Pivot']\n",
        "  # Calculate pivot proximity\n",
        "  data = calculate_pivot_proximity(data)\n",
        "  # Declare feature columns\n",
        "  features_columns = []\n",
        "\n",
        "  # # Add technical indicators on lower timeframe\n",
        "  lower_timeframe = timeframe\n",
        "  lower_timeframe_prefix = f\"{lower_timeframe}_\"\n",
        "  add_scaled_rsi(data, 14, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_rsi(data, 6, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(data, 5, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(data, 21, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_sma(data, 50, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_macd(data, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_bbands(data, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "\n",
        "  # Add technical indicators on middle timeframe\n",
        "  middle_timeframe = '1h'\n",
        "  middle_timeframe_prefix = f\"{middle_timeframe}_\"\n",
        "  middle_data = resample_candles(data, middle_timeframe)\n",
        "  add_scaled_rsi(middle_data, 14, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_rsi(middle_data, 6, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(middle_data, 5, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(middle_data, 21, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_macd(middle_data, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_bbands(middle_data, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "\n",
        "  # Add technical indicators on higher timeframe\n",
        "  higher_timeframe = '4h'\n",
        "  higher_timeframe_prefix = f\"{higher_timeframe}_\"\n",
        "  higher_data = resample_candles(data, higher_timeframe)\n",
        "  add_scaled_rsi(higher_data, 14, prefix=higher_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_rsi(higher_data, 6, prefix=higher_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(higher_data, 5, prefix=higher_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(higher_data, 21, prefix=higher_timeframe_prefix, features_columns=features_columns)\n",
        "\n",
        "  # Merge middle and higher timeframe data to lower timeframe data\n",
        "  data = merge_candlesticks_data(data, middle_data)\n",
        "  data = merge_candlesticks_data(data, higher_data)\n",
        "\n",
        "  # Drop rows with NaN values (due to rolling calculations)\n",
        "  data.dropna(inplace=True)\n",
        "\n",
        "  # delete Pivot_Porximity that have zero at the beginning and ending of the dataframe\n",
        "  non_zero_indices = data[data['Pivot_Proximity'] != 0].index\n",
        "  non_zero_at_begining = non_zero_indices[0]\n",
        "  non_zero_at_end = non_zero_indices[-1]\n",
        "  data = data.loc[non_zero_at_begining:non_zero_at_end]\n",
        "\n",
        "  # select only first 80 % of the data\n",
        "  training_data_ratio = 0.8 # 80%\n",
        "  training_data = data[:int(len(data) * training_data_ratio)]\n",
        "  training_data_start_date = training_data.index[0]\n",
        "  training_data_end_date = training_data.index[-1]\n",
        "\n",
        "  # Create training sequences\n",
        "  seq_length = 100\n",
        "  target_col = 'Pivot_Proximity'\n",
        "  X, y = create_sequences(training_data, features_columns, target_col, seq_length)\n",
        "\n",
        "  # Split into training and testing sets\n",
        "  train_validate_split_ratio = 0.2\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=train_validate_split_ratio, shuffle=False)\n",
        "  train_candles_length = len(X_train)\n",
        "  val_candles_length = len(X_test)\n",
        "\n",
        "  # Build the LSTM model\n",
        "  model_input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "  model = Sequential([\n",
        "      LSTM(100, return_sequences=True, input_shape=model_input_shape),\n",
        "      Dropout(0.2),\n",
        "      LSTM(50, return_sequences=False),\n",
        "      Dropout(0.2),\n",
        "      Dense(50, activation='relu'),\n",
        "      Dropout(0.2),\n",
        "      Dense(25, activation='relu'),\n",
        "      Dropout(0.2),\n",
        "      Dense(10, activation='relu'),\n",
        "      Dropout(0.2),\n",
        "      Dense(1, activation='tanh')\n",
        "  ])\n",
        "  model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "  # Define early stopping\n",
        "  custom_early_stopping = CustomEarlyStopping(patience=3)\n",
        "\n",
        "  # Step 5: Train the model\n",
        "  train_epochs = 20\n",
        "  history = model.fit(\n",
        "      X_train, y_train,\n",
        "      epochs=train_epochs,\n",
        "      batch_size=32,\n",
        "      validation_data=(X_test, y_test),\n",
        "      verbose=1,\n",
        "      callbacks=[custom_early_stopping]\n",
        "  )\n",
        "  final_epoch = len(history.history['loss'])\n",
        "  train_loss = history.history['loss'][-1]\n",
        "  train_mae = history.history['mae'][-1]\n",
        "  val_loss = history.history['val_loss'][-1]\n",
        "  val_mae = history.history['val_mae'][-1]\n",
        "\n",
        "  # test data\n",
        "  test_data = data[int(len(data) * training_data_ratio):]\n",
        "\n",
        "  # Predict in batches\n",
        "  batch_size = 128\n",
        "  predict_in_batches(model, test_data, features_columns, seq_length, batch_size)\n",
        "  test_data.dropna(inplace=True) # Early predicted values wont be availble due to sequencing\n",
        "  test_data_start_date = test_data.index[0]\n",
        "  test_data_end_date = test_data.index[-1]\n",
        "\n",
        "  # backtest configuration\n",
        "  trade_leverage = 10\n",
        "  trade_margin = 1000\n",
        "  trade_buy_threshold = 0.6\n",
        "  trade_sell_threshold = -0.6\n",
        "  broker_commision = 0.02 # In percentage\n",
        "\n",
        "  # Load data into a Pandas DataFrame\n",
        "  backtest_data = CustomPandasData(\n",
        "      dataname=test_data,\n",
        "      datetime=None,  # Use the index as the datetime\n",
        "      open='Open',         # Column index for Open\n",
        "      high='High',         # Column index for High\n",
        "      low='Low',          # Column index for Low\n",
        "      close='Close',        # Column index for Close\n",
        "      volume='Volume',       # Column index for Volume\n",
        "      openinterest=None,# No open interest column\n",
        "      predicted_value='Predicted_Value'  # Column index for Predicted_Value\n",
        "  )\n",
        "\n",
        "  # Create a Cerebro engine instance\n",
        "  cerebro = bt.Cerebro()\n",
        "\n",
        "  # Add the strategy\n",
        "  cerebro.addstrategy(\n",
        "      PredictedValueStrategy,\n",
        "      buy_threshold=trade_buy_threshold,\n",
        "      sell_threshold=trade_sell_threshold,\n",
        "      leverage=trade_leverage,\n",
        "      margin=trade_margin,\n",
        "      log=False\n",
        "  )\n",
        "\n",
        "  # Add the data feed\n",
        "  cerebro.adddata(backtest_data)\n",
        "\n",
        "  # Set the initial cash\n",
        "  cerebro.broker.set_cash(10000.0)\n",
        "\n",
        "  # Set the commission\n",
        "  cerebro.broker.setcommission(commission=broker_commision / 100)\n",
        "\n",
        "  # Add analyzers\n",
        "  cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe')\n",
        "  cerebro.addanalyzer(bt.analyzers.DrawDown, _name='drawdown')\n",
        "  cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name='tradeanalyzer')\n",
        "  cerebro.addanalyzer(bt.analyzers.Returns, _name='returns')\n",
        "  cerebro.addanalyzer(bt.analyzers.PyFolio, _name='pyfolio')\n",
        "\n",
        "  # Run the backtest\n",
        "  starting_portfolio_value = cerebro.broker.getvalue()\n",
        "  print('Starting Portfolio Value: %.2f' % starting_portfolio_value)\n",
        "  backtest_result = cerebro.run()\n",
        "  final_portfolio_value = cerebro.broker.getvalue()\n",
        "  print('Final Portfolio Value: %.2f' % final_portfolio_value)\n",
        "\n",
        "  # Capture analysis\n",
        "  strat = backtest_result[0]\n",
        "\n",
        "  trade_analysis = make_dict_even(strat.analyzers.tradeanalyzer.get_analysis())\n",
        "  # Errors of large numbers, fix\n",
        "  trade_analysis['len']['short']['lost']['min'] = str(trade_analysis['len']['short']['lost']['min'])\n",
        "  trade_analysis['len']['short']['lost']['max'] = str(trade_analysis['len']['short']['lost']['max'])\n",
        "  trade_analysis['len']['short']['won']['min'] = str(trade_analysis['len']['short']['won']['min'])\n",
        "  trade_analysis['len']['short']['won']['max'] = str(trade_analysis['len']['short']['won']['max'])\n",
        "  trade_analysis['len']['long']['lost']['min'] = str(trade_analysis['len']['long']['lost']['min'])\n",
        "  trade_analysis['len']['long']['lost']['max'] = str(trade_analysis['len']['long']['lost']['max'])\n",
        "  trade_analysis['len']['long']['won']['min'] = str(trade_analysis['len']['long']['won']['min'])\n",
        "  trade_analysis['len']['long']['won']['max'] = str(trade_analysis['len']['long']['won']['max'])\n",
        "\n",
        "  drawdown_analysis = make_dict_even(strat.analyzers.drawdown.get_analysis())\n",
        "  sharpe_analysis = make_dict_even(strat.analyzers.sharpe.get_analysis())\n",
        "\n",
        "  # Save the model\n",
        "  todays_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "  model_name = f\"binance_mtf_{timeframe}_{start_date}_{end_date}_{todays_date}\"\n",
        "  model_path = save_model_to_cloud_storage(model, model_name, bucket_name)\n",
        "  analysis_result = {\n",
        "      'timeframe': timeframe,\n",
        "      'start_date': start_date,\n",
        "      'end_date': end_date,\n",
        "      'symbol': symbol,\n",
        "      'exchange': exchange,\n",
        "      'features_columns': features_columns,\n",
        "      'lower_timeframe': lower_timeframe,\n",
        "      'middle_timeframe': middle_timeframe,\n",
        "      'higher_timeframe': higher_timeframe,\n",
        "      'seq_length': seq_length,\n",
        "      'training_data_ratio': training_data_ratio,\n",
        "      'epochs': train_epochs,\n",
        "      'model_input_shape': model_input_shape,\n",
        "      'model_name': model_name,\n",
        "      'model_path': model_path,\n",
        "      'train_loss': train_loss,\n",
        "      'train_mae': train_mae,\n",
        "      'val_loss': val_loss,\n",
        "      'val_mae': val_mae,\n",
        "      'final_epoch': final_epoch,\n",
        "      'training_data_start_date': training_data_start_date,\n",
        "      'training_data_end_date': training_data_end_date,\n",
        "      'test_data_start_date': test_data_start_date,\n",
        "      'test_data_end_date': test_data_end_date,\n",
        "      'trade_leverage': trade_leverage,\n",
        "      'trade_margin': trade_margin,\n",
        "      'trade_buy_threshold': trade_buy_threshold,\n",
        "      'trade_sell_threshold': trade_sell_threshold,\n",
        "      'starting_portfolio_value': starting_portfolio_value,\n",
        "      'final_portfolio_value': final_portfolio_value,\n",
        "      'broker_commision': broker_commision,\n",
        "      'trade_analysis': trade_analysis,\n",
        "      'drawdown_analysis': drawdown_analysis,\n",
        "      'sharpe_analysis': sharpe_analysis,\n",
        "  }\n",
        "  save_to_firestore(project_id, firestore_collection_name, analysis_result)\n",
        "  return analysis_result"
      ],
      "metadata": {
        "id": "EjeV2AzvnY1Q"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_model(**options)"
      ],
      "metadata": {
        "id": "xyyAezm9s7li",
        "outputId": "c8a6dddc-7540-4bb7-bf83-e08c00f85410",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 83ms/step - loss: 0.3510 - mae: 0.5090 - val_loss: 0.2740 - val_mae: 0.4491\n",
            "Epoch 2/20\n",
            "\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 80ms/step - loss: 0.2903 - mae: 0.4564 - val_loss: 0.2542 - val_mae: 0.4289\n",
            "Epoch 3/20\n",
            "\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 81ms/step - loss: 0.2803 - mae: 0.4476 - val_loss: 0.2532 - val_mae: 0.4278\n",
            "Epoch 4/20\n",
            "\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 81ms/step - loss: 0.2794 - mae: 0.4457 - val_loss: 0.2683 - val_mae: 0.4413\n",
            "Epoch 5/20\n",
            "\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 81ms/step - loss: 0.2752 - mae: 0.4419 - val_loss: 0.2565 - val_mae: 0.4295\n",
            "Epoch 6/20\n",
            "\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 79ms/step - loss: 0.2653 - mae: 0.4306 - val_loss: 0.2280 - val_mae: 0.4016\n",
            "Epoch 7/20\n",
            "\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 81ms/step - loss: 0.2552 - mae: 0.4184 - val_loss: 0.2266 - val_mae: 0.4005\n",
            "Epoch 8/20\n",
            "\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 79ms/step - loss: 0.2493 - mae: 0.4114 - val_loss: 0.2247 - val_mae: 0.3943\n",
            "Epoch 9/20\n",
            "\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 81ms/step - loss: 0.2443 - mae: 0.4080 - val_loss: 0.2210 - val_mae: 0.3881\n",
            "Epoch 10/20\n",
            "\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 79ms/step - loss: 0.2429 - mae: 0.4053 - val_loss: 0.2277 - val_mae: 0.3989\n",
            "Epoch 11/20\n",
            "\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 81ms/step - loss: 0.2374 - mae: 0.3995 - val_loss: 0.2250 - val_mae: 0.3959\n",
            "Epoch 12/20\n",
            "\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.2398 - mae: 0.4024\n",
            "Early stopping: Validation MAE increased 3 times in a row.\n",
            "\u001b[1m805/805\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 80ms/step - loss: 0.2398 - mae: 0.4024 - val_loss: 0.2202 - val_mae: 0.3924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-54e86c8c4e9a>:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['Predicted_Value'] = predicted_values\n",
            "<ipython-input-27-ee9412ddd521>:125: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data.dropna(inplace=True) # Early predicted values wont be availble due to sequencing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Portfolio Value: 10000.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Portfolio Value: 23482.39\n",
            "Model saved to gs://ife-storage/trade/models/binance_mtf_15m_01-01-2024_28-02-2025_2025-03-08.h5\n",
            "Data saved to Firestore: Collection=trade-models, Document ID=q6AMxOT6HTi60oRIgjWC\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'timeframe': '15m',\n",
              " 'start_date': '01-01-2024',\n",
              " 'end_date': '28-02-2025',\n",
              " 'symbol': 'BTCUSDT',\n",
              " 'exchange': 'binance',\n",
              " 'features_columns': ['15m_RSI_14',\n",
              "  '15m_RSI_6',\n",
              "  '15m_EMA_5',\n",
              "  '15m_EMA_21',\n",
              "  '15m_SMA_50',\n",
              "  '15m_MACD_12_26_9',\n",
              "  '15m_MACDs_12_26_9',\n",
              "  '15m_MACDh_12_26_9',\n",
              "  '15m_BBL_20_2.0',\n",
              "  '15m_BBM_20_2.0',\n",
              "  '15m_BBU_20_2.0',\n",
              "  '15m_BBB_20_2.0',\n",
              "  '15m_BBP_20_2.0',\n",
              "  '1h_RSI_14',\n",
              "  '1h_RSI_6',\n",
              "  '1h_EMA_5',\n",
              "  '1h_EMA_21',\n",
              "  '1h_MACD_12_26_9',\n",
              "  '1h_MACDs_12_26_9',\n",
              "  '1h_MACDh_12_26_9',\n",
              "  '1h_BBL_20_2.0',\n",
              "  '1h_BBM_20_2.0',\n",
              "  '1h_BBU_20_2.0',\n",
              "  '1h_BBB_20_2.0',\n",
              "  '1h_BBP_20_2.0',\n",
              "  '4h_RSI_14',\n",
              "  '4h_RSI_6',\n",
              "  '4h_EMA_5',\n",
              "  '4h_EMA_21'],\n",
              " 'lower_timeframe': '15m',\n",
              " 'middle_timeframe': '1h',\n",
              " 'higher_timeframe': '4h',\n",
              " 'seq_length': 100,\n",
              " 'training_data_ratio': 0.8,\n",
              " 'epochs': 20,\n",
              " 'model_input_shape': (100, 29),\n",
              " 'model_name': 'binance_mtf_15m_01-01-2024_28-02-2025_2025-03-08',\n",
              " 'model_path': 'gs://ife-storage/trade/models/binance_mtf_15m_01-01-2024_28-02-2025_2025-03-08.h5',\n",
              " 'train_loss': 0.2384079098701477,\n",
              " 'train_mae': 0.400396466255188,\n",
              " 'val_loss': 0.22022654116153717,\n",
              " 'val_mae': 0.39244627952575684,\n",
              " 'final_epoch': 12,\n",
              " 'training_data_start_date': Timestamp('2024-01-04 08:00:00'),\n",
              " 'training_data_end_date': Timestamp('2024-12-05 17:45:00'),\n",
              " 'test_data_start_date': Timestamp('2024-12-06 19:00:00'),\n",
              " 'test_data_end_date': Timestamp('2025-02-27 20:30:00'),\n",
              " 'trade_leverage': 10,\n",
              " 'trade_margin': 1000,\n",
              " 'trade_buy_threshold': 0.6,\n",
              " 'trade_sell_threshold': -0.6,\n",
              " 'starting_portfolio_value': 10000.0,\n",
              " 'final_portfolio_value': 23482.390119049604,\n",
              " 'broker_commision': 0.02,\n",
              " 'trade_analysis': {'total': {'total': 46, 'open': 2, 'closed': 46},\n",
              "  'streak': {'won': {'current': 8, 'longest': 18},\n",
              "   'lost': {'current': 0, 'longest': 2}},\n",
              "  'pnl': {'gross': {'total': 12372, 'average': 276},\n",
              "   'net': {'total': 12192, 'average': 272}},\n",
              "  'won': {'total': 42, 'pnl': {'total': 12848, 'average': 314, 'max': 1120}},\n",
              "  'lost': {'total': 4, 'pnl': {'total': -654, 'average': -164, 'max': -338}},\n",
              "  'long': {'total': 22,\n",
              "   'pnl': {'total': 6110,\n",
              "    'average': 278,\n",
              "    'won': {'total': 6408, 'average': 320, 'max': 1002},\n",
              "    'lost': {'total': -296, 'average': -148, 'max': -294}},\n",
              "   'won': 20,\n",
              "   'lost': 2},\n",
              "  'short': {'total': 24,\n",
              "   'pnl': {'total': 6082,\n",
              "    'average': 264,\n",
              "    'won': {'total': 6440, 'average': 308, 'max': 1120},\n",
              "    'lost': {'total': -358, 'average': -178, 'max': -338}},\n",
              "   'won': 22,\n",
              "   'lost': 2},\n",
              "  'len': {'total': 5882,\n",
              "   'average': 132,\n",
              "   'max': 612,\n",
              "   'min': 10,\n",
              "   'won': {'total': 5160, 'average': 126, 'max': 612, 'min': 10},\n",
              "   'lost': {'total': 722, 'average': 180, 'max': 560, 'min': 36},\n",
              "   'long': {'total': 2274,\n",
              "    'average': 104,\n",
              "    'max': 394,\n",
              "    'min': 10,\n",
              "    'won': {'total': 2148, 'average': 108, 'max': '394', 'min': '10'},\n",
              "    'lost': {'total': 128, 'average': 64, 'max': '76', 'min': '52'}},\n",
              "   'short': {'total': 3608,\n",
              "    'average': 158,\n",
              "    'max': 612,\n",
              "    'min': 18,\n",
              "    'won': {'total': 3012, 'average': 144, 'max': '612', 'min': '18'},\n",
              "    'lost': {'total': 596, 'average': 298, 'max': '560', 'min': '36'}}}},\n",
              " 'drawdown_analysis': {'len': 2,\n",
              "  'drawdown': 0,\n",
              "  'moneydown': 52,\n",
              "  'max': {'len': 646, 'drawdown': 6, 'moneydown': 940}},\n",
              " 'sharpe_analysis': {'sharperatio': 6}}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}