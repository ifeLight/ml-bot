{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQlex19Wp09aIj6ZUyhkFU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ifeLight/ml-bot/blob/main/binance-multi-timeframe-grade-opt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas-ta\n",
        "!pip install backtrader[plotting]\n",
        "!pip install plotly\n",
        "!pip install --upgrade firebase-admin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZkp-JVxXjc6",
        "outputId": "4ff9ee3b-5e33-4da8-bfa5-39ef2e710346"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas-ta\n",
            "  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m112.6/115.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from pandas-ta) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas-ta) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas-ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas-ta) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas-ta) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->pandas-ta) (1.17.0)\n",
            "Building wheels for collected packages: pandas-ta\n",
            "  Building wheel for pandas-ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas-ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218909 sha256=11e10f9b58df17b6ae74b4007f8a396eb72e6bc314e15c84007ccf1090927b42\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/33/8b/50b245c5c65433cd8f5cb24ac15d97e5a3db2d41a8b6ae957d\n",
            "Successfully built pandas-ta\n",
            "Installing collected packages: pandas-ta\n",
            "Successfully installed pandas-ta-0.3.14b0\n",
            "Collecting backtrader[plotting]\n",
            "  Downloading backtrader-1.9.78.123-py2.py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from backtrader[plotting]) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->backtrader[plotting]) (1.17.0)\n",
            "Downloading backtrader-1.9.78.123-py2.py3-none-any.whl (419 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.5/419.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: backtrader\n",
            "Successfully installed backtrader-1.9.78.123\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Requirement already satisfied: firebase-admin in /usr/local/lib/python3.11/dist-packages (6.6.0)\n",
            "Collecting firebase-admin\n",
            "  Downloading firebase_admin-6.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: cachecontrol>=0.12.14 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (0.14.2)\n",
            "Requirement already satisfied: google-api-python-client>=1.7.8 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (2.160.0)\n",
            "Requirement already satisfied: google-cloud-storage>=1.37.1 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (2.19.0)\n",
            "Requirement already satisfied: pyjwt>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.5.0->firebase-admin) (2.10.1)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=1.22.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-firestore>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (2.20.1)\n",
            "Requirement already satisfied: requests>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from cachecontrol>=0.12.14->firebase-admin) (2.32.3)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from cachecontrol>=0.12.14->firebase-admin) (1.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.69.1)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (4.25.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (2.38.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.62.3)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (4.1.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-firestore>=2.19.0->firebase-admin) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage>=1.37.1->firebase-admin) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage>=1.37.1->firebase-admin) (1.6.0)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.5.0->firebase-admin) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.5.0->firebase-admin) (1.17.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client>=1.7.8->firebase-admin) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16.0->cachecontrol>=0.12.14->firebase-admin) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16.0->cachecontrol>=0.12.14->firebase-admin) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16.0->cachecontrol>=0.12.14->firebase-admin) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16.0->cachecontrol>=0.12.14->firebase-admin) (2025.1.31)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.5.0->firebase-admin) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (0.6.1)\n",
            "Downloading firebase_admin-6.7.0-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.0/134.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: firebase-admin\n",
            "  Attempting uninstall: firebase-admin\n",
            "    Found existing installation: firebase-admin 6.6.0\n",
            "    Uninstalling firebase-admin-6.6.0:\n",
            "      Successfully uninstalled firebase-admin-6.6.0\n",
            "Successfully installed firebase-admin-6.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import backtrader as bt\n",
        "import pandas_ta as ta\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import subprocess\n",
        "import firebase_admin\n",
        "from firebase_admin import firestore\n",
        "from requests import Request, Session\n",
        "from requests.exceptions import ConnectionError, Timeout, TooManyRedirects\n",
        "from google.colab import auth\n",
        "import google.auth\n",
        "import itertools\n",
        "import hashlib\n",
        "import gc #garbage collection"
      ],
      "metadata": {
        "id": "dtUTSBApSZe4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "#Configure Google cloud project\n",
        "project_id = 'ifelight'\n",
        "!gcloud config set project {project_id}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELE8dAggI0FQ",
        "outputId": "465b7cc0-926f-4a32-ee1a-72e51c18a459"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Working GCP Bucket name\n",
        "bucket_name = 'ife-storage'\n",
        "# Working on Firestore name\n",
        "firestore_collection_name = 'trade-models'"
      ],
      "metadata": {
        "id": "Fs_TurcmdJql"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binance_base_url = 'https://52on3577u3.execute-api.eu-central-1.amazonaws.com'\n",
        "\n",
        "def get_binance_candles(symbol: str, interval='1h', limit: int = 50, **kwargs):\n",
        "    url = f'{binance_base_url}/api/v3/uiKlines?symbol={symbol}&interval={interval}&limit={limit}'\n",
        "    for key, value in kwargs.items():\n",
        "        url += f'&{key}={value}'\n",
        "    response = requests.get(url)\n",
        "    result = json.loads(response.text)\n",
        "    # print(result)\n",
        "    def map_result(x):\n",
        "        return {\n",
        "            'Date': x[0],\n",
        "            'Open': x[1],\n",
        "            'High': x[2],\n",
        "            'Low': x[3],\n",
        "            'Close': x[4],\n",
        "            'Volume': x[5],\n",
        "        }\n",
        "    mappeded_result = []\n",
        "    for x in result:\n",
        "        mappeded_result.append(map_result(x))\n",
        "    return mappeded_result\n",
        "\n",
        "\n",
        "def candles_to_df(data):\n",
        "    df =  pd.DataFrame(data)\n",
        "    df['Date'] = pd.to_datetime(df['Date'], unit='ms')\n",
        "    df['Open'] = df['Open'].astype(float)\n",
        "    df['High'] = df['High'].astype(float)\n",
        "    df['Low'] = df['Low'].astype(float)\n",
        "    df['Close'] = df['Close'].astype(float)\n",
        "    df['Volume'] = df['Volume'].astype(float)\n",
        "    df.set_index('Date', inplace=True)\n",
        "    return df\n",
        "\n",
        "def get_all_binance_candles(symbol: str, interval='1h', start_date=None, end_date=None, limit=1000):\n",
        "  raw_start_date = start_date\n",
        "  raw_end_date = end_date\n",
        "  start_date = pd.to_datetime(start_date) if start_date else pd.to_datetime('2015-01-01')\n",
        "  end_date = pd.to_datetime(end_date) if end_date else pd.to_datetime('today')\n",
        "  try:\n",
        "    if end_date <= pd.to_datetime('today'):\n",
        "      return load_candles_from_cloud_storage(symbol, interval, start_date, end_date)\n",
        "  except FileNotFoundError:\n",
        "    pass\n",
        "  result = []\n",
        "  while True:\n",
        "    candles = get_binance_candles(symbol, interval, limit, startTime=int(start_date.timestamp() * 1000), endTime=int(end_date.timestamp() * 1000))\n",
        "    if len(candles) <= 1:\n",
        "      break;\n",
        "    result += candles\n",
        "    start_date = pd.to_datetime(datetime.datetime.fromtimestamp(candles[-1]['Date'] / 1000))\n",
        "  candles_df = candles_to_df(result)\n",
        "  save_candles_to_cloud_storage(candles_df, symbol, interval, raw_start_date, raw_end_date)\n",
        "  return candles_df\n",
        "\n",
        "def candles_storage_file_name(symbol: str, interval='1h', start_date=None, end_date=None):\n",
        "  file_name = f'binance_{symbol}_{interval}_{start_date}_{end_date}.csv'\n",
        "  return file_name\n",
        "\n",
        "def load_candles_from_cloud_storage(symbol: str, interval: str, start_date=None, end_date=None):\n",
        "  file_name = candles_storage_file_name(symbol, interval, start_date, end_date)\n",
        "  try:\n",
        "    # Download the file from cloud storage.\n",
        "    subprocess.run(['gsutil', 'cp', f'gs://{bucket_name}/trade/candles/{file_name}', f'/tmp/{file_name}'], check=True)\n",
        "\n",
        "    # Load the data into a Pandas DataFrame.\n",
        "    with open(f'/tmp/{file_name}', 'r') as f:\n",
        "      return pd.read_csv(f, index_col=0, parse_dates=True)\n",
        "  except subprocess.CalledProcessError:\n",
        "    # Raise a FileNotFoundError if the file is not found in cloud storage.\n",
        "    raise FileNotFoundError(f\"File not found: gs://{bucket_name}/trade/candles/{file_name}\")\n",
        "\n",
        "def save_candles_to_cloud_storage(df: pd.DataFrame, symbol: str, interval: str, start_date, end_date):\n",
        "  file_name = candles_storage_file_name(symbol, interval, start_date, end_date)\n",
        "  df.to_csv(f'/tmp/{file_name}')\n",
        "  !gsutil cp /tmp/{file_name} gs://{bucket_name}/trade/candles/{file_name}\n"
      ],
      "metadata": {
        "id": "r0DdxBTXLcao"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_random_hash(max_length=8):\n",
        "    \"\"\"\n",
        "    Generate a random hash of a maximum length.\n",
        "\n",
        "    Args:\n",
        "        max_length (int): Maximum length of the hash (default is 8).\n",
        "\n",
        "    Returns:\n",
        "        str: A random hash of the specified maximum length.\n",
        "    \"\"\"\n",
        "    # Generate a random byte sequence\n",
        "    random_bytes = os.urandom(16)  # 16 bytes of random data\n",
        "    # Create a SHA-256 hash of the random bytes\n",
        "    hash_object = hashlib.sha256(random_bytes)\n",
        "    # Get the hexadecimal representation of the hash\n",
        "    hex_hash = hash_object.hexdigest()\n",
        "    # Truncate the hash to the desired maximum length\n",
        "    return hex_hash[:max_length]"
      ],
      "metadata": {
        "id": "C92jSivqMCPe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_pivots(df, window=5):\n",
        "    \"\"\"\n",
        "    Calculate the probability of price going up and down based on higher and lower pivots.\n",
        "    \"\"\"\n",
        "    df['Higher_Pivot'] = df['High'].rolling(window=2 * window + 1, center=True).apply(\n",
        "        lambda x: 1 if x.iloc[window] == x.max() else 0, raw=False\n",
        "    )\n",
        "    df['Lower_Pivot'] = df['Low'].rolling(window=2 * window + 1, center=True).apply(\n",
        "        lambda x: 1 if x.iloc[window] == x.min() else 0, raw=False\n",
        "    )\n",
        "\n",
        "    # Step 2: Ensure no two successive pivots of the same type\n",
        "    pivot_type = None  # Tracks the type of the last pivot\n",
        "    last_pivot_index = None  # Tracks the index of the last pivot\n",
        "\n",
        "    # Remove duplicated index\n",
        "    df = df[~df.index.duplicated(keep='first')]\n",
        "\n",
        "    for i in range(len(df.index)):\n",
        "      if df.loc[df.index[i], 'Higher_Pivot'].item() == 1:\n",
        "          if pivot_type == 'higher':\n",
        "              # Compare current higher pivot with the last higher pivot\n",
        "              if df.loc[df.index[i], 'High'].item() > df.loc[df.index[last_pivot_index], 'High'].item():\n",
        "                  # Remove the last higher pivot\n",
        "                  df.loc[df.index[last_pivot_index], 'Higher_Pivot'] = 0\n",
        "                  # Update the last pivot\n",
        "                  last_pivot_index = i\n",
        "              else:\n",
        "                  # Remove the current higher pivot\n",
        "                  df.loc[df.index[i], 'Higher_Pivot'] = 0\n",
        "          else:\n",
        "              # Update pivot type and index\n",
        "              pivot_type = 'higher'\n",
        "              last_pivot_index = i\n",
        "\n",
        "      elif df.loc[df.index[i], 'Lower_Pivot'].item() == 1:\n",
        "          if pivot_type == 'lower':\n",
        "              # Compare current lower pivot with the last lower pivot\n",
        "              if df.loc[df.index[i], 'Low'].item() < df.loc[df.index[last_pivot_index], 'Low'].item():\n",
        "                  # Remove the last lower pivot\n",
        "                  df.loc[df.index[last_pivot_index], 'Lower_Pivot'] = 0\n",
        "                  # Update the last pivot\n",
        "                  last_pivot_index = i\n",
        "              else:\n",
        "                  # Remove the current lower pivot\n",
        "                  df.loc[df.index[i], 'Lower_Pivot'] = 0\n",
        "          else:\n",
        "              # Update pivot type and index\n",
        "              pivot_type = 'lower'\n",
        "              last_pivot_index = i\n",
        "    return df"
      ],
      "metadata": {
        "id": "551o_MKlZhr2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_pivot_proximity(df):\n",
        "    \"\"\"\n",
        "    Calculates the pivot proximity using a loop-based approach, finding the closest\n",
        "    previous and next pivots without generating intermediate lists of all pivots.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with 'Pivot' and 'Close' columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with 'Pivot_Proximity' column added.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    pivot_values = df['Pivot'].values\n",
        "    close_values = df['Close'].values\n",
        "    proximity_values = [0.0] * len(df)\n",
        "\n",
        "    for i in range(len(df)):\n",
        "      pivot = pivot_values[i]\n",
        "      if pivot == -1:\n",
        "        proximity_values[i] = -1.0\n",
        "      elif pivot == 1:\n",
        "        proximity_values[i] = 1.0\n",
        "      else:\n",
        "        closest_previous_pivot_index = None\n",
        "        for j in range(i - 1, -1, -1):\n",
        "          if pivot_values[j] != 0:\n",
        "            closest_previous_pivot_index = j\n",
        "            break\n",
        "\n",
        "        if closest_previous_pivot_index is not None:\n",
        "          closest_previous_pivot_value = pivot_values[closest_previous_pivot_index]\n",
        "          closest_previous_close = close_values[closest_previous_pivot_index]\n",
        "\n",
        "          closest_next_pivot_index = None\n",
        "          if closest_previous_pivot_value == -1:\n",
        "            for j in range(i + 1, len(df)):\n",
        "              if pivot_values[j] == 1:\n",
        "                closest_next_pivot_index = j\n",
        "                break\n",
        "          else:\n",
        "            for j in range(i + 1, len(df)):\n",
        "              if pivot_values[j] == -1:\n",
        "                closest_next_pivot_index = j\n",
        "                break\n",
        "\n",
        "          if closest_next_pivot_index is not None:\n",
        "            closest_next_close = close_values[closest_next_pivot_index]\n",
        "\n",
        "            distance_to_previous = abs(close_values[i] - closest_previous_close)\n",
        "            distance_to_next = abs(close_values[i] - closest_next_close)\n",
        "\n",
        "            if distance_to_previous + distance_to_next != 0:\n",
        "              if closest_previous_pivot_value == -1:\n",
        "                proximity_values[i] = (distance_to_previous - distance_to_next) / (distance_to_previous + distance_to_next)\n",
        "              else:\n",
        "                proximity_values[i] = (distance_to_next - distance_to_previous) / (distance_to_previous + distance_to_next)\n",
        "\n",
        "    df['Pivot_Proximity'] = proximity_values\n",
        "    return df"
      ],
      "metadata": {
        "id": "Ozb2WlKwCCOB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_scaled_rsi(df, window=14, prefix = '', features_columns=[]):\n",
        "  series = ta.rsi(df['Close'], length=window)\n",
        "  column_name = f\"{prefix}RSI_{window}\"\n",
        "  df[column_name] = series / 100\n",
        "  if(column_name not in features_columns):\n",
        "    features_columns.append(column_name)\n",
        "  return df\n",
        "\n",
        "def add_scaled_ema(df, window=50, prefix = '', features_columns=[]):\n",
        "  series = ta.ema(df['Close'], length=window)\n",
        "  column_name = f\"{prefix}EMA_{window}\"\n",
        "  df[column_name] = series / df['Close']\n",
        "  if column_name not in features_columns:\n",
        "    features_columns.append(column_name)\n",
        "  return df\n",
        "\n",
        "def add_scaled_sma(df, window=50, prefix = '', features_columns=[]):\n",
        "  series = ta.sma(df['Close'], length=window)\n",
        "  column_name = f\"{prefix}SMA_{window}\"\n",
        "  df[column_name] = series / df['Close']\n",
        "  if column_name not in features_columns:\n",
        "    features_columns.append(column_name)\n",
        "  return df\n",
        "\n",
        "def add_scaled_macd(df, prefix= '', features_columns=[], fast = 12, slow = 26, signal=9):\n",
        "  macd_df = ta.macd(df['Close'], fast=fast, slow=slow, signal=signal)\n",
        "  suffix = f\"{fast}_{slow}_{signal}\"\n",
        "  macd_column_name = f\"{prefix}MACD_{suffix}\"\n",
        "  macds_column_name = f\"{prefix}MACDs_{suffix}\"\n",
        "  macdh_column_name = f\"{prefix}MACDh_{suffix}\"\n",
        "  df[macd_column_name] = macd_df[macd_df.columns[0]] / df['Close']\n",
        "  df[macds_column_name] = macd_df[macd_df.columns[2]] / df['Close']\n",
        "  df[macdh_column_name] = macd_df[macd_df.columns[1]] / df['Close']\n",
        "  if macd_column_name not in features_columns:\n",
        "    features_columns.append(macd_column_name)\n",
        "  if macds_column_name not in features_columns:\n",
        "    features_columns.append(macds_column_name)\n",
        "  if macdh_column_name not in features_columns:\n",
        "    features_columns.append(macdh_column_name)\n",
        "  return df\n",
        "\n",
        "def add_scaled_bbands(df, window=20, std=2.0, prefix= '', features_columns=[]):\n",
        "  bbands_df = ta.bbands(df['Close'], length=window, std=std)\n",
        "  suffix = f\"{window}_{std}\"\n",
        "  bbl_column_name = f\"{prefix}BBL_{suffix}\"\n",
        "  bbm_column_name = f\"{prefix}BBM_{suffix}\"\n",
        "  bbu_column_name = f\"{prefix}BBU_{suffix}\"\n",
        "  bbb_column_name = f\"{prefix}BBB_{suffix}\"\n",
        "  bbp_column_name = f\"{prefix}BBP_{suffix}\"\n",
        "  df[bbl_column_name] = bbands_df[bbands_df.columns[0]] / df['Close']\n",
        "  df[bbm_column_name] = bbands_df[bbands_df.columns[1]] / df['Close']\n",
        "  df[bbu_column_name] = bbands_df[bbands_df.columns[2]] / df['Close']\n",
        "  df[bbb_column_name] = bbands_df[bbands_df.columns[3]]\n",
        "  df[bbp_column_name] = bbands_df[bbands_df.columns[4]]\n",
        "  if bbl_column_name not in features_columns: features_columns.append(bbl_column_name)\n",
        "  if bbm_column_name not in features_columns: features_columns.append(bbm_column_name)\n",
        "  if bbu_column_name not in features_columns: features_columns.append(bbu_column_name)\n",
        "  if bbb_column_name not in features_columns: features_columns.append(bbb_column_name)\n",
        "  if bbp_column_name not in features_columns: features_columns.append(bbp_column_name)\n",
        "  return df"
      ],
      "metadata": {
        "id": "Rp7SlrZXaupS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_candlesticks_data(df1, df2):\n",
        "  \"\"\"\n",
        "  Merges two candlestick DataFrames with forward fill, handling different timeframes,\n",
        "  and prevents duplicate OHLCV columns.\n",
        "  Ensure both DataFrames have a datetime index.\n",
        "  And ensure the second DataFrame is the larger timeframe\n",
        "\n",
        "  Args:\n",
        "      df1: First candlestick DataFrame with datetime index.\n",
        "      df2: Second candlestick DataFrame with datetime index.\n",
        "\n",
        "  Returns:\n",
        "      Merged DataFrame with forward-filled values, and no duplicate OHLCV columns.\n",
        "  \"\"\"\n",
        "  # Ensure both DataFrames have a datetime index\n",
        "  if not isinstance(df1.index, pd.DatetimeIndex) or not isinstance(df2.index, pd.DatetimeIndex):\n",
        "      raise ValueError(\"DataFrames must have a datetime index.\")\n",
        "  # Identify OHLCV columns\n",
        "  ohlcv_cols = ['open', 'high', 'low', 'close', 'volume']\n",
        "\n",
        "  # Rename columns in df2 that conflict with df1's OHLCV columns\n",
        "  for col in df2.columns:\n",
        "    if col.lower() in ohlcv_cols and col.lower() in df1.columns.str.lower():\n",
        "      del df2[col]\n",
        "\n",
        "  # Merge the DataFrames using outer join, which preserves all dates\n",
        "  merged_df = pd.merge(df1, df2, how='outer', left_index=True, right_index=True, suffixes=('_df1', '_df2'))\n",
        "\n",
        "  # Forward fill the missing values for each column\n",
        "  for col in merged_df.columns:\n",
        "    merged_df[col] = merged_df[col].ffill()\n",
        "\n",
        "  return merged_df"
      ],
      "metadata": {
        "id": "8nz7qqCsit1b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resample_candles(df, interval='1h'):\n",
        "    \"\"\"\n",
        "    Resamples the DataFrame to the specified interval.\n",
        "    If the interval is in the format '15m', it's converted to '15min'.\n",
        "    \"\"\"\n",
        "    if interval.endswith('m') and not interval.endswith('min'):\n",
        "        interval = interval[:-1] + 'min'  # Convert '15m' to '15min'\n",
        "\n",
        "    return df.resample(interval).agg({\n",
        "        'Open': 'first',\n",
        "        'High': 'max',\n",
        "        'Low': 'min',\n",
        "        'Close': 'last',\n",
        "        'Volume': 'sum'\n",
        "    }).ffill()"
      ],
      "metadata": {
        "id": "t9eNtD9QgvIa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, features_columns, target_col, seq_length):\n",
        "    \"\"\"\n",
        "    Create sequences of `seq_length` time steps for LSTM input, optimized for performance.\n",
        "    \"\"\"\n",
        "    num_samples = len(data) - seq_length\n",
        "    num_features = len(features_columns)\n",
        "\n",
        "    X = np.zeros((num_samples, seq_length, num_features))\n",
        "    y = np.zeros(num_samples)\n",
        "\n",
        "    features_data = data[features_columns].values\n",
        "    target_data = data[target_col].values\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        X[i] = features_data[i:i + seq_length]\n",
        "        y[i] = target_data[i + seq_length]\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "77dtP0gVDcVF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomEarlyStopping(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, patience=2):\n",
        "        super(CustomEarlyStopping, self).__init__()\n",
        "        self.patience = patience  # Number of consecutive increases allowed\n",
        "        self.best_val_mae = float('inf')  # Track the best validation MAE\n",
        "        self.increase_count = 0  # Counter for consecutive increases\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current_val_mae = logs.get('val_mae')  # Get validation MAE for the current epoch\n",
        "\n",
        "        if current_val_mae < self.best_val_mae:\n",
        "            # If validation MAE improves, reset the counter\n",
        "            self.best_val_mae = current_val_mae\n",
        "            self.increase_count = 0\n",
        "        else:\n",
        "            # If validation MAE increases, increment the counter\n",
        "            self.increase_count += 1\n",
        "\n",
        "        # Stop training if validation MAE increases consecutively for 'patience' epochs\n",
        "        if self.increase_count >= self.patience:\n",
        "            print(f\"\\nEarly stopping: Validation MAE increased {self.patience} times in a row.\")\n",
        "            self.model.stop_training = True"
      ],
      "metadata": {
        "id": "PYV3ovEs6j_w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_in_batches(model, test_data, features_columns, seq_length, batch_size=128):\n",
        "    \"\"\"\n",
        "    Predicts in batches to speed up inference.\n",
        "    \"\"\"\n",
        "    num_samples = len(test_data) - seq_length\n",
        "    feature_array = test_data[features_columns].values\n",
        "    predicted_values = np.full(len(test_data), np.nan)\n",
        "\n",
        "    for start_idx in range(0, num_samples, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, num_samples)\n",
        "        batch_indices = range(start_idx + seq_length, end_idx + seq_length)\n",
        "        batch_input = np.array([feature_array[i - seq_length:i] for i in batch_indices])\n",
        "\n",
        "        if len(batch_input) > 0 :\n",
        "            predictions = model.predict(batch_input, verbose=0)\n",
        "            predicted_values[batch_indices] = predictions.flatten() #Flatten to 1d array.\n",
        "\n",
        "    test_data['Predicted_Value'] = predicted_values\n",
        "    return test_data"
      ],
      "metadata": {
        "id": "UxAXoG8PE9je"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PredictedValueStrategy(bt.Strategy):\n",
        "    params = (\n",
        "        ('buy_threshold', 0.6),\n",
        "        ('sell_threshold', 0.6),\n",
        "        ('leverage', 1),  # Leverage ratio\n",
        "        ('margin', 1000),\n",
        "        ('log', True)\n",
        "    )\n",
        "\n",
        "    def __init__(self):\n",
        "        # To keep track of pending orders and buy price/commission\n",
        "        self.order = None\n",
        "        self.buyprice = None\n",
        "        self.buycomm = None\n",
        "\n",
        "        # Add the Predicted_Value as a data feed\n",
        "        self.predicted_value = self.datas[0].predicted_value\n",
        "\n",
        "    def next(self):\n",
        "        # Check if an order is pending ... if yes, we cannot send a 2nd one\n",
        "        if self.order:\n",
        "            return\n",
        "         # # Check if we are in the market\n",
        "        if not self.position:\n",
        "          cash = self.broker.getcash()\n",
        "          position_size = (self.params.margin * self.params.leverage) / self.data.close[0]\n",
        "          # Long signal\n",
        "          if self.predicted_value[0] > self.params.buy_threshold:\n",
        "            self.log('LONG POSITION CREATED, %.2f' % self.datas[0].close[0])\n",
        "            self.order = self.buy(size=position_size)\n",
        "\n",
        "          # Short signal\n",
        "          elif self.predicted_value[0] < self.params.sell_threshold:\n",
        "            self.log('SHORT POSITION CREATED, %.2f' % self.datas[0].close[0])\n",
        "            self.order = self.sell(size=position_size)\n",
        "\n",
        "        else:\n",
        "          if self.predicted_value[0] > self.params.buy_threshold and self.position.size < 0:\n",
        "            self.log('CLOSE SHORT POSITION CREATED, %.2f' % self.datas[0].close[0])\n",
        "            self.order = self.close()\n",
        "          elif self.predicted_value[0] < self.params.sell_threshold and self.position.size > 0:\n",
        "            self.log('CLOSE LONG POSITION CREATED, %.2f' % self.datas[0].close[0])\n",
        "            self.order = self.close()\n",
        "\n",
        "    def notify_order(self, order):\n",
        "        if order.status in [order.Submitted, order.Accepted]:\n",
        "            # Buy/Sell order submitted/accepted to/by broker - Nothing to do\n",
        "            return\n",
        "        # Check if an order has been completed\n",
        "        # Attention: broker could reject order if not enough cash\n",
        "        if order.status in [order.Completed]:\n",
        "            if order.isbuy():\n",
        "                self.log('BUY EXECUTED, %.2f' % order.executed.price)\n",
        "            elif order.issell():\n",
        "                self.log('SELL EXECUTED, %.2f' % order.executed.price)\n",
        "        elif order.status in [order.Canceled, order.Margin, order.Rejected]:\n",
        "            if order.status == order.Canceled:\n",
        "                self.log('Order Canceled')\n",
        "            elif order.status == order.Margin:\n",
        "                self.log(f'Order Margin Not Enough - Available cash: {self.broker.getcash()}')\n",
        "            elif order.status == order.Rejected:\n",
        "                self.log('Order Rejected')\n",
        "\n",
        "        # Write down: no pending order\n",
        "        self.order = None\n",
        "\n",
        "    def notify_trade(self, trade):\n",
        "        if not trade.isclosed:\n",
        "            return\n",
        "        self.log(f'TRADE COMPLETED, GROSS {trade.pnl:.2f}, NET {trade.pnlcomm:.2f}, Available Cash {self.broker.getcash():.2f}')\n",
        "\n",
        "    def log(self, txt, dt=None):\n",
        "        if not self.params.log:\n",
        "          return\n",
        "        dt = dt or self.datas[0].datetime.date(0)\n",
        "        time = self.datas[0].datetime.time()\n",
        "        print(f'{dt.isoformat()} {time.isoformat()}, {txt}')\n"
      ],
      "metadata": {
        "id": "SvTq9isY3ctm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extend PandasData to include the custom column\n",
        "class CustomPandasData(bt.feeds.PandasData):\n",
        "    # Add custom columns\n",
        "    lines = ('predicted_value',)  # Add the custom line\n",
        "    params = (\n",
        "        ('predicted_value', 'Predicted_Value'),  # Map the column name\n",
        "    )\n"
      ],
      "metadata": {
        "id": "gRqdeATXlKaO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dict_even(data):\n",
        "    \"\"\"\n",
        "    Recursively makes all numeric values in a dictionary even.\n",
        "\n",
        "    Args:\n",
        "        data (dict): The input dictionary (can have nested dictionaries or lists).\n",
        "\n",
        "    Returns:\n",
        "        dict: A new dictionary with all numeric values made even.\n",
        "    \"\"\"\n",
        "    if isinstance(data, dict):\n",
        "        new_dict = {}\n",
        "        for key, value in data.items():\n",
        "            new_dict[key] = make_dict_even(value)\n",
        "        return new_dict\n",
        "    elif isinstance(data, list):\n",
        "        return [make_dict_even(item) for item in data]\n",
        "    elif isinstance(data, (int, float)):\n",
        "        if isinstance(data, int):\n",
        "            return data if data % 2 == 0 else data + 1\n",
        "        else: # float. We will round to an int, and then make even.\n",
        "            int_value = round(data)\n",
        "            return int_value if int_value % 2 == 0 else int_value + 1\n",
        "\n",
        "    else:\n",
        "        return data  # Return non-numeric values as they are"
      ],
      "metadata": {
        "id": "Y1RTvfAxGxYu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_cloud_storage_path(bucket_name, local_file_name):\n",
        "    \"\"\"\n",
        "    Returns the cloud storage path for a given model name.\n",
        "\n",
        "    Parameters:\n",
        "        local_file_name (str): The name of the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The cloud storage path. (e.g., gs://<bucket_name>/trade/models/<model_name>.h5).\n",
        "    \"\"\"\n",
        "    return f'gs://{bucket_name}/trade/models/{local_file_name}'\n",
        "\n",
        "def save_model_to_cloud_storage(model: tf.keras.Model, model_name: str, bucket_name: str):\n",
        "    \"\"\"\n",
        "    Saves a TensorFlow model to Google Cloud Storage and returns the cloud storage file path.\n",
        "\n",
        "    Parameters:\n",
        "        model (tf.keras.Model): The TensorFlow model to save.\n",
        "        model_name (str): The name of the model (used to create the file name).\n",
        "        bucket_name (str): The name of the Google Cloud Storage bucket.\n",
        "\n",
        "    Returns:\n",
        "        str: The cloud storage file path (e.g., gs://<bucket_name>/trade/models/<model_name>.h5).\n",
        "    \"\"\"\n",
        "    # Define the local and cloud storage file paths\n",
        "    local_file_name = f'{model_name}.h5'\n",
        "    local_file_path = f'/tmp/{local_file_name}'\n",
        "    cloud_file_path = get_model_cloud_storage_path(bucket_name, local_file_name)\n",
        "\n",
        "    # Save the model locally\n",
        "    model.save(local_file_path)\n",
        "\n",
        "    try:\n",
        "        # Upload the model to Google Cloud Storage\n",
        "        subprocess.run(['gsutil', 'cp', local_file_path, cloud_file_path], check=True)\n",
        "        print(f\"Model saved to {cloud_file_path}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle errors during the upload process\n",
        "        raise RuntimeError(f\"Failed to upload model to Google Cloud Storage: {e}\")\n",
        "    finally:\n",
        "        # Clean up the local file\n",
        "        if os.path.exists(local_file_path):\n",
        "            os.remove(local_file_path)\n",
        "    return cloud_file_path\n",
        "\n",
        "def load_model_from_cloud_storage(model_name: str, bucket_name: str):\n",
        "    \"\"\"\n",
        "    Loads a TensorFlow model from Google Cloud Storage.\n",
        "\n",
        "    Parameters:\n",
        "        model_name (str): The name of the model (used to create the file name).\n",
        "        bucket_name (str): The name of the Google Cloud Storage bucket.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: The loaded TensorFlow model.\n",
        "    \"\"\"\n",
        "    # Define the local and cloud storage file paths\n",
        "    local_file_name = f'{model_name}.h5'\n",
        "    local_file_path = f'/tmp/{local_file_name}'\n",
        "    cloud_file_path = get_model_cloud_storage_path(bucket_name, local_file_name)\n",
        "\n",
        "    try:\n",
        "        # Download the model from Google Cloud Storage\n",
        "        subprocess.run(['gsutil', 'cp', cloud_file_path, local_file_path], check=True)\n",
        "\n",
        "        # Load the model\n",
        "        model = tf.keras.models.load_model(local_file_path)\n",
        "        print(f\"Model loaded from {cloud_file_path}\")\n",
        "        return model\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        raise FileNotFoundError(f\"Model not found in Google Cloud Storage: {e}\")\n",
        "    finally:\n",
        "        # Clean up the local file\n",
        "        if os.path.exists(local_file_path):\n",
        "            os.remove(local_file_path)\n"
      ],
      "metadata": {
        "id": "8eAPe1BN4bO-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_firestore(project_id):\n",
        "    \"\"\"\n",
        "    Initialize the Firestore client using the service account key.\n",
        "\n",
        "    Parameters:\n",
        "        project_id (str): The Google Cloud project ID.\n",
        "\n",
        "    Returns:\n",
        "        firestore.Client: Initialized Firestore client.\n",
        "    \"\"\"\n",
        "    if not firebase_admin._apps:\n",
        "        # cred = credentials.Certificate(service_account_key_path)\n",
        "        cred = firebase_admin.credentials.ApplicationDefault()\n",
        "        cred._project_id = project_id  # Add this line\n",
        "        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project_id\n",
        "        firebase_admin.initialize_app(cred, {'projectId': project_id})\n",
        "    return firestore.client()\n",
        "\n",
        "\n",
        "def save_to_firestore(project_id, collection_name, data):\n",
        "    \"\"\"\n",
        "    Save data to a Firestore collection.\n",
        "\n",
        "    Parameters:\n",
        "        project_id (str): The Google Cloud project ID.\n",
        "        service_account_key_path (str): Path to the Firebase service account key JSON file.\n",
        "        collection_name (str): Name of the Firestore collection.\n",
        "        data (dict): Data to save in the document.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Initialize Firestore client\n",
        "    db = initialize_firestore(project_id)\n",
        "\n",
        "    # Save data to Firestore\n",
        "    doc_ref = db.collection(collection_name).document()\n",
        "    doc_ref.set(data)\n",
        "    print(f\"Data saved to Firestore: Collection={collection_name}, Document ID={doc_ref.id}\")\n",
        "\n",
        "\n",
        "def delete_documents_by_field(project_id, collection_name, field_name, field_value, batch_size=500):\n",
        "  \"\"\"\n",
        "  Deletes documents in a Firestore collection where a specific field matches a value.\n",
        "\n",
        "  Args:\n",
        "      collection_name: The name of the Firestore collection.\n",
        "      field_name: The name of the field to filter by.\n",
        "      field_value: The value to filter the field against.\n",
        "      batch_size: The number of documents to delete in each batch.\n",
        "  \"\"\"\n",
        "  db = initialize_firestore(project_id)\n",
        "  collection_ref = db.collection(collection_name)\n",
        "\n",
        "  try:\n",
        "      while True:\n",
        "          query = collection_ref.where(field_name, '==', field_value).limit(batch_size)\n",
        "          docs = query.stream()\n",
        "\n",
        "          deleted_count = 0\n",
        "          batch = db.batch()\n",
        "\n",
        "          for doc in docs:\n",
        "              batch.delete(doc.reference)\n",
        "              deleted_count += 1\n",
        "\n",
        "          if deleted_count == 0:\n",
        "              print(f\"No documents found with {field_name} == {field_value} in {collection_name}.\")\n",
        "              break\n",
        "\n",
        "          batch.commit()\n",
        "          print(f\"Deleted {deleted_count} documents from {collection_name} where {field_name} == {field_value}.\")\n",
        "          if deleted_count < batch_size: # if less than batch size deleted, then there are no more documents matching.\n",
        "              break\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "ySNwgm17Ce54"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_param_grid(params):\n",
        "    \"\"\"\n",
        "    Generate a grid of all possible hyperparameter combinations.\n",
        "\n",
        "    Args:\n",
        "        params (dict): A dictionary where keys are hyperparameter names and values are lists of possible values.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, where each dictionary represents a unique combination of hyperparameters.\n",
        "    \"\"\"\n",
        "    keys = params.keys()\n",
        "    values = params.values()\n",
        "\n",
        "    # Generate all possible combinations of hyperparameters\n",
        "    param_grid = [dict(zip(keys, combination)) for combination in itertools.product(*values)]\n",
        "\n",
        "    return param_grid\n"
      ],
      "metadata": {
        "id": "XQkkV3Y2KpLx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_out_of_range_values(obj):\n",
        "    \"\"\"\n",
        "    Recursively traverse a nested object and convert out-of-range values to zero.\n",
        "\n",
        "    Args:\n",
        "        obj: The input object (can be a dictionary, list, or other nested structure).\n",
        "\n",
        "    Returns:\n",
        "        The same object with out-of-range values replaced by zero.\n",
        "    \"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        # If the object is a dictionary, recursively process its values\n",
        "        for key, value in obj.items():\n",
        "            obj[key] = fix_out_of_range_values(value)\n",
        "    elif isinstance(obj, list):\n",
        "        # If the object is a list, recursively process its elements\n",
        "        for i in range(len(obj)):\n",
        "            obj[i] = fix_out_of_range_values(obj[i])\n",
        "    else:\n",
        "        # If the object is a value, check if it's out of range\n",
        "        try:\n",
        "            # Attempt to perform a numeric operation to check if the value is valid\n",
        "            # For example, check if the value can be compared to a range\n",
        "            if not (-1e15 < obj < 1e14):  # Adjust the range as needed\n",
        "                obj = 0\n",
        "        except (TypeError, ValueError):\n",
        "            # If the value is not numeric or causes an error, replace it with zero\n",
        "            obj = 0\n",
        "    return obj"
      ],
      "metadata": {
        "id": "XJU3s2FgYVaF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_in_chunks(X, y, chunk_size=80000, train_validate_split_ratio=0.2, train_epochs=20):\n",
        "    \"\"\"Retrains the model in chunks to handle memory limit issues.\"\"\"\n",
        "\n",
        "    num_chunks = int(np.ceil(len(X) / chunk_size))\n",
        "    model = None  # Initialize model outside the loop\n",
        "    initial_learning_rate = 0.001  # Initial learning rate\n",
        "    learning_rate_decay = 0.9     # Decay factor (adjust as needed)\n",
        "\n",
        "    if num_chunks > 1 and len(X) % chunk_size < 0.6 * chunk_size:\n",
        "        num_chunks -= 1  # Combine last two chunks\n",
        "\n",
        "    for chunk_idx in range(num_chunks):\n",
        "        print(f\"Processing chunk {chunk_idx + 1}/{num_chunks}\")\n",
        "        checkpoint_filepath = 'weights-' + str(chunk_idx) + '.{epoch:02d}-{val_loss:.4f}.keras'\n",
        "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "          filepath=checkpoint_filepath,\n",
        "          save_best_only=True,\n",
        "          monitor='val_loss',\n",
        "          mode='min'\n",
        "      )\n",
        "        start_idx = chunk_idx * chunk_size\n",
        "        end_idx = min((chunk_idx + 1) * chunk_size, len(X))\n",
        "\n",
        "        X_chunk = X[start_idx:end_idx]\n",
        "        y_chunk = y[start_idx:end_idx]\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_chunk, y_chunk, test_size=train_validate_split_ratio, shuffle=False\n",
        "        )\n",
        "\n",
        "        model_input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "        history = None\n",
        "\n",
        "        if model is None:\n",
        "            # Build the model only for the first chunk\n",
        "            model = Sequential([\n",
        "                LSTM(100, return_sequences=True, input_shape=model_input_shape),\n",
        "                Dropout(0.2),\n",
        "                LSTM(50, return_sequences=False),\n",
        "                Dropout(0.2),\n",
        "                Dense(50, activation='relu'),\n",
        "                Dropout(0.2),\n",
        "                Dense(25, activation='relu'),\n",
        "                Dropout(0.2),\n",
        "                Dense(10, activation='relu'),\n",
        "                Dropout(0.2),\n",
        "                Dense(1, activation='tanh')\n",
        "            ])\n",
        "            optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
        "            model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "        else:\n",
        "            # Reduce learning rate for subsequent chunks\n",
        "            current_learning_rate = tf.keras.backend.get_value(model.optimizer.learning_rate)\n",
        "            new_learning_rate = current_learning_rate * learning_rate_decay\n",
        "            # new_learning_rate = float(new_learning_rate)\n",
        "            # tf.keras.backend.set_value(model.optimizer.learning_rate, new_learning_rate)\n",
        "            model.optimizer.learning_rate.assign(new_learning_rate)\n",
        "            print(f\"Learning rate reduced to: {new_learning_rate}\")\n",
        "\n",
        "        custom_early_stopping = CustomEarlyStopping(patience=3)\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=train_epochs,\n",
        "            batch_size=32,\n",
        "            validation_data=(X_test, y_test),\n",
        "            verbose=1,\n",
        "            callbacks=[custom_early_stopping, model_checkpoint_callback]\n",
        "        )\n",
        "\n",
        "        final_epoch = len(history.history['loss'])\n",
        "        train_loss = history.history['loss'][-1]\n",
        "        train_mae = history.history['mae'][-1]\n",
        "        val_loss = history.history['val_loss'][-1]\n",
        "        val_mae = history.history['val_mae'][-1]\n",
        "\n",
        "        # Get the best val_loss\n",
        "        best_val_loss = min(history.history['val_loss'])\n",
        "        best_epoch = history.history['val_loss'].index(best_val_loss) + 1\n",
        "\n",
        "        print(f\"Best Epoch: {best_epoch}\")\n",
        "        print(f\"Best Val Loss: {best_val_loss}\")\n",
        "\n",
        "        # Load the model of the best\n",
        "        model.load_weights(checkpoint_filepath.format(epoch=best_epoch, val_loss=best_val_loss))\n",
        "\n",
        "        print(f\"Chunk {chunk_idx + 1} training complete.\")\n",
        "        print(f\"Final Epoch: {final_epoch}\")\n",
        "        print(f\"Train Loss: {train_loss}\")\n",
        "        print(f\"Train MAE: {train_mae}\")\n",
        "        print(f\"Val Loss: {val_loss}\")\n",
        "        print(f\"Val MAE: {val_mae}\")\n",
        "        gc.collect() #garbage collection to free memory.\n",
        "        tf.keras.backend.clear_session()#clear session.\n",
        "\n",
        "    # Delete all weight files\n",
        "    for filename in os.listdir('.'):\n",
        "        if filename.startswith('weights-') and filename.endswith('.keras'):\n",
        "            os.remove(filename)\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "awMVWpsot8s_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU if available\n",
        "# Check if a GPU is available\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Restrict TensorFlow to use the first GPU\n",
        "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "    except RuntimeError as e:\n",
        "        # Visible devices must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"No GPU found.\")"
      ],
      "metadata": {
        "id": "OmJTiylVTUXj",
        "outputId": "b37929e5-9af7-474b-c061-039b6d3411fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "options = {\n",
        "  'timeframe': '5m',\n",
        "  'symbol': 'BTCUSDT',\n",
        "  'exchange': 'binance',\n",
        "  'start_date': '01-01-2024',\n",
        "  'end_date': '28-02-2025' ,\n",
        "  'pivot_windows': 10,\n",
        "  'version': 'v1_0_5',\n",
        "  'middle_timeframe': '15m',\n",
        "  'higher_timeframe': '1h',\n",
        "  'seq_length': 500\n",
        "}"
      ],
      "metadata": {
        "id": "i2eIpKkrnB80"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_model(**options):\n",
        "  # Extract options\n",
        "  timeframe = options['timeframe'] if 'timeframe' in options else '15m'\n",
        "  symbol = options['symbol'] if 'symbol' in options else 'BTCUSDT'\n",
        "  exchange = options['exchange'] if 'exchange' in options else 'binance'\n",
        "  start_date = options['start_date'] if 'start_date' in options else '01-01-2024'\n",
        "  end_date = options['end_date'] if 'end_date' in options else '28-02-2025'\n",
        "  pivot_windows = options['pivot_windows'] if 'pivot_windows' in options else 10\n",
        "  version = options['version'] if 'version' in options else 'v1'\n",
        "  middle_timeframe = options['middle_timeframe'] if 'middle_timeframe' in options else '1h'\n",
        "  higher_timeframe = options['higher_timeframe'] if 'higher_timeframe' in options else '4h'\n",
        "  seq_length = options['seq_length'] if 'seq_length' in options else 100\n",
        "\n",
        "  # Get raw candles\n",
        "  data = get_all_binance_candles(symbol, timeframe, start_date, end_date)\n",
        "  # Add pivots\n",
        "  data = add_pivots(data, int(pivot_windows))\n",
        "  data['Pivot'] = np.where(data['Higher_Pivot'] == 1, -1, np.where(data['Lower_Pivot'] == 1, 1, 0))\n",
        "  del data['Higher_Pivot']\n",
        "  del data['Lower_Pivot']\n",
        "  # Calculate pivot proximity\n",
        "  data = calculate_pivot_proximity(data)\n",
        "  # Declare feature columns\n",
        "  features_columns = []\n",
        "\n",
        "  # # Add technical indicators on lower timeframe\n",
        "  lower_timeframe = timeframe\n",
        "  lower_timeframe_prefix = f\"{lower_timeframe}_\"\n",
        "  add_scaled_rsi(data, 14, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_rsi(data, 6, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(data, 5, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(data, 21, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_sma(data, 50, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_macd(data, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_bbands(data, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "\n",
        "  # Add technical indicators on middle timeframe\n",
        "  middle_timeframe_prefix = f\"{middle_timeframe}_\"\n",
        "  middle_data = resample_candles(data, middle_timeframe)\n",
        "  add_scaled_rsi(middle_data, 14, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_rsi(middle_data, 6, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(middle_data, 5, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(middle_data, 21, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_macd(middle_data, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_bbands(middle_data, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "\n",
        "  # Add technical indicators on higher timeframe\n",
        "  higher_timeframe_prefix = f\"{higher_timeframe}_\"\n",
        "  higher_data = resample_candles(data, higher_timeframe)\n",
        "  add_scaled_rsi(higher_data, 14, prefix=higher_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_rsi(higher_data, 6, prefix=higher_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(higher_data, 5, prefix=higher_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(higher_data, 21, prefix=higher_timeframe_prefix, features_columns=features_columns)\n",
        "\n",
        "  # Merge middle and higher timeframe data to lower timeframe data\n",
        "  data = merge_candlesticks_data(data, middle_data)\n",
        "  data = merge_candlesticks_data(data, higher_data)\n",
        "\n",
        "  # Drop rows with NaN values (due to rolling calculations)\n",
        "  data.dropna(inplace=True)\n",
        "\n",
        "  # delete Pivot_Porximity that have zero at the beginning and ending of the dataframe\n",
        "  non_zero_indices = data[data['Pivot_Proximity'] != 0].index\n",
        "  non_zero_at_begining = non_zero_indices[0]\n",
        "  non_zero_at_end = non_zero_indices[-1]\n",
        "  data = data.loc[non_zero_at_begining:non_zero_at_end]\n",
        "\n",
        "  # select only first 80 % of the data\n",
        "  training_data_ratio = 0.8 # 80%\n",
        "  training_data = data[:int(len(data) * training_data_ratio)]\n",
        "  training_data_start_date = training_data.index[0]\n",
        "  training_data_end_date = training_data.index[-1]\n",
        "\n",
        "  # Create training sequences\n",
        "  target_col = 'Pivot_Proximity'\n",
        "  X, y = create_sequences(training_data, features_columns, target_col, seq_length)\n",
        "\n",
        "  # Split into training and testing sets\n",
        "  train_validate_split_ratio = 0.2\n",
        "  train_candles_length = (1 - train_validate_split_ratio) * len(X)\n",
        "  val_candles_length = len(X) - train_candles_length\n",
        "\n",
        "  # Build & Train the model\n",
        "  train_epochs = 20\n",
        "  model, history = train_in_chunks(X, y, chunk_size=80000, train_validate_split_ratio=train_validate_split_ratio, train_epochs=train_epochs)\n",
        "  model_input_shape = (X.shape[1], X.shape[2])\n",
        "  final_epoch = len(history.history['loss'])\n",
        "  train_loss = history.history['loss'][-1]\n",
        "  train_mae = history.history['mae'][-1]\n",
        "  val_loss = history.history['val_loss'][-1]\n",
        "  val_mae = history.history['val_mae'][-1]\n",
        "\n",
        "  # Save model to cloud storage\n",
        "  # Save the model\n",
        "  random_hash = generate_random_hash()\n",
        "  model_name = f\"binance_mtf_{version}_{timeframe}_{start_date}_{end_date}_{random_hash}\"\n",
        "  model_path = save_model_to_cloud_storage(model, model_name, bucket_name)\n",
        "\n",
        "  # test data\n",
        "  test_data = data[int(len(data) * training_data_ratio):]\n",
        "\n",
        "  # Predict in batches\n",
        "  batch_size = 128\n",
        "  predict_in_batches(model, test_data, features_columns, seq_length, batch_size)\n",
        "  test_data.dropna(inplace=True) # Early predicted values wont be availble due to sequencing\n",
        "  test_data_start_date = test_data.index[0]\n",
        "  test_data_end_date = test_data.index[-1]\n",
        "\n",
        "  # Load data into a Pandas DataFrame\n",
        "  backtest_data = CustomPandasData(\n",
        "      dataname=test_data,\n",
        "      datetime=None,  # Use the index as the datetime\n",
        "      open='Open',         # Column index for Open\n",
        "      high='High',         # Column index for High\n",
        "      low='Low',          # Column index for Low\n",
        "      close='Close',        # Column index for Close\n",
        "      volume='Volume',       # Column index for Volume\n",
        "      openinterest=None,# No open interest column\n",
        "      predicted_value='Predicted_Value'  # Column index for Predicted_Value\n",
        "  )\n",
        "\n",
        "  bactest_grid_params = generate_param_grid({\n",
        "      'buy_threshold': [0.3, 0.4, 0.5, 0.6],\n",
        "      'sell_threshold': [-0.3, -0.4, -0.5, -0.6],\n",
        "      'leverage': [7, 10, 15, 20, 25, 30],\n",
        "  })\n",
        "\n",
        "  for param in bactest_grid_params:\n",
        "    # Grid trade params\n",
        "    trade_buy_threshold = param['buy_threshold']\n",
        "    trade_sell_threshold = param['sell_threshold']\n",
        "    trade_leverage = param['leverage']\n",
        "    print(f\"Backtesting- Buy Threshold: {trade_buy_threshold}, Sell Threshold: {trade_sell_threshold}, leverage: {trade_leverage}\")\n",
        "\n",
        "    # backtest configuration\n",
        "    trade_margin = 1000\n",
        "    broker_commision = 0.02 # In percentage\n",
        "\n",
        "    # Create a Cerebro engine instance\n",
        "    cerebro = bt.Cerebro()\n",
        "\n",
        "    # Add the strategy\n",
        "    cerebro.addstrategy(\n",
        "        PredictedValueStrategy,\n",
        "        buy_threshold=trade_buy_threshold,\n",
        "        sell_threshold=trade_sell_threshold,\n",
        "        leverage=trade_leverage,\n",
        "        margin=trade_margin,\n",
        "        log=False\n",
        "    )\n",
        "\n",
        "    # Add the data feed\n",
        "    cerebro.adddata(backtest_data)\n",
        "\n",
        "    # Set the initial cash\n",
        "    cerebro.broker.set_cash(10000.0)\n",
        "\n",
        "    # Set the commission\n",
        "    cerebro.broker.setcommission(commission=broker_commision / 100)\n",
        "\n",
        "    # Add analyzers\n",
        "    cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe')\n",
        "    cerebro.addanalyzer(bt.analyzers.DrawDown, _name='drawdown')\n",
        "    cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name='tradeanalyzer')\n",
        "    cerebro.addanalyzer(bt.analyzers.Returns, _name='returns')\n",
        "\n",
        "    # Run the backtest\n",
        "    starting_portfolio_value = cerebro.broker.getvalue()\n",
        "    print('Starting Portfolio Value: %.2f' % starting_portfolio_value)\n",
        "    backtest_result = cerebro.run()\n",
        "    final_portfolio_value = cerebro.broker.getvalue()\n",
        "    print('Final Portfolio Value: %.2f' % final_portfolio_value)\n",
        "\n",
        "    # Capture analysis\n",
        "    strat = backtest_result[0]\n",
        "\n",
        "    trade_analysis = make_dict_even(strat.analyzers.tradeanalyzer.get_analysis())\n",
        "    trade_analysis = fix_out_of_range_values(trade_analysis)\n",
        "\n",
        "    drawdown_analysis = make_dict_even(strat.analyzers.drawdown.get_analysis())\n",
        "    drawdown_analysis = fix_out_of_range_values(drawdown_analysis)\n",
        "    sharpe_analysis = make_dict_even(strat.analyzers.sharpe.get_analysis())\n",
        "    sharpe_analysis = fix_out_of_range_values(sharpe_analysis)\n",
        "\n",
        "    analysis_result = {\n",
        "        'timeframe': timeframe,\n",
        "        'start_date': start_date,\n",
        "        'end_date': end_date,\n",
        "        'symbol': symbol,\n",
        "        'exchange': exchange,\n",
        "        'features_columns': features_columns,\n",
        "        'lower_timeframe': lower_timeframe,\n",
        "        'middle_timeframe': middle_timeframe,\n",
        "        'higher_timeframe': higher_timeframe,\n",
        "        'seq_length': seq_length,\n",
        "        'training_data_ratio': training_data_ratio,\n",
        "        'epochs': train_epochs,\n",
        "        'model_input_shape': model_input_shape,\n",
        "        'model_name': model_name,\n",
        "        'model_path': model_path,\n",
        "        'train_loss': train_loss,\n",
        "        'train_mae': train_mae,\n",
        "        'val_loss': val_loss,\n",
        "        'val_mae': val_mae,\n",
        "        'final_epoch': final_epoch,\n",
        "        'training_data_start_date': training_data_start_date,\n",
        "        'training_data_end_date': training_data_end_date,\n",
        "        'test_data_start_date': test_data_start_date,\n",
        "        'test_data_end_date': test_data_end_date,\n",
        "        'trade_leverage': trade_leverage,\n",
        "        'trade_margin': trade_margin,\n",
        "        'trade_buy_threshold': trade_buy_threshold,\n",
        "        'trade_sell_threshold': trade_sell_threshold,\n",
        "        'starting_portfolio_value': starting_portfolio_value,\n",
        "        'final_portfolio_value': final_portfolio_value,\n",
        "        'broker_commision': broker_commision,\n",
        "        'trade_analysis': trade_analysis,\n",
        "        'drawdown_analysis': drawdown_analysis,\n",
        "        'sharpe_analysis': sharpe_analysis,\n",
        "        'version': version,\n",
        "        'created_at': datetime.datetime.now()\n",
        "    }\n",
        "    save_to_firestore(project_id, firestore_collection_name, analysis_result)\n",
        "    print(analysis_result)"
      ],
      "metadata": {
        "id": "EjeV2AzvnY1Q"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_periods =  [(datetime.datetime(x,1,1), datetime.datetime(x,1,1) + datetime.timedelta(days=365*3)) for x in range(2017,2024)]\n",
        "for start_date, end_date in evaluate_periods:\n",
        "  options['start_date'] = start_date.strftime('%d-%m-%Y')\n",
        "  options['end_date'] = end_date.strftime('%d-%m-%Y')\n",
        "  print(f\"Evaluating from {start_date} to {end_date}\")\n",
        "  generate_model(**options)"
      ],
      "metadata": {
        "id": "rI-yYG2aR3sS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "606adb8f-8df5-48fe-d653-c17780d050f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating from 2017-01-01 00:00:00 to 2020-01-01 00:00:00\n",
            "Copying file:///tmp/binance_BTCUSDT_5m_01-01-2017_01-01-2020.csv [Content-Type=text/csv]...\n",
            "|\n",
            "Operation completed over 1 objects/14.7 MiB.                                     \n",
            "Processing chunk 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 35ms/step - loss: 0.3344 - mae: 0.4921 - val_loss: 0.2738 - val_mae: 0.4419\n",
            "Epoch 2/20\n",
            "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 34ms/step - loss: 0.2836 - mae: 0.4461 - val_loss: 0.2867 - val_mae: 0.4546\n",
            "Epoch 3/20\n",
            "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 34ms/step - loss: 0.2719 - mae: 0.4332 - val_loss: 0.2575 - val_mae: 0.4274\n",
            "Epoch 4/20\n",
            "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 34ms/step - loss: 0.2665 - mae: 0.4269 - val_loss: 0.2622 - val_mae: 0.4281\n",
            "Epoch 5/20\n",
            "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 34ms/step - loss: 0.2614 - mae: 0.4229 - val_loss: 0.2613 - val_mae: 0.4274\n",
            "Epoch 6/20\n",
            "\u001b[1m1999/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2584 - mae: 0.4188\n",
            "Early stopping: Validation MAE increased 3 times in a row.\n",
            "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 34ms/step - loss: 0.2584 - mae: 0.4188 - val_loss: 0.2826 - val_mae: 0.4412\n",
            "Best Epoch: 3\n",
            "Best Val Loss: 0.2574780285358429\n",
            "Chunk 1 training complete.\n",
            "Final Epoch: 6\n",
            "Train Loss: 0.2580051124095917\n",
            "Train MAE: 0.41837653517723083\n",
            "Val Loss: 0.2825894057750702\n",
            "Val MAE: 0.44117337465286255\n",
            "Processing chunk 2/2\n",
            "Learning rate reduced to: 0.0009000000427477062\n",
            "Epoch 1/20\n",
            "\u001b[1m 947/2000\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 32ms/step - loss: 0.2677 - mae: 0.4307"
          ]
        }
      ]
    }
  ]
}