{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOR87Y1EKRWLIaGRjLLxPGO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ifeLight/ml-bot/blob/main/binance-multi-timeframe-grade-opt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas-ta\n",
        "!pip install backtrader[plotting]\n",
        "!pip install plotly\n",
        "!pip install --upgrade firebase-admin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZkp-JVxXjc6",
        "outputId": "055275bb-c1f3-4c79-c6b1-e8ebd1285a02"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas-ta\n",
            "  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m112.6/115.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from pandas-ta) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas-ta) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas-ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas-ta) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->pandas-ta) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->pandas-ta) (1.17.0)\n",
            "Building wheels for collected packages: pandas-ta\n",
            "  Building wheel for pandas-ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas-ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218909 sha256=f3f5a9fb95b5b67ac9632a0ee92c18ea3f6b76f0ef268d509abc141ef0dd5273\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/33/8b/50b245c5c65433cd8f5cb24ac15d97e5a3db2d41a8b6ae957d\n",
            "Successfully built pandas-ta\n",
            "Installing collected packages: pandas-ta\n",
            "Successfully installed pandas-ta-0.3.14b0\n",
            "Collecting backtrader[plotting]\n",
            "  Downloading backtrader-1.9.78.123-py2.py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from backtrader[plotting]) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->backtrader[plotting]) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->backtrader[plotting]) (1.17.0)\n",
            "Downloading backtrader-1.9.78.123-py2.py3-none-any.whl (419 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.5/419.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: backtrader\n",
            "Successfully installed backtrader-1.9.78.123\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Requirement already satisfied: firebase-admin in /usr/local/lib/python3.11/dist-packages (6.6.0)\n",
            "Collecting firebase-admin\n",
            "  Downloading firebase_admin-6.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: cachecontrol>=0.12.14 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (0.14.2)\n",
            "Requirement already satisfied: google-api-python-client>=1.7.8 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (2.160.0)\n",
            "Requirement already satisfied: google-cloud-storage>=1.37.1 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (2.19.0)\n",
            "Requirement already satisfied: pyjwt>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.5.0->firebase-admin) (2.10.1)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=1.22.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-firestore>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (2.20.1)\n",
            "Requirement already satisfied: requests>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from cachecontrol>=0.12.14->firebase-admin) (2.32.3)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from cachecontrol>=0.12.14->firebase-admin) (1.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.69.1)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (4.25.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (2.38.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.62.3)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (4.1.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-firestore>=2.19.0->firebase-admin) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage>=1.37.1->firebase-admin) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage>=1.37.1->firebase-admin) (1.6.0)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.5.0->firebase-admin) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.5.0->firebase-admin) (1.17.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client>=1.7.8->firebase-admin) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16.0->cachecontrol>=0.12.14->firebase-admin) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16.0->cachecontrol>=0.12.14->firebase-admin) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16.0->cachecontrol>=0.12.14->firebase-admin) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16.0->cachecontrol>=0.12.14->firebase-admin) (2025.1.31)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.5.0->firebase-admin) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (0.6.1)\n",
            "Downloading firebase_admin-6.7.0-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.0/134.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: firebase-admin\n",
            "  Attempting uninstall: firebase-admin\n",
            "    Found existing installation: firebase-admin 6.6.0\n",
            "    Uninstalling firebase-admin-6.6.0:\n",
            "      Successfully uninstalled firebase-admin-6.6.0\n",
            "Successfully installed firebase-admin-6.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import backtrader as bt\n",
        "import pandas_ta as ta\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import subprocess\n",
        "import firebase_admin\n",
        "from firebase_admin import firestore\n",
        "from requests import Request, Session\n",
        "from requests.exceptions import ConnectionError, Timeout, TooManyRedirects\n",
        "from google.colab import auth\n",
        "import google.auth\n",
        "import itertools\n",
        "import hashlib\n",
        "import gc #garbage collection"
      ],
      "metadata": {
        "id": "dtUTSBApSZe4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "#Configure Google cloud project\n",
        "project_id = 'ifelight'\n",
        "!gcloud config set project {project_id}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELE8dAggI0FQ",
        "outputId": "5a06ca39-2819-465a-f87e-5ae2f22e633e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Working GCP Bucket name\n",
        "bucket_name = 'ife-storage'\n",
        "# Working on Firestore name\n",
        "firestore_collection_name = 'trade-models'"
      ],
      "metadata": {
        "id": "Fs_TurcmdJql"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binance_base_url = 'https://52on3577u3.execute-api.eu-central-1.amazonaws.com'\n",
        "\n",
        "def get_binance_candles(symbol: str, interval='1h', limit: int = 50, **kwargs):\n",
        "    url = f'{binance_base_url}/api/v3/uiKlines?symbol={symbol}&interval={interval}&limit={limit}'\n",
        "    for key, value in kwargs.items():\n",
        "        url += f'&{key}={value}'\n",
        "    response = requests.get(url)\n",
        "    result = json.loads(response.text)\n",
        "    # print(result)\n",
        "    def map_result(x):\n",
        "        return {\n",
        "            'Date': x[0],\n",
        "            'Open': x[1],\n",
        "            'High': x[2],\n",
        "            'Low': x[3],\n",
        "            'Close': x[4],\n",
        "            'Volume': x[5],\n",
        "        }\n",
        "    mappeded_result = []\n",
        "    for x in result:\n",
        "        mappeded_result.append(map_result(x))\n",
        "    return mappeded_result\n",
        "\n",
        "\n",
        "def candles_to_df(data):\n",
        "    df =  pd.DataFrame(data)\n",
        "    df['Date'] = pd.to_datetime(df['Date'], unit='ms')\n",
        "    df['Open'] = df['Open'].astype(float)\n",
        "    df['High'] = df['High'].astype(float)\n",
        "    df['Low'] = df['Low'].astype(float)\n",
        "    df['Close'] = df['Close'].astype(float)\n",
        "    df['Volume'] = df['Volume'].astype(float)\n",
        "    df.set_index('Date', inplace=True)\n",
        "    return df\n",
        "\n",
        "def get_all_binance_candles(symbol: str, interval='1h', start_date=None, end_date=None, limit=1000):\n",
        "  raw_start_date = start_date\n",
        "  raw_end_date = end_date\n",
        "  start_date = pd.to_datetime(start_date) if start_date else pd.to_datetime('2015-01-01')\n",
        "  end_date = pd.to_datetime(end_date) if end_date else pd.to_datetime('today')\n",
        "  try:\n",
        "    if end_date <= pd.to_datetime('today'):\n",
        "      return load_candles_from_cloud_storage(symbol, interval, start_date, end_date)\n",
        "  except FileNotFoundError:\n",
        "    pass\n",
        "  result = []\n",
        "  while True:\n",
        "    candles = get_binance_candles(symbol, interval, limit, startTime=int(start_date.timestamp() * 1000), endTime=int(end_date.timestamp() * 1000))\n",
        "    if len(candles) <= 1:\n",
        "      break;\n",
        "    result += candles\n",
        "    start_date = pd.to_datetime(datetime.datetime.fromtimestamp(candles[-1]['Date'] / 1000))\n",
        "  candles_df = candles_to_df(result)\n",
        "  save_candles_to_cloud_storage(candles_df, symbol, interval, raw_start_date, raw_end_date)\n",
        "  return candles_df\n",
        "\n",
        "def candles_storage_file_name(symbol: str, interval='1h', start_date=None, end_date=None):\n",
        "  file_name = f'binance_{symbol}_{interval}_{start_date}_{end_date}.csv'\n",
        "  return file_name\n",
        "\n",
        "def load_candles_from_cloud_storage(symbol: str, interval: str, start_date=None, end_date=None):\n",
        "  file_name = candles_storage_file_name(symbol, interval, start_date, end_date)\n",
        "  try:\n",
        "    # Download the file from cloud storage.\n",
        "    subprocess.run(['gsutil', 'cp', f'gs://{bucket_name}/trade/candles/{file_name}', f'/tmp/{file_name}'], check=True)\n",
        "\n",
        "    # Load the data into a Pandas DataFrame.\n",
        "    with open(f'/tmp/{file_name}', 'r') as f:\n",
        "      return pd.read_csv(f, index_col=0, parse_dates=True)\n",
        "  except subprocess.CalledProcessError:\n",
        "    # Raise a FileNotFoundError if the file is not found in cloud storage.\n",
        "    raise FileNotFoundError(f\"File not found: gs://{bucket_name}/trade/candles/{file_name}\")\n",
        "\n",
        "def save_candles_to_cloud_storage(df: pd.DataFrame, symbol: str, interval: str, start_date, end_date):\n",
        "  file_name = candles_storage_file_name(symbol, interval, start_date, end_date)\n",
        "  df.to_csv(f'/tmp/{file_name}')\n",
        "  !gsutil cp /tmp/{file_name} gs://{bucket_name}/trade/candles/{file_name}\n"
      ],
      "metadata": {
        "id": "r0DdxBTXLcao"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_random_hash(max_length=8):\n",
        "    \"\"\"\n",
        "    Generate a random hash of a maximum length.\n",
        "\n",
        "    Args:\n",
        "        max_length (int): Maximum length of the hash (default is 8).\n",
        "\n",
        "    Returns:\n",
        "        str: A random hash of the specified maximum length.\n",
        "    \"\"\"\n",
        "    # Generate a random byte sequence\n",
        "    random_bytes = os.urandom(16)  # 16 bytes of random data\n",
        "    # Create a SHA-256 hash of the random bytes\n",
        "    hash_object = hashlib.sha256(random_bytes)\n",
        "    # Get the hexadecimal representation of the hash\n",
        "    hex_hash = hash_object.hexdigest()\n",
        "    # Truncate the hash to the desired maximum length\n",
        "    return hex_hash[:max_length]"
      ],
      "metadata": {
        "id": "C92jSivqMCPe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_pivots(df, window=5):\n",
        "    \"\"\"\n",
        "    Calculate the probability of price going up and down based on higher and lower pivots.\n",
        "    \"\"\"\n",
        "    df['Higher_Pivot'] = df['High'].rolling(window=2 * window + 1, center=True).apply(\n",
        "        lambda x: 1 if x.iloc[window] == x.max() else 0, raw=False\n",
        "    )\n",
        "    df['Lower_Pivot'] = df['Low'].rolling(window=2 * window + 1, center=True).apply(\n",
        "        lambda x: 1 if x.iloc[window] == x.min() else 0, raw=False\n",
        "    )\n",
        "\n",
        "    # Step 2: Ensure no two successive pivots of the same type\n",
        "    pivot_type = None  # Tracks the type of the last pivot\n",
        "    last_pivot_index = None  # Tracks the index of the last pivot\n",
        "\n",
        "    # Remove duplicated index\n",
        "    df = df[~df.index.duplicated(keep='first')]\n",
        "\n",
        "    for i in range(len(df.index)):\n",
        "      if df.loc[df.index[i], 'Higher_Pivot'].item() == 1:\n",
        "          if pivot_type == 'higher':\n",
        "              # Compare current higher pivot with the last higher pivot\n",
        "              if df.loc[df.index[i], 'High'].item() > df.loc[df.index[last_pivot_index], 'High'].item():\n",
        "                  # Remove the last higher pivot\n",
        "                  df.loc[df.index[last_pivot_index], 'Higher_Pivot'] = 0\n",
        "                  # Update the last pivot\n",
        "                  last_pivot_index = i\n",
        "              else:\n",
        "                  # Remove the current higher pivot\n",
        "                  df.loc[df.index[i], 'Higher_Pivot'] = 0\n",
        "          else:\n",
        "              # Update pivot type and index\n",
        "              pivot_type = 'higher'\n",
        "              last_pivot_index = i\n",
        "\n",
        "      elif df.loc[df.index[i], 'Lower_Pivot'].item() == 1:\n",
        "          if pivot_type == 'lower':\n",
        "              # Compare current lower pivot with the last lower pivot\n",
        "              if df.loc[df.index[i], 'Low'].item() < df.loc[df.index[last_pivot_index], 'Low'].item():\n",
        "                  # Remove the last lower pivot\n",
        "                  df.loc[df.index[last_pivot_index], 'Lower_Pivot'] = 0\n",
        "                  # Update the last pivot\n",
        "                  last_pivot_index = i\n",
        "              else:\n",
        "                  # Remove the current lower pivot\n",
        "                  df.loc[df.index[i], 'Lower_Pivot'] = 0\n",
        "          else:\n",
        "              # Update pivot type and index\n",
        "              pivot_type = 'lower'\n",
        "              last_pivot_index = i\n",
        "    return df"
      ],
      "metadata": {
        "id": "551o_MKlZhr2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_pivot_proximity(df):\n",
        "    \"\"\"\n",
        "    Calculates the pivot proximity using a loop-based approach, finding the closest\n",
        "    previous and next pivots without generating intermediate lists of all pivots.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with 'Pivot' and 'Close' columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with 'Pivot_Proximity' column added.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    pivot_values = df['Pivot'].values\n",
        "    close_values = df['Close'].values\n",
        "    proximity_values = [0.0] * len(df)\n",
        "\n",
        "    for i in range(len(df)):\n",
        "      pivot = pivot_values[i]\n",
        "      if pivot == -1:\n",
        "        proximity_values[i] = -1.0\n",
        "      elif pivot == 1:\n",
        "        proximity_values[i] = 1.0\n",
        "      else:\n",
        "        closest_previous_pivot_index = None\n",
        "        for j in range(i - 1, -1, -1):\n",
        "          if pivot_values[j] != 0:\n",
        "            closest_previous_pivot_index = j\n",
        "            break\n",
        "\n",
        "        if closest_previous_pivot_index is not None:\n",
        "          closest_previous_pivot_value = pivot_values[closest_previous_pivot_index]\n",
        "          closest_previous_close = close_values[closest_previous_pivot_index]\n",
        "\n",
        "          closest_next_pivot_index = None\n",
        "          if closest_previous_pivot_value == -1:\n",
        "            for j in range(i + 1, len(df)):\n",
        "              if pivot_values[j] == 1:\n",
        "                closest_next_pivot_index = j\n",
        "                break\n",
        "          else:\n",
        "            for j in range(i + 1, len(df)):\n",
        "              if pivot_values[j] == -1:\n",
        "                closest_next_pivot_index = j\n",
        "                break\n",
        "\n",
        "          if closest_next_pivot_index is not None:\n",
        "            closest_next_close = close_values[closest_next_pivot_index]\n",
        "\n",
        "            distance_to_previous = abs(close_values[i] - closest_previous_close)\n",
        "            distance_to_next = abs(close_values[i] - closest_next_close)\n",
        "\n",
        "            if distance_to_previous + distance_to_next != 0:\n",
        "              if closest_previous_pivot_value == -1:\n",
        "                proximity_values[i] = (distance_to_previous - distance_to_next) / (distance_to_previous + distance_to_next)\n",
        "              else:\n",
        "                proximity_values[i] = (distance_to_next - distance_to_previous) / (distance_to_previous + distance_to_next)\n",
        "\n",
        "    df['Pivot_Proximity'] = proximity_values\n",
        "    return df"
      ],
      "metadata": {
        "id": "Ozb2WlKwCCOB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_scaled_rsi(df, window=14, prefix = '', features_columns=[]):\n",
        "  series = ta.rsi(df['Close'], length=window)\n",
        "  column_name = f\"{prefix}RSI_{window}\"\n",
        "  df[column_name] = series / 100\n",
        "  if(column_name not in features_columns):\n",
        "    features_columns.append(column_name)\n",
        "  return df\n",
        "\n",
        "def add_scaled_ema(df, window=50, prefix = '', features_columns=[]):\n",
        "  series = ta.ema(df['Close'], length=window)\n",
        "  column_name = f\"{prefix}EMA_{window}\"\n",
        "  df[column_name] = series / df['Close']\n",
        "  if column_name not in features_columns:\n",
        "    features_columns.append(column_name)\n",
        "  return df\n",
        "\n",
        "def add_scaled_sma(df, window=50, prefix = '', features_columns=[]):\n",
        "  series = ta.sma(df['Close'], length=window)\n",
        "  column_name = f\"{prefix}SMA_{window}\"\n",
        "  df[column_name] = series / df['Close']\n",
        "  if column_name not in features_columns:\n",
        "    features_columns.append(column_name)\n",
        "  return df\n",
        "\n",
        "def add_scaled_macd(df, prefix= '', features_columns=[], fast = 12, slow = 26, signal=9):\n",
        "  macd_df = ta.macd(df['Close'], fast=fast, slow=slow, signal=signal)\n",
        "  suffix = f\"{fast}_{slow}_{signal}\"\n",
        "  macd_column_name = f\"{prefix}MACD_{suffix}\"\n",
        "  macds_column_name = f\"{prefix}MACDs_{suffix}\"\n",
        "  macdh_column_name = f\"{prefix}MACDh_{suffix}\"\n",
        "  df[macd_column_name] = macd_df[macd_df.columns[0]] / df['Close']\n",
        "  df[macds_column_name] = macd_df[macd_df.columns[2]] / df['Close']\n",
        "  df[macdh_column_name] = macd_df[macd_df.columns[1]] / df['Close']\n",
        "  if macd_column_name not in features_columns:\n",
        "    features_columns.append(macd_column_name)\n",
        "  if macds_column_name not in features_columns:\n",
        "    features_columns.append(macds_column_name)\n",
        "  if macdh_column_name not in features_columns:\n",
        "    features_columns.append(macdh_column_name)\n",
        "  return df\n",
        "\n",
        "def add_scaled_bbands(df, window=20, std=2.0, prefix= '', features_columns=[]):\n",
        "  bbands_df = ta.bbands(df['Close'], length=window, std=std)\n",
        "  suffix = f\"{window}_{std}\"\n",
        "  bbl_column_name = f\"{prefix}BBL_{suffix}\"\n",
        "  bbm_column_name = f\"{prefix}BBM_{suffix}\"\n",
        "  bbu_column_name = f\"{prefix}BBU_{suffix}\"\n",
        "  bbb_column_name = f\"{prefix}BBB_{suffix}\"\n",
        "  bbp_column_name = f\"{prefix}BBP_{suffix}\"\n",
        "  df[bbl_column_name] = bbands_df[bbands_df.columns[0]] / df['Close']\n",
        "  df[bbm_column_name] = bbands_df[bbands_df.columns[1]] / df['Close']\n",
        "  df[bbu_column_name] = bbands_df[bbands_df.columns[2]] / df['Close']\n",
        "  df[bbb_column_name] = bbands_df[bbands_df.columns[3]]\n",
        "  df[bbp_column_name] = bbands_df[bbands_df.columns[4]]\n",
        "  if bbl_column_name not in features_columns: features_columns.append(bbl_column_name)\n",
        "  if bbm_column_name not in features_columns: features_columns.append(bbm_column_name)\n",
        "  if bbu_column_name not in features_columns: features_columns.append(bbu_column_name)\n",
        "  if bbb_column_name not in features_columns: features_columns.append(bbb_column_name)\n",
        "  if bbp_column_name not in features_columns: features_columns.append(bbp_column_name)\n",
        "  return df"
      ],
      "metadata": {
        "id": "Rp7SlrZXaupS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_candlesticks_data(df1, df2):\n",
        "  \"\"\"\n",
        "  Merges two candlestick DataFrames with forward fill, handling different timeframes,\n",
        "  and prevents duplicate OHLCV columns.\n",
        "  Ensure both DataFrames have a datetime index.\n",
        "  And ensure the second DataFrame is the larger timeframe\n",
        "\n",
        "  Args:\n",
        "      df1: First candlestick DataFrame with datetime index.\n",
        "      df2: Second candlestick DataFrame with datetime index.\n",
        "\n",
        "  Returns:\n",
        "      Merged DataFrame with forward-filled values, and no duplicate OHLCV columns.\n",
        "  \"\"\"\n",
        "  # Ensure both DataFrames have a datetime index\n",
        "  if not isinstance(df1.index, pd.DatetimeIndex) or not isinstance(df2.index, pd.DatetimeIndex):\n",
        "      raise ValueError(\"DataFrames must have a datetime index.\")\n",
        "  # Identify OHLCV columns\n",
        "  ohlcv_cols = ['open', 'high', 'low', 'close', 'volume']\n",
        "\n",
        "  # Rename columns in df2 that conflict with df1's OHLCV columns\n",
        "  for col in df2.columns:\n",
        "    if col.lower() in ohlcv_cols and col.lower() in df1.columns.str.lower():\n",
        "      del df2[col]\n",
        "\n",
        "  # Merge the DataFrames using outer join, which preserves all dates\n",
        "  merged_df = pd.merge(df1, df2, how='outer', left_index=True, right_index=True, suffixes=('_df1', '_df2'))\n",
        "\n",
        "  # Forward fill the missing values for each column\n",
        "  for col in merged_df.columns:\n",
        "    merged_df[col] = merged_df[col].ffill()\n",
        "\n",
        "  return merged_df"
      ],
      "metadata": {
        "id": "8nz7qqCsit1b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resample_candles(df, interval='1h'):\n",
        "    \"\"\"\n",
        "    Resamples the DataFrame to the specified interval.\n",
        "    If the interval is in the format '15m', it's converted to '15min'.\n",
        "    \"\"\"\n",
        "    if interval.endswith('m') and not interval.endswith('min'):\n",
        "        interval = interval[:-1] + 'min'  # Convert '15m' to '15min'\n",
        "\n",
        "    return df.resample(interval).agg({\n",
        "        'Open': 'first',\n",
        "        'High': 'max',\n",
        "        'Low': 'min',\n",
        "        'Close': 'last',\n",
        "        'Volume': 'sum'\n",
        "    }).ffill()"
      ],
      "metadata": {
        "id": "t9eNtD9QgvIa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, features_columns, target_col, seq_length):\n",
        "    \"\"\"\n",
        "    Create sequences of `seq_length` time steps for LSTM input, optimized for performance.\n",
        "    \"\"\"\n",
        "    num_samples = len(data) - seq_length\n",
        "    num_features = len(features_columns)\n",
        "\n",
        "    X = np.zeros((num_samples, seq_length, num_features))\n",
        "    y = np.zeros(num_samples)\n",
        "\n",
        "    features_data = data[features_columns].values\n",
        "    target_data = data[target_col].values\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        X[i] = features_data[i:i + seq_length]\n",
        "        y[i] = target_data[i + seq_length]\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "77dtP0gVDcVF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomEarlyStopping(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, patience=2):\n",
        "        super(CustomEarlyStopping, self).__init__()\n",
        "        self.patience = patience  # Number of consecutive increases allowed\n",
        "        self.best_val_mae = float('inf')  # Track the best validation MAE\n",
        "        self.increase_count = 0  # Counter for consecutive increases\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current_val_mae = logs.get('val_mae')  # Get validation MAE for the current epoch\n",
        "\n",
        "        if current_val_mae < self.best_val_mae:\n",
        "            # If validation MAE improves, reset the counter\n",
        "            self.best_val_mae = current_val_mae\n",
        "            self.increase_count = 0\n",
        "        else:\n",
        "            # If validation MAE increases, increment the counter\n",
        "            self.increase_count += 1\n",
        "\n",
        "        # Stop training if validation MAE increases consecutively for 'patience' epochs\n",
        "        if self.increase_count >= self.patience:\n",
        "            print(f\"\\nEarly stopping: Validation MAE increased {self.patience} times in a row.\")\n",
        "            self.model.stop_training = True"
      ],
      "metadata": {
        "id": "PYV3ovEs6j_w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_in_batches(model, test_data, features_columns, seq_length, batch_size=128):\n",
        "    \"\"\"\n",
        "    Predicts in batches to speed up inference.\n",
        "    \"\"\"\n",
        "    num_samples = len(test_data) - seq_length\n",
        "    feature_array = test_data[features_columns].values\n",
        "    predicted_values = np.full(len(test_data), np.nan)\n",
        "\n",
        "    for start_idx in range(0, num_samples, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, num_samples)\n",
        "        batch_indices = range(start_idx + seq_length, end_idx + seq_length)\n",
        "        batch_input = np.array([feature_array[i - seq_length:i] for i in batch_indices])\n",
        "\n",
        "        if len(batch_input) > 0 :\n",
        "            predictions = model.predict(batch_input, verbose=0)\n",
        "            predicted_values[batch_indices] = predictions.flatten() #Flatten to 1d array.\n",
        "\n",
        "    test_data['Predicted_Value'] = predicted_values\n",
        "    return test_data"
      ],
      "metadata": {
        "id": "UxAXoG8PE9je"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PredictedValueStrategy(bt.Strategy):\n",
        "    params = (\n",
        "        ('buy_threshold', 0.6),\n",
        "        ('sell_threshold', 0.6),\n",
        "        ('leverage', 1),  # Leverage ratio\n",
        "        ('margin', 1000),\n",
        "        ('log', True)\n",
        "    )\n",
        "\n",
        "    def __init__(self):\n",
        "        # To keep track of pending orders and buy price/commission\n",
        "        self.order = None\n",
        "        self.buyprice = None\n",
        "        self.buycomm = None\n",
        "\n",
        "        # Add the Predicted_Value as a data feed\n",
        "        self.predicted_value = self.datas[0].predicted_value\n",
        "\n",
        "    def next(self):\n",
        "        # Check if an order is pending ... if yes, we cannot send a 2nd one\n",
        "        if self.order:\n",
        "            return\n",
        "         # # Check if we are in the market\n",
        "        if not self.position:\n",
        "          cash = self.broker.getcash()\n",
        "          position_size = (self.params.margin * self.params.leverage) / self.data.close[0]\n",
        "          # Long signal\n",
        "          if self.predicted_value[0] > self.params.buy_threshold:\n",
        "            self.log('LONG POSITION CREATED, %.2f' % self.datas[0].close[0])\n",
        "            self.order = self.buy(size=position_size)\n",
        "\n",
        "          # Short signal\n",
        "          elif self.predicted_value[0] < self.params.sell_threshold:\n",
        "            self.log('SHORT POSITION CREATED, %.2f' % self.datas[0].close[0])\n",
        "            self.order = self.sell(size=position_size)\n",
        "\n",
        "        else:\n",
        "          if self.predicted_value[0] > self.params.buy_threshold and self.position.size < 0:\n",
        "            self.log('CLOSE SHORT POSITION CREATED, %.2f' % self.datas[0].close[0])\n",
        "            self.order = self.close()\n",
        "          elif self.predicted_value[0] < self.params.sell_threshold and self.position.size > 0:\n",
        "            self.log('CLOSE LONG POSITION CREATED, %.2f' % self.datas[0].close[0])\n",
        "            self.order = self.close()\n",
        "\n",
        "    def notify_order(self, order):\n",
        "        if order.status in [order.Submitted, order.Accepted]:\n",
        "            # Buy/Sell order submitted/accepted to/by broker - Nothing to do\n",
        "            return\n",
        "        # Check if an order has been completed\n",
        "        # Attention: broker could reject order if not enough cash\n",
        "        if order.status in [order.Completed]:\n",
        "            if order.isbuy():\n",
        "                self.log('BUY EXECUTED, %.2f' % order.executed.price)\n",
        "            elif order.issell():\n",
        "                self.log('SELL EXECUTED, %.2f' % order.executed.price)\n",
        "        elif order.status in [order.Canceled, order.Margin, order.Rejected]:\n",
        "            if order.status == order.Canceled:\n",
        "                self.log('Order Canceled')\n",
        "            elif order.status == order.Margin:\n",
        "                self.log(f'Order Margin Not Enough - Available cash: {self.broker.getcash()}')\n",
        "            elif order.status == order.Rejected:\n",
        "                self.log('Order Rejected')\n",
        "\n",
        "        # Write down: no pending order\n",
        "        self.order = None\n",
        "\n",
        "    def notify_trade(self, trade):\n",
        "        if not trade.isclosed:\n",
        "            return\n",
        "        self.log(f'TRADE COMPLETED, GROSS {trade.pnl:.2f}, NET {trade.pnlcomm:.2f}, Available Cash {self.broker.getcash():.2f}')\n",
        "\n",
        "    def log(self, txt, dt=None):\n",
        "        if not self.params.log:\n",
        "          return\n",
        "        dt = dt or self.datas[0].datetime.date(0)\n",
        "        time = self.datas[0].datetime.time()\n",
        "        print(f'{dt.isoformat()} {time.isoformat()}, {txt}')\n"
      ],
      "metadata": {
        "id": "SvTq9isY3ctm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extend PandasData to include the custom column\n",
        "class CustomPandasData(bt.feeds.PandasData):\n",
        "    # Add custom columns\n",
        "    lines = ('predicted_value',)  # Add the custom line\n",
        "    params = (\n",
        "        ('predicted_value', 'Predicted_Value'),  # Map the column name\n",
        "    )\n"
      ],
      "metadata": {
        "id": "gRqdeATXlKaO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dict_even(data):\n",
        "    \"\"\"\n",
        "    Recursively makes all numeric values in a dictionary even.\n",
        "\n",
        "    Args:\n",
        "        data (dict): The input dictionary (can have nested dictionaries or lists).\n",
        "\n",
        "    Returns:\n",
        "        dict: A new dictionary with all numeric values made even.\n",
        "    \"\"\"\n",
        "    if isinstance(data, dict):\n",
        "        new_dict = {}\n",
        "        for key, value in data.items():\n",
        "            new_dict[key] = make_dict_even(value)\n",
        "        return new_dict\n",
        "    elif isinstance(data, list):\n",
        "        return [make_dict_even(item) for item in data]\n",
        "    elif isinstance(data, (int, float)):\n",
        "        if isinstance(data, int):\n",
        "            return data if data % 2 == 0 else data + 1\n",
        "        else: # float. We will round to an int, and then make even.\n",
        "            int_value = round(data)\n",
        "            return int_value if int_value % 2 == 0 else int_value + 1\n",
        "\n",
        "    else:\n",
        "        return data  # Return non-numeric values as they are"
      ],
      "metadata": {
        "id": "Y1RTvfAxGxYu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_cloud_storage_path(bucket_name, local_file_name):\n",
        "    \"\"\"\n",
        "    Returns the cloud storage path for a given model name.\n",
        "\n",
        "    Parameters:\n",
        "        local_file_name (str): The name of the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The cloud storage path. (e.g., gs://<bucket_name>/trade/models/<model_name>.h5).\n",
        "    \"\"\"\n",
        "    return f'gs://{bucket_name}/trade/models/{local_file_name}'\n",
        "\n",
        "def save_model_to_cloud_storage(model: tf.keras.Model, model_name: str, bucket_name: str):\n",
        "    \"\"\"\n",
        "    Saves a TensorFlow model to Google Cloud Storage and returns the cloud storage file path.\n",
        "\n",
        "    Parameters:\n",
        "        model (tf.keras.Model): The TensorFlow model to save.\n",
        "        model_name (str): The name of the model (used to create the file name).\n",
        "        bucket_name (str): The name of the Google Cloud Storage bucket.\n",
        "\n",
        "    Returns:\n",
        "        str: The cloud storage file path (e.g., gs://<bucket_name>/trade/models/<model_name>.h5).\n",
        "    \"\"\"\n",
        "    # Define the local and cloud storage file paths\n",
        "    local_file_name = f'{model_name}.h5'\n",
        "    local_file_path = f'/tmp/{local_file_name}'\n",
        "    cloud_file_path = get_model_cloud_storage_path(bucket_name, local_file_name)\n",
        "\n",
        "    # Save the model locally\n",
        "    model.save(local_file_path)\n",
        "\n",
        "    try:\n",
        "        # Upload the model to Google Cloud Storage\n",
        "        subprocess.run(['gsutil', 'cp', local_file_path, cloud_file_path], check=True)\n",
        "        print(f\"Model saved to {cloud_file_path}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle errors during the upload process\n",
        "        raise RuntimeError(f\"Failed to upload model to Google Cloud Storage: {e}\")\n",
        "    finally:\n",
        "        # Clean up the local file\n",
        "        if os.path.exists(local_file_path):\n",
        "            os.remove(local_file_path)\n",
        "    return cloud_file_path\n",
        "\n",
        "def load_model_from_cloud_storage(model_name: str, bucket_name: str):\n",
        "    \"\"\"\n",
        "    Loads a TensorFlow model from Google Cloud Storage.\n",
        "\n",
        "    Parameters:\n",
        "        model_name (str): The name of the model (used to create the file name).\n",
        "        bucket_name (str): The name of the Google Cloud Storage bucket.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: The loaded TensorFlow model.\n",
        "    \"\"\"\n",
        "    # Define the local and cloud storage file paths\n",
        "    local_file_name = f'{model_name}.h5'\n",
        "    local_file_path = f'/tmp/{local_file_name}'\n",
        "    cloud_file_path = get_model_cloud_storage_path(bucket_name, local_file_name)\n",
        "\n",
        "    try:\n",
        "        # Download the model from Google Cloud Storage\n",
        "        subprocess.run(['gsutil', 'cp', cloud_file_path, local_file_path], check=True)\n",
        "\n",
        "        # Load the model\n",
        "        model = tf.keras.models.load_model(local_file_path)\n",
        "        print(f\"Model loaded from {cloud_file_path}\")\n",
        "        return model\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        raise FileNotFoundError(f\"Model not found in Google Cloud Storage: {e}\")\n",
        "    finally:\n",
        "        # Clean up the local file\n",
        "        if os.path.exists(local_file_path):\n",
        "            os.remove(local_file_path)\n"
      ],
      "metadata": {
        "id": "8eAPe1BN4bO-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_firestore(project_id):\n",
        "    \"\"\"\n",
        "    Initialize the Firestore client using the service account key.\n",
        "\n",
        "    Parameters:\n",
        "        project_id (str): The Google Cloud project ID.\n",
        "\n",
        "    Returns:\n",
        "        firestore.Client: Initialized Firestore client.\n",
        "    \"\"\"\n",
        "    if not firebase_admin._apps:\n",
        "        # cred = credentials.Certificate(service_account_key_path)\n",
        "        cred = firebase_admin.credentials.ApplicationDefault()\n",
        "        cred._project_id = project_id  # Add this line\n",
        "        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project_id\n",
        "        firebase_admin.initialize_app(cred, {'projectId': project_id})\n",
        "    return firestore.client()\n",
        "\n",
        "\n",
        "def save_to_firestore(project_id, collection_name, data):\n",
        "    \"\"\"\n",
        "    Save data to a Firestore collection.\n",
        "\n",
        "    Parameters:\n",
        "        project_id (str): The Google Cloud project ID.\n",
        "        service_account_key_path (str): Path to the Firebase service account key JSON file.\n",
        "        collection_name (str): Name of the Firestore collection.\n",
        "        data (dict): Data to save in the document.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Initialize Firestore client\n",
        "    db = initialize_firestore(project_id)\n",
        "\n",
        "    # Save data to Firestore\n",
        "    doc_ref = db.collection(collection_name).document()\n",
        "    doc_ref.set(data)\n",
        "    print(f\"Data saved to Firestore: Collection={collection_name}, Document ID={doc_ref.id}\")\n",
        "\n",
        "\n",
        "def delete_documents_by_field(project_id, collection_name, field_name, field_value, batch_size=500):\n",
        "  \"\"\"\n",
        "  Deletes documents in a Firestore collection where a specific field matches a value.\n",
        "\n",
        "  Args:\n",
        "      collection_name: The name of the Firestore collection.\n",
        "      field_name: The name of the field to filter by.\n",
        "      field_value: The value to filter the field against.\n",
        "      batch_size: The number of documents to delete in each batch.\n",
        "  \"\"\"\n",
        "  db = initialize_firestore(project_id)\n",
        "  collection_ref = db.collection(collection_name)\n",
        "\n",
        "  try:\n",
        "      while True:\n",
        "          query = collection_ref.where(field_name, '==', field_value).limit(batch_size)\n",
        "          docs = query.stream()\n",
        "\n",
        "          deleted_count = 0\n",
        "          batch = db.batch()\n",
        "\n",
        "          for doc in docs:\n",
        "              batch.delete(doc.reference)\n",
        "              deleted_count += 1\n",
        "\n",
        "          if deleted_count == 0:\n",
        "              print(f\"No documents found with {field_name} == {field_value} in {collection_name}.\")\n",
        "              break\n",
        "\n",
        "          batch.commit()\n",
        "          print(f\"Deleted {deleted_count} documents from {collection_name} where {field_name} == {field_value}.\")\n",
        "          if deleted_count < batch_size: # if less than batch size deleted, then there are no more documents matching.\n",
        "              break\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "ySNwgm17Ce54"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_param_grid(params):\n",
        "    \"\"\"\n",
        "    Generate a grid of all possible hyperparameter combinations.\n",
        "\n",
        "    Args:\n",
        "        params (dict): A dictionary where keys are hyperparameter names and values are lists of possible values.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, where each dictionary represents a unique combination of hyperparameters.\n",
        "    \"\"\"\n",
        "    keys = params.keys()\n",
        "    values = params.values()\n",
        "\n",
        "    # Generate all possible combinations of hyperparameters\n",
        "    param_grid = [dict(zip(keys, combination)) for combination in itertools.product(*values)]\n",
        "\n",
        "    return param_grid\n"
      ],
      "metadata": {
        "id": "XQkkV3Y2KpLx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_out_of_range_values(obj):\n",
        "    \"\"\"\n",
        "    Recursively traverse a nested object and convert out-of-range values to zero.\n",
        "\n",
        "    Args:\n",
        "        obj: The input object (can be a dictionary, list, or other nested structure).\n",
        "\n",
        "    Returns:\n",
        "        The same object with out-of-range values replaced by zero.\n",
        "    \"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        # If the object is a dictionary, recursively process its values\n",
        "        for key, value in obj.items():\n",
        "            obj[key] = fix_out_of_range_values(value)\n",
        "    elif isinstance(obj, list):\n",
        "        # If the object is a list, recursively process its elements\n",
        "        for i in range(len(obj)):\n",
        "            obj[i] = fix_out_of_range_values(obj[i])\n",
        "    else:\n",
        "        # If the object is a value, check if it's out of range\n",
        "        try:\n",
        "            # Attempt to perform a numeric operation to check if the value is valid\n",
        "            # For example, check if the value can be compared to a range\n",
        "            if not (-1e15 < obj < 1e14):  # Adjust the range as needed\n",
        "                obj = 0\n",
        "        except (TypeError, ValueError):\n",
        "            # If the value is not numeric or causes an error, replace it with zero\n",
        "            obj = 0\n",
        "    return obj"
      ],
      "metadata": {
        "id": "XJU3s2FgYVaF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrain_in_chunks(X, y, chunk_size=100000, train_validate_split_ratio=0.2, train_epochs=20, bucket_name = \"yourbucketname\", version = \"yourversion\", timeframe = \"yourtimeframe\", start_date = \"yourstartdate\", end_date = \"yourenddate\"):\n",
        "    \"\"\"Retrains the model in chunks to handle memory limit issues.\"\"\"\n",
        "\n",
        "    num_chunks = int(np.ceil(len(X) / chunk_size))\n",
        "    model = None  # Initialize model outside the loop\n",
        "\n",
        "    for chunk_idx in range(num_chunks):\n",
        "        print(f\"Processing chunk {chunk_idx + 1}/{num_chunks}\")\n",
        "\n",
        "        start_idx = chunk_idx * chunk_size\n",
        "        end_idx = min((chunk_idx + 1) * chunk_size, len(X))\n",
        "\n",
        "        X_chunk = X[start_idx:end_idx]\n",
        "        y_chunk = y[start_idx:end_idx]\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_chunk, y_chunk, test_size=train_validate_split_ratio, shuffle=False\n",
        "        )\n",
        "\n",
        "        model_input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "        if model is None:\n",
        "            # Build the model only for the first chunk\n",
        "            model = Sequential([\n",
        "                LSTM(100, return_sequences=True, input_shape=model_input_shape),\n",
        "                Dropout(0.2),\n",
        "                LSTM(50, return_sequences=False),\n",
        "                Dropout(0.2),\n",
        "                Dense(50, activation='relu'),\n",
        "                Dropout(0.2),\n",
        "                Dense(25, activation='relu'),\n",
        "                Dropout(0.2),\n",
        "                Dense(10, activation='relu'),\n",
        "                Dropout(0.2),\n",
        "                Dense(1, activation='tanh')\n",
        "            ])\n",
        "            model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "        custom_early_stopping = CustomEarlyStopping(patience=3)\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=train_epochs,\n",
        "            batch_size=32,\n",
        "            validation_data=(X_test, y_test),\n",
        "            verbose=1,\n",
        "            callbacks=[custom_early_stopping]\n",
        "        )\n",
        "\n",
        "        final_epoch = len(history.history['loss'])\n",
        "        train_loss = history.history['loss'][-1]\n",
        "        train_mae = history.history['mae'][-1]\n",
        "        val_loss = history.history['val_loss'][-1]\n",
        "        val_mae = history.history['val_mae'][-1]\n",
        "\n",
        "        print(f\"Chunk {chunk_idx + 1} training complete.\")\n",
        "        gc.collect() #garbage collection to free memory.\n",
        "        tf.keras.backend.clear_session()#clear session.\n",
        "\n",
        "    # Save the final trained model\n",
        "    random_hash = generate_random_hash()\n",
        "    model_name = f\"binance_mtf_{version}_{timeframe}_{start_date}_{end_date}_{random_hash}\"\n",
        "    model_path = save_model_to_cloud_storage(model, model_name, bucket_name)\n",
        "    return model_path"
      ],
      "metadata": {
        "id": "awMVWpsot8s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU if available\n",
        "# Check if a GPU is available\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Restrict TensorFlow to use the first GPU\n",
        "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "    except RuntimeError as e:\n",
        "        # Visible devices must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"No GPU found.\")"
      ],
      "metadata": {
        "id": "OmJTiylVTUXj",
        "outputId": "fbb80ce0-249d-4dae-9405-558d6ad3a0f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "options = {\n",
        "  'timeframe': '5m',\n",
        "  'symbol': 'BTCUSDT',\n",
        "  'exchange': 'binance',\n",
        "  'start_date': '01-01-2024',\n",
        "  'end_date': '28-02-2025' ,\n",
        "  'pivot_windows': 10,\n",
        "  'version': 'v1_0_4',\n",
        "  'middle_timeframe': '15m',\n",
        "  'higher_timeframe': '1h',\n",
        "  'seq_length': 500\n",
        "}"
      ],
      "metadata": {
        "id": "i2eIpKkrnB80"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_model(**options):\n",
        "  # Extract options\n",
        "  timeframe = options['timeframe'] if 'timeframe' in options else '15m'\n",
        "  symbol = options['symbol'] if 'symbol' in options else 'BTCUSDT'\n",
        "  exchange = options['exchange'] if 'exchange' in options else 'binance'\n",
        "  start_date = options['start_date'] if 'start_date' in options else '01-01-2024'\n",
        "  end_date = options['end_date'] if 'end_date' in options else '28-02-2025'\n",
        "  pivot_windows = options['pivot_windows'] if 'pivot_windows' in options else 10\n",
        "  version = options['version'] if 'version' in options else 'v1'\n",
        "  middle_timeframe = options['middle_timeframe'] if 'middle_timeframe' in options else '1h'\n",
        "  higher_timeframe = options['higher_timeframe'] if 'higher_timeframe' in options else '4h'\n",
        "  seq_length = options['seq_length'] if 'seq_length' in options else 100\n",
        "\n",
        "  # Get raw candles\n",
        "  data = get_all_binance_candles(symbol, timeframe, start_date, end_date)\n",
        "  # Add pivots\n",
        "  data = add_pivots(data, int(pivot_windows))\n",
        "  data['Pivot'] = np.where(data['Higher_Pivot'] == 1, -1, np.where(data['Lower_Pivot'] == 1, 1, 0))\n",
        "  del data['Higher_Pivot']\n",
        "  del data['Lower_Pivot']\n",
        "  # Calculate pivot proximity\n",
        "  data = calculate_pivot_proximity(data)\n",
        "  # Declare feature columns\n",
        "  features_columns = []\n",
        "\n",
        "  # # Add technical indicators on lower timeframe\n",
        "  lower_timeframe = timeframe\n",
        "  lower_timeframe_prefix = f\"{lower_timeframe}_\"\n",
        "  add_scaled_rsi(data, 14, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_rsi(data, 6, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(data, 5, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(data, 21, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_sma(data, 50, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_macd(data, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_bbands(data, prefix=lower_timeframe_prefix, features_columns=features_columns)\n",
        "\n",
        "  # Add technical indicators on middle timeframe\n",
        "  middle_timeframe_prefix = f\"{middle_timeframe}_\"\n",
        "  middle_data = resample_candles(data, middle_timeframe)\n",
        "  add_scaled_rsi(middle_data, 14, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_rsi(middle_data, 6, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(middle_data, 5, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(middle_data, 21, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_macd(middle_data, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_bbands(middle_data, prefix=middle_timeframe_prefix, features_columns=features_columns)\n",
        "\n",
        "  # Add technical indicators on higher timeframe\n",
        "  higher_timeframe_prefix = f\"{higher_timeframe}_\"\n",
        "  higher_data = resample_candles(data, higher_timeframe)\n",
        "  add_scaled_rsi(higher_data, 14, prefix=higher_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_rsi(higher_data, 6, prefix=higher_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(higher_data, 5, prefix=higher_timeframe_prefix, features_columns=features_columns)\n",
        "  add_scaled_ema(higher_data, 21, prefix=higher_timeframe_prefix, features_columns=features_columns)\n",
        "\n",
        "  # Merge middle and higher timeframe data to lower timeframe data\n",
        "  data = merge_candlesticks_data(data, middle_data)\n",
        "  data = merge_candlesticks_data(data, higher_data)\n",
        "\n",
        "  # Drop rows with NaN values (due to rolling calculations)\n",
        "  data.dropna(inplace=True)\n",
        "\n",
        "  # delete Pivot_Porximity that have zero at the beginning and ending of the dataframe\n",
        "  non_zero_indices = data[data['Pivot_Proximity'] != 0].index\n",
        "  non_zero_at_begining = non_zero_indices[0]\n",
        "  non_zero_at_end = non_zero_indices[-1]\n",
        "  data = data.loc[non_zero_at_begining:non_zero_at_end]\n",
        "\n",
        "  # select only first 80 % of the data\n",
        "  training_data_ratio = 0.8 # 80%\n",
        "  training_data = data[:int(len(data) * training_data_ratio)]\n",
        "  training_data_start_date = training_data.index[0]\n",
        "  training_data_end_date = training_data.index[-1]\n",
        "\n",
        "  # Create training sequences\n",
        "  target_col = 'Pivot_Proximity'\n",
        "  X, y = create_sequences(training_data, features_columns, target_col, seq_length)\n",
        "\n",
        "  # Split into training and testing sets\n",
        "  train_validate_split_ratio = 0.2\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=train_validate_split_ratio, shuffle=False)\n",
        "  train_candles_length = len(X_train)\n",
        "  val_candles_length = len(X_test)\n",
        "\n",
        "  # Build the LSTM model\n",
        "  model_input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "  model = Sequential([\n",
        "      LSTM(100, return_sequences=True, input_shape=model_input_shape),\n",
        "      Dropout(0.2),\n",
        "      LSTM(50, return_sequences=False),\n",
        "      Dropout(0.2),\n",
        "      Dense(50, activation='relu'),\n",
        "      Dropout(0.2),\n",
        "      Dense(25, activation='relu'),\n",
        "      Dropout(0.2),\n",
        "      Dense(10, activation='relu'),\n",
        "      Dropout(0.2),\n",
        "      Dense(1, activation='tanh')\n",
        "  ])\n",
        "  model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "  # Define early stopping\n",
        "  custom_early_stopping = CustomEarlyStopping(patience=3)\n",
        "\n",
        "  # Step 5: Train the model\n",
        "  train_epochs = 20\n",
        "  history = model.fit(\n",
        "      X_train, y_train,\n",
        "      epochs=train_epochs,\n",
        "      batch_size=32,\n",
        "      validation_data=(X_test, y_test),\n",
        "      verbose=1,\n",
        "      callbacks=[custom_early_stopping]\n",
        "  )\n",
        "  final_epoch = len(history.history['loss'])\n",
        "  train_loss = history.history['loss'][-1]\n",
        "  train_mae = history.history['mae'][-1]\n",
        "  val_loss = history.history['val_loss'][-1]\n",
        "  val_mae = history.history['val_mae'][-1]\n",
        "\n",
        "  # Save model to cloud storage\n",
        "  # Save the model\n",
        "  random_hash = generate_random_hash()\n",
        "  model_name = f\"binance_mtf_{version}_{timeframe}_{start_date}_{end_date}_{random_hash}\"\n",
        "  model_path = save_model_to_cloud_storage(model, model_name, bucket_name)\n",
        "\n",
        "  # test data\n",
        "  test_data = data[int(len(data) * training_data_ratio):]\n",
        "\n",
        "  # Predict in batches\n",
        "  batch_size = 128\n",
        "  predict_in_batches(model, test_data, features_columns, seq_length, batch_size)\n",
        "  test_data.dropna(inplace=True) # Early predicted values wont be availble due to sequencing\n",
        "  test_data_start_date = test_data.index[0]\n",
        "  test_data_end_date = test_data.index[-1]\n",
        "\n",
        "  # Load data into a Pandas DataFrame\n",
        "  backtest_data = CustomPandasData(\n",
        "      dataname=test_data,\n",
        "      datetime=None,  # Use the index as the datetime\n",
        "      open='Open',         # Column index for Open\n",
        "      high='High',         # Column index for High\n",
        "      low='Low',          # Column index for Low\n",
        "      close='Close',        # Column index for Close\n",
        "      volume='Volume',       # Column index for Volume\n",
        "      openinterest=None,# No open interest column\n",
        "      predicted_value='Predicted_Value'  # Column index for Predicted_Value\n",
        "  )\n",
        "\n",
        "  bactest_grid_params = generate_param_grid({\n",
        "      'buy_threshold': [0.3, 0.4, 0.5, 0.6],\n",
        "      'sell_threshold': [-0.3, -0.4, -0.5, -0.6],\n",
        "      'leverage': [7, 10, 15, 20, 25, 30],\n",
        "  })\n",
        "\n",
        "  for param in bactest_grid_params:\n",
        "    # Grid trade params\n",
        "    trade_buy_threshold = param['buy_threshold']\n",
        "    trade_sell_threshold = param['sell_threshold']\n",
        "    trade_leverage = param['leverage']\n",
        "    print(f\"Backtesting- Buy Threshold: {trade_buy_threshold}, Sell Threshold: {trade_sell_threshold}, leverage: {trade_leverage}\")\n",
        "\n",
        "    # backtest configuration\n",
        "    trade_margin = 1000\n",
        "    broker_commision = 0.02 # In percentage\n",
        "\n",
        "    # Create a Cerebro engine instance\n",
        "    cerebro = bt.Cerebro()\n",
        "\n",
        "    # Add the strategy\n",
        "    cerebro.addstrategy(\n",
        "        PredictedValueStrategy,\n",
        "        buy_threshold=trade_buy_threshold,\n",
        "        sell_threshold=trade_sell_threshold,\n",
        "        leverage=trade_leverage,\n",
        "        margin=trade_margin,\n",
        "        log=False\n",
        "    )\n",
        "\n",
        "    # Add the data feed\n",
        "    cerebro.adddata(backtest_data)\n",
        "\n",
        "    # Set the initial cash\n",
        "    cerebro.broker.set_cash(10000.0)\n",
        "\n",
        "    # Set the commission\n",
        "    cerebro.broker.setcommission(commission=broker_commision / 100)\n",
        "\n",
        "    # Add analyzers\n",
        "    cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe')\n",
        "    cerebro.addanalyzer(bt.analyzers.DrawDown, _name='drawdown')\n",
        "    cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name='tradeanalyzer')\n",
        "    cerebro.addanalyzer(bt.analyzers.Returns, _name='returns')\n",
        "\n",
        "    # Run the backtest\n",
        "    starting_portfolio_value = cerebro.broker.getvalue()\n",
        "    print('Starting Portfolio Value: %.2f' % starting_portfolio_value)\n",
        "    backtest_result = cerebro.run()\n",
        "    final_portfolio_value = cerebro.broker.getvalue()\n",
        "    print('Final Portfolio Value: %.2f' % final_portfolio_value)\n",
        "\n",
        "    # Capture analysis\n",
        "    strat = backtest_result[0]\n",
        "\n",
        "    trade_analysis = make_dict_even(strat.analyzers.tradeanalyzer.get_analysis())\n",
        "    trade_analysis = fix_out_of_range_values(trade_analysis)\n",
        "\n",
        "    drawdown_analysis = make_dict_even(strat.analyzers.drawdown.get_analysis())\n",
        "    drawdown_analysis = fix_out_of_range_values(drawdown_analysis)\n",
        "    sharpe_analysis = make_dict_even(strat.analyzers.sharpe.get_analysis())\n",
        "    sharpe_analysis = fix_out_of_range_values(sharpe_analysis)\n",
        "\n",
        "    analysis_result = {\n",
        "        'timeframe': timeframe,\n",
        "        'start_date': start_date,\n",
        "        'end_date': end_date,\n",
        "        'symbol': symbol,\n",
        "        'exchange': exchange,\n",
        "        'features_columns': features_columns,\n",
        "        'lower_timeframe': lower_timeframe,\n",
        "        'middle_timeframe': middle_timeframe,\n",
        "        'higher_timeframe': higher_timeframe,\n",
        "        'seq_length': seq_length,\n",
        "        'training_data_ratio': training_data_ratio,\n",
        "        'epochs': train_epochs,\n",
        "        'model_input_shape': model_input_shape,\n",
        "        'model_name': model_name,\n",
        "        'model_path': model_path,\n",
        "        'train_loss': train_loss,\n",
        "        'train_mae': train_mae,\n",
        "        'val_loss': val_loss,\n",
        "        'val_mae': val_mae,\n",
        "        'final_epoch': final_epoch,\n",
        "        'training_data_start_date': training_data_start_date,\n",
        "        'training_data_end_date': training_data_end_date,\n",
        "        'test_data_start_date': test_data_start_date,\n",
        "        'test_data_end_date': test_data_end_date,\n",
        "        'trade_leverage': trade_leverage,\n",
        "        'trade_margin': trade_margin,\n",
        "        'trade_buy_threshold': trade_buy_threshold,\n",
        "        'trade_sell_threshold': trade_sell_threshold,\n",
        "        'starting_portfolio_value': starting_portfolio_value,\n",
        "        'final_portfolio_value': final_portfolio_value,\n",
        "        'broker_commision': broker_commision,\n",
        "        'trade_analysis': trade_analysis,\n",
        "        'drawdown_analysis': drawdown_analysis,\n",
        "        'sharpe_analysis': sharpe_analysis,\n",
        "        'version': version,\n",
        "        'created_at': datetime.datetime.now()\n",
        "    }\n",
        "    save_to_firestore(project_id, firestore_collection_name, analysis_result)\n",
        "    print(analysis_result)"
      ],
      "metadata": {
        "id": "EjeV2AzvnY1Q"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_periods =  [(datetime.datetime(x,1,1), datetime.datetime(x,1,1) + datetime.timedelta(days=365*3)) for x in range(2017,2024)]\n",
        "for start_date, end_date in evaluate_periods:\n",
        "  options['start_date'] = start_date.strftime('%d-%m-%Y')\n",
        "  options['end_date'] = end_date.strftime('%d-%m-%Y')\n",
        "  print(f\"Evaluating from {start_date} to {end_date}\")\n",
        "  generate_model(**options)"
      ],
      "metadata": {
        "id": "rI-yYG2aR3sS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6f686ad-c9e7-4e32-8959-a15b05976c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating from 2017-01-01 00:00:00 to 2020-01-01 00:00:00\n",
            "Copying file:///tmp/binance_BTCUSDT_5m_01-01-2017_01-01-2020.csv [Content-Type=text/csv]...\n",
            "/ [1 files][ 14.7 MiB/ 14.7 MiB]                                                \n",
            "Operation completed over 1 objects/14.7 MiB.                                     \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m4959/4959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 40ms/step - loss: 0.3123 - mae: 0.4728 - val_loss: 0.2501 - val_mae: 0.4207\n",
            "Epoch 2/20\n",
            "\u001b[1m4959/4959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 39ms/step - loss: 0.2632 - mae: 0.4256 - val_loss: 0.2341 - val_mae: 0.4020\n",
            "Epoch 3/20\n",
            "\u001b[1m4959/4959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 39ms/step - loss: 0.2574 - mae: 0.4190 - val_loss: 0.2289 - val_mae: 0.3930\n",
            "Epoch 4/20\n",
            "\u001b[1m4959/4959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 40ms/step - loss: 0.2545 - mae: 0.4152 - val_loss: 0.2330 - val_mae: 0.4031\n",
            "Epoch 5/20\n",
            "\u001b[1m4959/4959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 40ms/step - loss: 0.2503 - mae: 0.4110 - val_loss: 0.2313 - val_mae: 0.4028\n",
            "Epoch 6/20\n",
            "\u001b[1m4959/4959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.2486 - mae: 0.4097\n",
            "Early stopping: Validation MAE increased 3 times in a row.\n",
            "\u001b[1m4959/4959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 40ms/step - loss: 0.2486 - mae: 0.4097 - val_loss: 0.2308 - val_mae: 0.3985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to gs://ife-storage/trade/models/binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-54e86c8c4e9a>:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['Predicted_Value'] = predicted_values\n",
            "<ipython-input-24-39052b2707e6>:132: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data.dropna(inplace=True) # Early predicted values wont be availble due to sequencing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backtesting- Buy Threshold: 0.3, Sell Threshold: -0.3, leverage: 7\n",
            "Starting Portfolio Value: 10000.00\n",
            "Final Portfolio Value: 51929.96\n",
            "Data saved to Firestore: Collection=trade-models, Document ID=nhJ942cB552PyD9YfOxL\n",
            "{'timeframe': '5m', 'start_date': '01-01-2017', 'end_date': '01-01-2020', 'symbol': 'BTCUSDT', 'exchange': 'binance', 'features_columns': ['5m_RSI_14', '5m_RSI_6', '5m_EMA_5', '5m_EMA_21', '5m_SMA_50', '5m_MACD_12_26_9', '5m_MACDs_12_26_9', '5m_MACDh_12_26_9', '5m_BBL_20_2.0', '5m_BBM_20_2.0', '5m_BBU_20_2.0', '5m_BBB_20_2.0', '5m_BBP_20_2.0', '15m_RSI_14', '15m_RSI_6', '15m_EMA_5', '15m_EMA_21', '15m_MACD_12_26_9', '15m_MACDs_12_26_9', '15m_MACDh_12_26_9', '15m_BBL_20_2.0', '15m_BBM_20_2.0', '15m_BBU_20_2.0', '15m_BBB_20_2.0', '15m_BBP_20_2.0', '1h_RSI_14', '1h_RSI_6', '1h_EMA_5', '1h_EMA_21'], 'lower_timeframe': '5m', 'middle_timeframe': '15m', 'higher_timeframe': '1h', 'seq_length': 500, 'training_data_ratio': 0.8, 'epochs': 20, 'model_input_shape': (500, 29), 'model_name': 'binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9', 'model_path': 'gs://ife-storage/trade/models/binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9.h5', 'train_loss': 0.24852071702480316, 'train_mae': 0.4091811776161194, 'val_loss': 0.23084217309951782, 'val_mae': 0.3985236585140228, 'final_epoch': 6, 'training_data_start_date': Timestamp('2017-08-18 00:00:00'), 'training_data_end_date': Timestamp('2019-07-11 23:55:00'), 'test_data_start_date': Timestamp('2019-07-13 17:40:00'), 'test_data_end_date': Timestamp('2019-12-31 22:50:00'), 'trade_leverage': 7, 'trade_margin': 1000, 'trade_buy_threshold': 0.3, 'trade_sell_threshold': -0.3, 'starting_portfolio_value': 10000.0, 'final_portfolio_value': 51929.96438959595, 'broker_commision': 0.02, 'trade_analysis': {'total': {'total': 2128, 'open': 2, 'closed': 2128}, 'streak': {'won': {'current': 2, 'longest': 26}, 'lost': {'current': 0, 'longest': 4}}, 'pnl': {'gross': {'total': 47906, 'average': 24}, 'net': {'total': 41950, 'average': 20}}, 'won': {'total': 1706, 'pnl': {'total': 60200, 'average': 36, 'max': 592}}, 'lost': {'total': 422, 'pnl': {'total': -18248, 'average': -42, 'max': -696}}, 'long': {'total': 1108, 'pnl': {'total': 20110, 'average': 18, 'won': {'total': 32026, 'average': 38, 'max': 592}, 'lost': {'total': -11916, 'average': -44, 'max': -696}}, 'won': 846, 'lost': 262}, 'short': {'total': 1020, 'pnl': {'total': 21842, 'average': 22, 'won': {'total': 28174, 'average': 34, 'max': 330}, 'lost': {'total': -6332, 'average': -40, 'max': -330}}, 'won': 860, 'lost': 160}, 'len': {'total': 43996, 'average': 22, 'max': 134, 'min': 2, 'won': {'total': 28772, 'average': 18, 'max': 90, 'min': 2}, 'lost': {'total': 15224, 'average': 36, 'max': 134, 'min': 2}, 'long': {'total': 28580, 'average': 26, 'max': 134, 'min': 2, 'won': {'total': 17810, 'average': 22, 'max': 90, 'min': 2}, 'lost': {'total': 10770, 'average': 42, 'max': 134, 'min': 2}}, 'short': {'total': 15416, 'average': 16, 'max': 68, 'min': 2, 'won': {'total': 10962, 'average': 14, 'max': 50, 'min': 2}, 'lost': {'total': 4454, 'average': 28, 'max': 68, 'min': 2}}}}, 'drawdown_analysis': {'len': 80, 'drawdown': 0, 'moneydown': 30, 'max': {'len': 932, 'drawdown': 4, 'moneydown': 1246}}, 'sharpe_analysis': {'sharperatio': 0}, 'version': 'v1_0_4', 'created_at': datetime.datetime(2025, 3, 15, 2, 55, 52, 43210)}\n",
            "Backtesting- Buy Threshold: 0.3, Sell Threshold: -0.3, leverage: 10\n",
            "Starting Portfolio Value: 10000.00\n",
            "Final Portfolio Value: 70050.52\n",
            "Data saved to Firestore: Collection=trade-models, Document ID=n93mQ4VVp4bK1dpzfeIz\n",
            "{'timeframe': '5m', 'start_date': '01-01-2017', 'end_date': '01-01-2020', 'symbol': 'BTCUSDT', 'exchange': 'binance', 'features_columns': ['5m_RSI_14', '5m_RSI_6', '5m_EMA_5', '5m_EMA_21', '5m_SMA_50', '5m_MACD_12_26_9', '5m_MACDs_12_26_9', '5m_MACDh_12_26_9', '5m_BBL_20_2.0', '5m_BBM_20_2.0', '5m_BBU_20_2.0', '5m_BBB_20_2.0', '5m_BBP_20_2.0', '15m_RSI_14', '15m_RSI_6', '15m_EMA_5', '15m_EMA_21', '15m_MACD_12_26_9', '15m_MACDs_12_26_9', '15m_MACDh_12_26_9', '15m_BBL_20_2.0', '15m_BBM_20_2.0', '15m_BBU_20_2.0', '15m_BBB_20_2.0', '15m_BBP_20_2.0', '1h_RSI_14', '1h_RSI_6', '1h_EMA_5', '1h_EMA_21'], 'lower_timeframe': '5m', 'middle_timeframe': '15m', 'higher_timeframe': '1h', 'seq_length': 500, 'training_data_ratio': 0.8, 'epochs': 20, 'model_input_shape': (500, 29), 'model_name': 'binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9', 'model_path': 'gs://ife-storage/trade/models/binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9.h5', 'train_loss': 0.24852071702480316, 'train_mae': 0.4091811776161194, 'val_loss': 0.23084217309951782, 'val_mae': 0.3985236585140228, 'final_epoch': 6, 'training_data_start_date': Timestamp('2017-08-18 00:00:00'), 'training_data_end_date': Timestamp('2019-07-11 23:55:00'), 'test_data_start_date': Timestamp('2019-07-13 17:40:00'), 'test_data_end_date': Timestamp('2019-12-31 22:50:00'), 'trade_leverage': 10, 'trade_margin': 1000, 'trade_buy_threshold': 0.3, 'trade_sell_threshold': -0.3, 'starting_portfolio_value': 10000.0, 'final_portfolio_value': 70050.51674830214, 'broker_commision': 0.02, 'trade_analysis': {'total': {'total': 2128, 'open': 2, 'closed': 2126}, 'streak': {'won': {'current': 2, 'longest': 26}, 'lost': {'current': 0, 'longest': 4}}, 'pnl': {'gross': {'total': 68582, 'average': 32}, 'net': {'total': 60080, 'average': 28}}, 'won': {'total': 1706, 'pnl': {'total': 86000, 'average': 50, 'max': 846}}, 'lost': {'total': 422, 'pnl': {'total': -25920, 'average': -62, 'max': -994}}, 'long': {'total': 1108, 'pnl': {'total': 28876, 'average': 26, 'won': {'total': 45752, 'average': 54, 'max': 846}, 'lost': {'total': -16876, 'average': -64, 'max': -994}}, 'won': 846, 'lost': 262}, 'short': {'total': 1020, 'pnl': {'total': 31204, 'average': 32, 'won': {'total': 40250, 'average': 48, 'max': 472}, 'lost': {'total': -9046, 'average': -56, 'max': -472}}, 'won': 860, 'lost': 160}, 'len': {'total': 43970, 'average': 22, 'max': 134, 'min': 2, 'won': {'total': 28774, 'average': 18, 'max': 90, 'min': 2}, 'lost': {'total': 15196, 'average': 36, 'max': 134, 'min': 2}, 'long': {'total': 28552, 'average': 26, 'max': 134, 'min': 2, 'won': {'total': 17810, 'average': 22, 'max': 90, 'min': 2}, 'lost': {'total': 10742, 'average': 42, 'max': 134, 'min': 2}}, 'short': {'total': 15418, 'average': 16, 'max': 68, 'min': 2, 'won': {'total': 10964, 'average': 14, 'max': 50, 'min': 2}, 'lost': {'total': 4454, 'average': 28, 'max': 68, 'min': 2}}}}, 'drawdown_analysis': {'len': 80, 'drawdown': 0, 'moneydown': 44, 'max': {'len': 932, 'drawdown': 6, 'moneydown': 1778}}, 'sharpe_analysis': {'sharperatio': 0}, 'version': 'v1_0_4', 'created_at': datetime.datetime(2025, 3, 15, 2, 56, 5, 264636)}\n",
            "Backtesting- Buy Threshold: 0.3, Sell Threshold: -0.3, leverage: 15\n",
            "Starting Portfolio Value: 10000.00\n",
            "Final Portfolio Value: 95185.60\n",
            "Data saved to Firestore: Collection=trade-models, Document ID=qVUVTCAnBoWfI2c2LKj2\n",
            "{'timeframe': '5m', 'start_date': '01-01-2017', 'end_date': '01-01-2020', 'symbol': 'BTCUSDT', 'exchange': 'binance', 'features_columns': ['5m_RSI_14', '5m_RSI_6', '5m_EMA_5', '5m_EMA_21', '5m_SMA_50', '5m_MACD_12_26_9', '5m_MACDs_12_26_9', '5m_MACDh_12_26_9', '5m_BBL_20_2.0', '5m_BBM_20_2.0', '5m_BBU_20_2.0', '5m_BBB_20_2.0', '5m_BBP_20_2.0', '15m_RSI_14', '15m_RSI_6', '15m_EMA_5', '15m_EMA_21', '15m_MACD_12_26_9', '15m_MACDs_12_26_9', '15m_MACDh_12_26_9', '15m_BBL_20_2.0', '15m_BBM_20_2.0', '15m_BBU_20_2.0', '15m_BBB_20_2.0', '15m_BBP_20_2.0', '1h_RSI_14', '1h_RSI_6', '1h_EMA_5', '1h_EMA_21'], 'lower_timeframe': '5m', 'middle_timeframe': '15m', 'higher_timeframe': '1h', 'seq_length': 500, 'training_data_ratio': 0.8, 'epochs': 20, 'model_input_shape': (500, 29), 'model_name': 'binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9', 'model_path': 'gs://ife-storage/trade/models/binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9.h5', 'train_loss': 0.24852071702480316, 'train_mae': 0.4091811776161194, 'val_loss': 0.23084217309951782, 'val_mae': 0.3985236585140228, 'final_epoch': 6, 'training_data_start_date': Timestamp('2017-08-18 00:00:00'), 'training_data_end_date': Timestamp('2019-07-11 23:55:00'), 'test_data_start_date': Timestamp('2019-07-13 17:40:00'), 'test_data_end_date': Timestamp('2019-12-31 22:50:00'), 'trade_leverage': 15, 'trade_margin': 1000, 'trade_buy_threshold': 0.3, 'trade_sell_threshold': -0.3, 'starting_portfolio_value': 10000.0, 'final_portfolio_value': 95185.59888905556, 'broker_commision': 0.02, 'trade_analysis': {'total': {'total': 2076, 'open': 2, 'closed': 2076}, 'streak': {'won': {'current': 2, 'longest': 26}, 'lost': {'current': 0, 'longest': 4}}, 'pnl': {'gross': {'total': 97676, 'average': 48}, 'net': {'total': 85228, 'average': 42}}, 'won': {'total': 1664, 'pnl': {'total': 121882, 'average': 74, 'max': 1268}}, 'lost': {'total': 412, 'pnl': {'total': -36652, 'average': -88, 'max': -1492}}, 'long': {'total': 1050, 'pnl': {'total': 38994, 'average': 38, 'won': {'total': 61512, 'average': 78, 'max': 1268}, 'lost': {'total': -22516, 'average': -90, 'max': -1492}}, 'won': 804, 'lost': 248}, 'short': {'total': 1026, 'pnl': {'total': 46234, 'average': 46, 'won': {'total': 60370, 'average': 70, 'max': 708}, 'lost': {'total': -14136, 'average': -86, 'max': -708}}, 'won': 860, 'lost': 166}, 'len': {'total': 42764, 'average': 22, 'max': 134, 'min': 2, 'won': {'total': 27934, 'average': 18, 'max': 90, 'min': 2}, 'lost': {'total': 14830, 'average': 36, 'max': 134, 'min': 2}, 'long': {'total': 27216, 'average': 26, 'max': 134, 'min': 2, 'won': {'total': 17026, 'average': 22, 'max': 90, 'min': 2}, 'lost': {'total': 10192, 'average': 42, 'max': 134, 'min': 2}}, 'short': {'total': 15548, 'average': 16, 'max': 68, 'min': 2, 'won': {'total': 10910, 'average': 14, 'max': 50, 'min': 2}, 'lost': {'total': 4638, 'average': 28, 'max': 68, 'min': 2}}}}, 'drawdown_analysis': {'len': 80, 'drawdown': 0, 'moneydown': 64, 'max': {'len': 932, 'drawdown': 6, 'moneydown': 2668}}, 'sharpe_analysis': {'sharperatio': 0}, 'version': 'v1_0_4', 'created_at': datetime.datetime(2025, 3, 15, 2, 56, 18, 349911)}\n",
            "Backtesting- Buy Threshold: 0.3, Sell Threshold: -0.3, leverage: 20\n",
            "Starting Portfolio Value: 10000.00\n",
            "Final Portfolio Value: 121885.92\n",
            "Data saved to Firestore: Collection=trade-models, Document ID=3wCjyeKwJFtvuFddZJZr\n",
            "{'timeframe': '5m', 'start_date': '01-01-2017', 'end_date': '01-01-2020', 'symbol': 'BTCUSDT', 'exchange': 'binance', 'features_columns': ['5m_RSI_14', '5m_RSI_6', '5m_EMA_5', '5m_EMA_21', '5m_SMA_50', '5m_MACD_12_26_9', '5m_MACDs_12_26_9', '5m_MACDh_12_26_9', '5m_BBL_20_2.0', '5m_BBM_20_2.0', '5m_BBU_20_2.0', '5m_BBB_20_2.0', '5m_BBP_20_2.0', '15m_RSI_14', '15m_RSI_6', '15m_EMA_5', '15m_EMA_21', '15m_MACD_12_26_9', '15m_MACDs_12_26_9', '15m_MACDh_12_26_9', '15m_BBL_20_2.0', '15m_BBM_20_2.0', '15m_BBU_20_2.0', '15m_BBB_20_2.0', '15m_BBP_20_2.0', '1h_RSI_14', '1h_RSI_6', '1h_EMA_5', '1h_EMA_21'], 'lower_timeframe': '5m', 'middle_timeframe': '15m', 'higher_timeframe': '1h', 'seq_length': 500, 'training_data_ratio': 0.8, 'epochs': 20, 'model_input_shape': (500, 29), 'model_name': 'binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9', 'model_path': 'gs://ife-storage/trade/models/binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9.h5', 'train_loss': 0.24852071702480316, 'train_mae': 0.4091811776161194, 'val_loss': 0.23084217309951782, 'val_mae': 0.3985236585140228, 'final_epoch': 6, 'training_data_start_date': Timestamp('2017-08-18 00:00:00'), 'training_data_end_date': Timestamp('2019-07-11 23:55:00'), 'test_data_start_date': Timestamp('2019-07-13 17:40:00'), 'test_data_end_date': Timestamp('2019-12-31 22:50:00'), 'trade_leverage': 20, 'trade_margin': 1000, 'trade_buy_threshold': 0.3, 'trade_sell_threshold': -0.3, 'starting_portfolio_value': 10000.0, 'final_portfolio_value': 121885.91899558954, 'broker_commision': 0.02, 'trade_analysis': {'total': {'total': 2058, 'open': 2, 'closed': 2056}, 'streak': {'won': {'current': 2, 'longest': 26}, 'lost': {'current': 0, 'longest': 4}}, 'pnl': {'gross': {'total': 128388, 'average': 62}, 'net': {'total': 111942, 'average': 54}}, 'won': {'total': 1646, 'pnl': {'total': 160038, 'average': 98, 'max': 1690}}, 'lost': {'total': 410, 'pnl': {'total': -48094, 'average': -116, 'max': -1990}}, 'long': {'total': 1026, 'pnl': {'total': 49508, 'average': 48, 'won': {'total': 78674, 'average': 102, 'max': 1690}, 'lost': {'total': -29166, 'average': -118, 'max': -1990}}, 'won': 782, 'lost': 246}, 'short': {'total': 1030, 'pnl': {'total': 62436, 'average': 62, 'won': {'total': 81364, 'average': 94, 'max': 944}, 'lost': {'total': -18928, 'average': -114, 'max': -944}}, 'won': 866, 'lost': 166}, 'len': {'total': 42274, 'average': 22, 'max': 134, 'min': 2, 'won': {'total': 27524, 'average': 18, 'max': 90, 'min': 2}, 'lost': {'total': 14752, 'average': 36, 'max': 134, 'min': 2}, 'long': {'total': 26660, 'average': 26, 'max': 134, 'min': 2, 'won': {'total': 16550, 'average': 22, 'max': 90, 'min': 2}, 'lost': {'total': 10110, 'average': 42, 'max': 134, 'min': 2}}, 'short': {'total': 15616, 'average': 16, 'max': 68, 'min': 2, 'won': {'total': 10974, 'average': 14, 'max': 50, 'min': 2}, 'lost': {'total': 4642, 'average': 28, 'max': 68, 'min': 2}}}}, 'drawdown_analysis': {'len': 80, 'drawdown': 0, 'moneydown': 86, 'max': {'len': 932, 'drawdown': 8, 'moneydown': 3556}}, 'sharpe_analysis': {'sharperatio': 0}, 'version': 'v1_0_4', 'created_at': datetime.datetime(2025, 3, 15, 2, 56, 31, 582188)}\n",
            "Backtesting- Buy Threshold: 0.3, Sell Threshold: -0.3, leverage: 25\n",
            "Starting Portfolio Value: 10000.00\n",
            "Final Portfolio Value: 148833.65\n",
            "Data saved to Firestore: Collection=trade-models, Document ID=uWEoRqnur7sEST4oPs6j\n",
            "{'timeframe': '5m', 'start_date': '01-01-2017', 'end_date': '01-01-2020', 'symbol': 'BTCUSDT', 'exchange': 'binance', 'features_columns': ['5m_RSI_14', '5m_RSI_6', '5m_EMA_5', '5m_EMA_21', '5m_SMA_50', '5m_MACD_12_26_9', '5m_MACDs_12_26_9', '5m_MACDh_12_26_9', '5m_BBL_20_2.0', '5m_BBM_20_2.0', '5m_BBU_20_2.0', '5m_BBB_20_2.0', '5m_BBP_20_2.0', '15m_RSI_14', '15m_RSI_6', '15m_EMA_5', '15m_EMA_21', '15m_MACD_12_26_9', '15m_MACDs_12_26_9', '15m_MACDh_12_26_9', '15m_BBL_20_2.0', '15m_BBM_20_2.0', '15m_BBU_20_2.0', '15m_BBB_20_2.0', '15m_BBP_20_2.0', '1h_RSI_14', '1h_RSI_6', '1h_EMA_5', '1h_EMA_21'], 'lower_timeframe': '5m', 'middle_timeframe': '15m', 'higher_timeframe': '1h', 'seq_length': 500, 'training_data_ratio': 0.8, 'epochs': 20, 'model_input_shape': (500, 29), 'model_name': 'binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9', 'model_path': 'gs://ife-storage/trade/models/binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9.h5', 'train_loss': 0.24852071702480316, 'train_mae': 0.4091811776161194, 'val_loss': 0.23084217309951782, 'val_mae': 0.3985236585140228, 'final_epoch': 6, 'training_data_start_date': Timestamp('2017-08-18 00:00:00'), 'training_data_end_date': Timestamp('2019-07-11 23:55:00'), 'test_data_start_date': Timestamp('2019-07-13 17:40:00'), 'test_data_end_date': Timestamp('2019-12-31 22:50:00'), 'trade_leverage': 25, 'trade_margin': 1000, 'trade_buy_threshold': 0.3, 'trade_sell_threshold': -0.3, 'starting_portfolio_value': 10000.0, 'final_portfolio_value': 148833.6487413823, 'broker_commision': 0.02, 'trade_analysis': {'total': {'total': 2046, 'open': 2, 'closed': 2046}, 'streak': {'won': {'current': 2, 'longest': 26}, 'lost': {'current': 0, 'longest': 4}}, 'pnl': {'gross': {'total': 159350, 'average': 78}, 'net': {'total': 138904, 'average': 68}}, 'won': {'total': 1640, 'pnl': {'total': 198664, 'average': 122, 'max': 2112}}, 'lost': {'total': 406, 'pnl': {'total': -59760, 'average': -148, 'max': -2488}}, 'long': {'total': 1014, 'pnl': {'total': 60514, 'average': 60, 'won': {'total': 96644, 'average': 126, 'max': 2112}, 'lost': {'total': -36130, 'average': -150, 'max': -2488}}, 'won': 772, 'lost': 242}, 'short': {'total': 1032, 'pnl': {'total': 78392, 'average': 76, 'won': {'total': 102022, 'average': 118, 'max': 1224}, 'lost': {'total': -23630, 'average': -144, 'max': -1180}}, 'won': 868, 'lost': 164}, 'len': {'total': 41976, 'average': 22, 'max': 134, 'min': 2, 'won': {'total': 27432, 'average': 18, 'max': 90, 'min': 2}, 'lost': {'total': 14544, 'average': 36, 'max': 134, 'min': 2}, 'long': {'total': 26334, 'average': 26, 'max': 134, 'min': 2, 'won': {'total': 16378, 'average': 22, 'max': 90, 'min': 2}, 'lost': {'total': 9956, 'average': 42, 'max': 134, 'min': 2}}, 'short': {'total': 15642, 'average': 16, 'max': 68, 'min': 2, 'won': {'total': 11054, 'average': 14, 'max': 52, 'min': 2}, 'lost': {'total': 4588, 'average': 28, 'max': 68, 'min': 2}}}}, 'drawdown_analysis': {'len': 80, 'drawdown': 0, 'moneydown': 108, 'max': {'len': 932, 'drawdown': 8, 'moneydown': 4446}}, 'sharpe_analysis': {'sharperatio': 0}, 'version': 'v1_0_4', 'created_at': datetime.datetime(2025, 3, 15, 2, 56, 44, 880319)}\n",
            "Backtesting- Buy Threshold: 0.3, Sell Threshold: -0.3, leverage: 30\n",
            "Starting Portfolio Value: 10000.00\n",
            "Final Portfolio Value: 166899.51\n",
            "Data saved to Firestore: Collection=trade-models, Document ID=HPsRmqlv50RHO9fH1rAP\n",
            "{'timeframe': '5m', 'start_date': '01-01-2017', 'end_date': '01-01-2020', 'symbol': 'BTCUSDT', 'exchange': 'binance', 'features_columns': ['5m_RSI_14', '5m_RSI_6', '5m_EMA_5', '5m_EMA_21', '5m_SMA_50', '5m_MACD_12_26_9', '5m_MACDs_12_26_9', '5m_MACDh_12_26_9', '5m_BBL_20_2.0', '5m_BBM_20_2.0', '5m_BBU_20_2.0', '5m_BBB_20_2.0', '5m_BBP_20_2.0', '15m_RSI_14', '15m_RSI_6', '15m_EMA_5', '15m_EMA_21', '15m_MACD_12_26_9', '15m_MACDs_12_26_9', '15m_MACDh_12_26_9', '15m_BBL_20_2.0', '15m_BBM_20_2.0', '15m_BBU_20_2.0', '15m_BBB_20_2.0', '15m_BBP_20_2.0', '1h_RSI_14', '1h_RSI_6', '1h_EMA_5', '1h_EMA_21'], 'lower_timeframe': '5m', 'middle_timeframe': '15m', 'higher_timeframe': '1h', 'seq_length': 500, 'training_data_ratio': 0.8, 'epochs': 20, 'model_input_shape': (500, 29), 'model_name': 'binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9', 'model_path': 'gs://ife-storage/trade/models/binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9.h5', 'train_loss': 0.24852071702480316, 'train_mae': 0.4091811776161194, 'val_loss': 0.23084217309951782, 'val_mae': 0.3985236585140228, 'final_epoch': 6, 'training_data_start_date': Timestamp('2017-08-18 00:00:00'), 'training_data_end_date': Timestamp('2019-07-11 23:55:00'), 'test_data_start_date': Timestamp('2019-07-13 17:40:00'), 'test_data_end_date': Timestamp('2019-12-31 22:50:00'), 'trade_leverage': 30, 'trade_margin': 1000, 'trade_buy_threshold': 0.3, 'trade_sell_threshold': -0.3, 'starting_portfolio_value': 10000.0, 'final_portfolio_value': 166899.5062887287, 'broker_commision': 0.02, 'trade_analysis': {'total': {'total': 1974, 'open': 2, 'closed': 1974}, 'streak': {'won': {'current': 2, 'longest': 26}, 'lost': {'current': 0, 'longest': 4}}, 'pnl': {'gross': {'total': 180654, 'average': 92}, 'net': {'total': 156984, 'average': 80}}, 'won': {'total': 1578, 'pnl': {'total': 227434, 'average': 144, 'max': 2534}}, 'lost': {'total': 396, 'pnl': {'total': -70448, 'average': -178, 'max': -2986}}, 'long': {'total': 934, 'pnl': {'total': 63936, 'average': 70, 'won': {'total': 105178, 'average': 150, 'max': 2534}, 'lost': {'total': -41242, 'average': -180, 'max': -2986}}, 'won': 704, 'lost': 230}, 'short': {'total': 1040, 'pnl': {'total': 93050, 'average': 90, 'won': {'total': 122256, 'average': 140, 'max': 1470}, 'lost': {'total': -29206, 'average': -174, 'max': -1416}}, 'won': 874, 'lost': 168}, 'len': {'total': 40094, 'average': 20, 'max': 134, 'min': 2, 'won': {'total': 26010, 'average': 16, 'max': 90, 'min': 2}, 'lost': {'total': 14084, 'average': 36, 'max': 134, 'min': 2}, 'long': {'total': 24280, 'average': 26, 'max': 134, 'min': 2, 'won': {'total': 14888, 'average': 22, 'max': 90, 'min': 2}, 'lost': {'total': 9392, 'average': 42, 'max': 134, 'min': 2}}, 'short': {'total': 15816, 'average': 16, 'max': 68, 'min': 2, 'won': {'total': 11122, 'average': 14, 'max': 52, 'min': 2}, 'lost': {'total': 4694, 'average': 28, 'max': 68, 'min': 2}}}}, 'drawdown_analysis': {'len': 80, 'drawdown': 0, 'moneydown': 128, 'max': {'len': 1226, 'drawdown': 10, 'moneydown': 5334}}, 'sharpe_analysis': {'sharperatio': 0}, 'version': 'v1_0_4', 'created_at': datetime.datetime(2025, 3, 15, 2, 56, 58, 199238)}\n",
            "Backtesting- Buy Threshold: 0.3, Sell Threshold: -0.4, leverage: 7\n",
            "Starting Portfolio Value: 10000.00\n",
            "Final Portfolio Value: 46765.05\n",
            "Data saved to Firestore: Collection=trade-models, Document ID=kKuBDUteNNWjQd0sXkRH\n",
            "{'timeframe': '5m', 'start_date': '01-01-2017', 'end_date': '01-01-2020', 'symbol': 'BTCUSDT', 'exchange': 'binance', 'features_columns': ['5m_RSI_14', '5m_RSI_6', '5m_EMA_5', '5m_EMA_21', '5m_SMA_50', '5m_MACD_12_26_9', '5m_MACDs_12_26_9', '5m_MACDh_12_26_9', '5m_BBL_20_2.0', '5m_BBM_20_2.0', '5m_BBU_20_2.0', '5m_BBB_20_2.0', '5m_BBP_20_2.0', '15m_RSI_14', '15m_RSI_6', '15m_EMA_5', '15m_EMA_21', '15m_MACD_12_26_9', '15m_MACDs_12_26_9', '15m_MACDh_12_26_9', '15m_BBL_20_2.0', '15m_BBM_20_2.0', '15m_BBU_20_2.0', '15m_BBB_20_2.0', '15m_BBP_20_2.0', '1h_RSI_14', '1h_RSI_6', '1h_EMA_5', '1h_EMA_21'], 'lower_timeframe': '5m', 'middle_timeframe': '15m', 'higher_timeframe': '1h', 'seq_length': 500, 'training_data_ratio': 0.8, 'epochs': 20, 'model_input_shape': (500, 29), 'model_name': 'binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9', 'model_path': 'gs://ife-storage/trade/models/binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9.h5', 'train_loss': 0.24852071702480316, 'train_mae': 0.4091811776161194, 'val_loss': 0.23084217309951782, 'val_mae': 0.3985236585140228, 'final_epoch': 6, 'training_data_start_date': Timestamp('2017-08-18 00:00:00'), 'training_data_end_date': Timestamp('2019-07-11 23:55:00'), 'test_data_start_date': Timestamp('2019-07-13 17:40:00'), 'test_data_end_date': Timestamp('2019-12-31 22:50:00'), 'trade_leverage': 7, 'trade_margin': 1000, 'trade_buy_threshold': 0.3, 'trade_sell_threshold': -0.4, 'starting_portfolio_value': 10000.0, 'final_portfolio_value': 46765.05101634916, 'broker_commision': 0.02, 'trade_analysis': {'total': {'total': 1506, 'open': 2, 'closed': 1504}, 'streak': {'won': {'current': 0, 'longest': 26}, 'lost': {'current': 2, 'longest': 4}}, 'pnl': {'gross': {'total': 40996, 'average': 28}, 'net': {'total': 36784, 'average': 24}}, 'won': {'total': 1222, 'pnl': {'total': 52050, 'average': 44, 'max': 604}}, 'lost': {'total': 284, 'pnl': {'total': -15266, 'average': -54, 'max': -684}}, 'long': {'total': 840, 'pnl': {'total': 18746, 'average': 22, 'won': {'total': 30582, 'average': 50, 'max': 604}, 'lost': {'total': -11834, 'average': -54, 'max': -684}}, 'won': 626, 'lost': 214}, 'short': {'total': 666, 'pnl': {'total': 18038, 'average': 28, 'won': {'total': 21470, 'average': 36, 'max': 330}, 'lost': {'total': -3430, 'average': -50, 'max': -330}}, 'won': 596, 'lost': 70}, 'len': {'total': 43910, 'average': 30, 'max': 278, 'min': 2, 'won': {'total': 27998, 'average': 24, 'max': 278, 'min': 2}, 'lost': {'total': 15914, 'average': 56, 'max': 230, 'min': 4}, 'long': {'total': 34826, 'average': 42, 'max': 278, 'min': 2, 'won': {'total': 20830, 'average': 34, 'max': 278, 'min': 2}, 'lost': {'total': 13996, 'average': 66, 'max': 230, 'min': 8}}, 'short': {'total': 9086, 'average': 14, 'max': 66, 'min': 2, 'won': {'total': 7168, 'average': 12, 'max': 46, 'min': 2}, 'lost': {'total': 1918, 'average': 28, 'max': 66, 'min': 4}}}}, 'drawdown_analysis': {'len': 80, 'drawdown': 0, 'moneydown': 56, 'max': {'len': 770, 'drawdown': 6, 'moneydown': 1246}}, 'sharpe_analysis': {'sharperatio': 0}, 'version': 'v1_0_4', 'created_at': datetime.datetime(2025, 3, 15, 2, 57, 10, 574775)}\n",
            "Backtesting- Buy Threshold: 0.3, Sell Threshold: -0.4, leverage: 10\n",
            "Starting Portfolio Value: 10000.00\n",
            "Final Portfolio Value: 62879.47\n",
            "Data saved to Firestore: Collection=trade-models, Document ID=DIwP9TZax7cQbEMyw4iP\n",
            "{'timeframe': '5m', 'start_date': '01-01-2017', 'end_date': '01-01-2020', 'symbol': 'BTCUSDT', 'exchange': 'binance', 'features_columns': ['5m_RSI_14', '5m_RSI_6', '5m_EMA_5', '5m_EMA_21', '5m_SMA_50', '5m_MACD_12_26_9', '5m_MACDs_12_26_9', '5m_MACDh_12_26_9', '5m_BBL_20_2.0', '5m_BBM_20_2.0', '5m_BBU_20_2.0', '5m_BBB_20_2.0', '5m_BBP_20_2.0', '15m_RSI_14', '15m_RSI_6', '15m_EMA_5', '15m_EMA_21', '15m_MACD_12_26_9', '15m_MACDs_12_26_9', '15m_MACDh_12_26_9', '15m_BBL_20_2.0', '15m_BBM_20_2.0', '15m_BBU_20_2.0', '15m_BBB_20_2.0', '15m_BBP_20_2.0', '1h_RSI_14', '1h_RSI_6', '1h_EMA_5', '1h_EMA_21'], 'lower_timeframe': '5m', 'middle_timeframe': '15m', 'higher_timeframe': '1h', 'seq_length': 500, 'training_data_ratio': 0.8, 'epochs': 20, 'model_input_shape': (500, 29), 'model_name': 'binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9', 'model_path': 'gs://ife-storage/trade/models/binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9.h5', 'train_loss': 0.24852071702480316, 'train_mae': 0.4091811776161194, 'val_loss': 0.23084217309951782, 'val_mae': 0.3985236585140228, 'final_epoch': 6, 'training_data_start_date': Timestamp('2017-08-18 00:00:00'), 'training_data_end_date': Timestamp('2019-07-11 23:55:00'), 'test_data_start_date': Timestamp('2019-07-13 17:40:00'), 'test_data_end_date': Timestamp('2019-12-31 22:50:00'), 'trade_leverage': 10, 'trade_margin': 1000, 'trade_buy_threshold': 0.3, 'trade_sell_threshold': -0.4, 'starting_portfolio_value': 10000.0, 'final_portfolio_value': 62879.46950979776, 'broker_commision': 0.02, 'trade_analysis': {'total': {'total': 1506, 'open': 2, 'closed': 1504}, 'streak': {'won': {'current': 0, 'longest': 26}, 'lost': {'current': 2, 'longest': 4}}, 'pnl': {'gross': {'total': 58924, 'average': 40}, 'net': {'total': 52908, 'average': 36}}, 'won': {'total': 1222, 'pnl': {'total': 74542, 'average': 62, 'max': 864}}, 'lost': {'total': 282, 'pnl': {'total': -21634, 'average': -76, 'max': -976}}, 'long': {'total': 838, 'pnl': {'total': 26916, 'average': 32, 'won': {'total': 43650, 'average': 70, 'max': 864}, 'lost': {'total': -16734, 'average': -78, 'max': -976}}, 'won': 626, 'lost': 214}, 'short': {'total': 666, 'pnl': {'total': 25992, 'average': 40, 'won': {'total': 30892, 'average': 52, 'max': 472}, 'lost': {'total': -4900, 'average': -70, 'max': -472}}, 'won': 598, 'lost': 70}, 'len': {'total': 43884, 'average': 30, 'max': 278, 'min': 2, 'won': {'total': 28002, 'average': 24, 'max': 278, 'min': 2}, 'lost': {'total': 15884, 'average': 56, 'max': 230, 'min': 4}, 'long': {'total': 34794, 'average': 42, 'max': 278, 'min': 2, 'won': {'total': 20828, 'average': 34, 'max': 278, 'min': 2}, 'lost': {'total': 13966, 'average': 66, 'max': 230, 'min': 8}}, 'short': {'total': 9090, 'average': 14, 'max': 66, 'min': 2, 'won': {'total': 7174, 'average': 12, 'max': 46, 'min': 2}, 'lost': {'total': 1918, 'average': 28, 'max': 66, 'min': 4}}}}, 'drawdown_analysis': {'len': 80, 'drawdown': 0, 'moneydown': 80, 'max': {'len': 770, 'drawdown': 6, 'moneydown': 1778}}, 'sharpe_analysis': {'sharperatio': 0}, 'version': 'v1_0_4', 'created_at': datetime.datetime(2025, 3, 15, 2, 57, 23, 233863)}\n",
            "Backtesting- Buy Threshold: 0.3, Sell Threshold: -0.4, leverage: 15\n",
            "Starting Portfolio Value: 10000.00\n",
            "Final Portfolio Value: 85087.76\n",
            "Data saved to Firestore: Collection=trade-models, Document ID=t0Nno6RMi2c9fi95m6Pb\n",
            "{'timeframe': '5m', 'start_date': '01-01-2017', 'end_date': '01-01-2020', 'symbol': 'BTCUSDT', 'exchange': 'binance', 'features_columns': ['5m_RSI_14', '5m_RSI_6', '5m_EMA_5', '5m_EMA_21', '5m_SMA_50', '5m_MACD_12_26_9', '5m_MACDs_12_26_9', '5m_MACDh_12_26_9', '5m_BBL_20_2.0', '5m_BBM_20_2.0', '5m_BBU_20_2.0', '5m_BBB_20_2.0', '5m_BBP_20_2.0', '15m_RSI_14', '15m_RSI_6', '15m_EMA_5', '15m_EMA_21', '15m_MACD_12_26_9', '15m_MACDs_12_26_9', '15m_MACDh_12_26_9', '15m_BBL_20_2.0', '15m_BBM_20_2.0', '15m_BBU_20_2.0', '15m_BBB_20_2.0', '15m_BBP_20_2.0', '1h_RSI_14', '1h_RSI_6', '1h_EMA_5', '1h_EMA_21'], 'lower_timeframe': '5m', 'middle_timeframe': '15m', 'higher_timeframe': '1h', 'seq_length': 500, 'training_data_ratio': 0.8, 'epochs': 20, 'model_input_shape': (500, 29), 'model_name': 'binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9', 'model_path': 'gs://ife-storage/trade/models/binance_mtf_v1_0_4_5m_01-01-2017_01-01-2020_5692bcc9.h5', 'train_loss': 0.24852071702480316, 'train_mae': 0.4091811776161194, 'val_loss': 0.23084217309951782, 'val_mae': 0.3985236585140228, 'final_epoch': 6, 'training_data_start_date': Timestamp('2017-08-18 00:00:00'), 'training_data_end_date': Timestamp('2019-07-11 23:55:00'), 'test_data_start_date': Timestamp('2019-07-13 17:40:00'), 'test_data_end_date': Timestamp('2019-12-31 22:50:00'), 'trade_leverage': 15, 'trade_margin': 1000, 'trade_buy_threshold': 0.3, 'trade_sell_threshold': -0.4, 'starting_portfolio_value': 10000.0, 'final_portfolio_value': 85087.75706866705, 'broker_commision': 0.02, 'trade_analysis': {'total': {'total': 1472, 'open': 2, 'closed': 1470}, 'streak': {'won': {'current': 0, 'longest': 26}, 'lost': {'current': 2, 'longest': 4}}, 'pnl': {'gross': {'total': 83948, 'average': 58}, 'net': {'total': 75130, 'average': 52}}, 'won': {'total': 1194, 'pnl': {'total': 106310, 'average': 90, 'max': 1294}}, 'lost': {'total': 276, 'pnl': {'total': -31180, 'average': -112, 'max': -1464}}, 'long': {'total': 794, 'pnl': {'total': 35766, 'average': 46, 'won': {'total': 58554, 'average': 100, 'max': 1294}, 'lost': {'total': -22786, 'average': -112, 'max': -1464}}, 'won': 592, 'lost': 202}, 'short': {'total': 676, 'pnl': {'total': 39364, 'average': 58, 'won': {'total': 47758, 'average': 80, 'max': 708}, 'lost': {'total': -8394, 'average': -112, 'max': -708}}, 'won': 602, 'lost': 74}, 'len': {'total': 42534, 'average': 30, 'max': 278, 'min': 2, 'won': {'total': 27126, 'average': 24, 'max': 278, 'min': 2}, 'lost': {'total': 15408, 'average': 56, 'max': 230, 'min': 4}, 'long': {'total': 33270, 'average': 42, 'max': 278, 'min': 2, 'won': {'total': 19954, 'average': 34, 'max': 278, 'min': 2}, 'lost': {'total': 13316, 'average': 66, 'max': 230, 'min': 8}}, 'short': {'total': 9264, 'average': 14, 'max': 66, 'min': 2, 'won': {'total': 7172, 'average': 12, 'max': 46, 'min': 2}, 'lost': {'total': 2094, 'average': 28, 'max': 66, 'min': 4}}}}, 'drawdown_analysis': {'len': 80, 'drawdown': 0, 'moneydown': 120, 'max': {'len': 770, 'drawdown': 6, 'moneydown': 2668}}, 'sharpe_analysis': {'sharperatio': 0}, 'version': 'v1_0_4', 'created_at': datetime.datetime(2025, 3, 15, 2, 57, 36, 73989)}\n",
            "Backtesting- Buy Threshold: 0.3, Sell Threshold: -0.4, leverage: 20\n",
            "Starting Portfolio Value: 10000.00\n"
          ]
        }
      ]
    }
  ]
}